---
title: "June"
---

June was a big one.

We launched **Model Catalog**, one of our most requested features from enterprise users, and something weâ€™ve spent the last few weeks building, testing, and refining. Itâ€™s now the central control plane for managing every model your teams use through Portkey.

Along with that, we rolled out several core updates across the platform and gateway including OpenAI Agents TS SDK support, circuit breaker, global endpoints for Vertex AI, expanded Azure coverage, and more.

June also saw increased GitHub momentum as we rolled out one of our largest infrastructure upgrades to date!

<img src="/images/changelog/Group%201000007181.png" alt="June Changelog Highlight" width="100%" style={{ borderRadius: "12px" }} />

Hereâ€™s everything that went live in June.


## Summary

| Area         | Key Highlights                                                                                                                               |
| :----------- | :------------------------------------------------------------------------------------------------------------------------------------------- |
| **Platform**            | â€¢ Model Catalog launch  <br /> â€¢ OpenAI background mode  <br /> â€¢ Circuit breaker config|
| **Gateway & Providers** | â€¢ Support for Sutra, o3-pro, Magistral, and Gemini 2.5 models  <br /> â€¢ Vertex AI global endpoints  <br /> â€¢ Anthropic Computer Use tool support  <br /> â€¢ Support for additional Azure OpenAI endpoints  <br /> â€¢ Bedrock Inference Profiles  <br /> â€¢ Prompt caching for tools (Anthropic) |
| **Integrations**        | â€¢ OpenAI Agent SDK (TypeScript)  <br /> â€¢ Langroid native support  <br /> â€¢ Strands SDK & ADK integration  <br /> â€¢ Cursor integration  <br /> â€¢ Gemini CLI support |
| **Community & Partnerships** | â€¢ FutureAGI integration  <br /> â€¢ Prompt Security integration  <br /> â€¢ Lasso Security integration  <br /> â€¢ Arize AI cookbook |

---

## ðŸŽ‰ Introducing the Model Catalog ðŸŽ‰
Take control of which models your teams can use across every provider, from one place.
With Model Catalog, you can:
- Manage access to 1,600+ models across OpenAI, Anthropic, Azure, and more
- Decide which models are available to which teams
- Set rate limits and budgets per workspace
- Automatically enable new models as theyâ€™re released

<Frame>
  <video width="700" autoPlay loop muted playsInline>
    <source src="https://cfassets.portkey.ai/WEBSITE%2FPRODUCT%20PAGES%2FModel%20Catalog%2FNew%20Hero.mp4" type="video/mp4" />
    Your browser does not support the video tag.
  </video>
</Frame>

No more scattered configuration or manual provisioning. Just clean, centralized governance for all your AI usage.

To enable it in your organizattion,reach out to us at support@portkey.ai

## Circuit breaker

When a model starts failing or slowing down, Portkey can now temporarily route traffic to fallback targets â€” then automatically restore routing once things stabilize.
Itâ€™s a smarter way to handle failovers without manual intervention.
Available as a config option. [Learn more ->](https://portkey.ai/docs/product/ai-gateway/circuit-breaker)

```mermaid
flowchart TD
    A[User sends request] --> B[Gateway selects a strategy]
    B --> C[Strategy has multiple targets]
    C --> D{All targets OPEN?}
    D -->|Yes| E[Ignore circuit breaker]
    E --> F[Use all targets for routing]
    D -->|No| G["Use only healthy targets"]
    F --> H[Route to best target]
    G --> H
```

## OpenAI Agent SDK (TypeScript)

With the OpenAI Agents SDK (TS), building agents in TypeScript just got easier - no Python, no context switching, just native tooling and agent workflows.

But turning those prototypes into production-ready systems reveals real gaps:

- No logging or tracing
- No retries or failover
- No cost visibility or access control
- No simple way to switch LLM providers

Thatâ€™s where Portkey comes in. Route your calls through Portkey and unlock observability, guardrails, prompt versioning, and multi-provider support instantly. [Learn more ->](https://portkey.ai/docs/integrations/agents/openai-agents-ts)

## Strands Agents SDK

Enterprises building on Strands Agents SDK now have a cleaner path to production â€” thanks to Portkey as the LLM gateway.
With a single abstraction layer, you can:
- Access 2,000+ LLMs with provider-agnostic logic
- Standardize tool calling across providers
- Add conditional routing for cost control
- Enable retries, rate limits, and full observability

Huge thanks to Federico Kamelhar for leading the integration effort and bringing Portkey support to Strands.
If you love Strands & Portkey **[contribute to this PR](https://portkey.sh/L1tDBwa)** and help us stabilize this integration.

<a href="https://github.com/strands-agents/sdk-python/pull/197" target="_blank" rel="noopener">
  <img src="https://pbs.twimg.com/card_img/1939677278008684544/8r3mN30U?format=jpg&name=medium" alt="LinkedIn June Highlight" width="100%" style={{ borderRadius: "12px" }} />
</a>


## Gateway & Providers

**AWS Bedrock inference profiles**

You can now route Bedrock requests by inference profile, allowing access to multiple regions or compute configurations through a single virtual key. [Learn more -> ](https://portkey.ai/docs/integrations/llms/bedrock#inference-profiles).

**Vertex AI global endpoint support**

Portkey helps you improve model availability and reduce 429 (rate limit) errors with support for Vertex AIâ€™s global endpoints.
You can now set `region = global` in your Vertex AI Virtual Key config to automatically access Googleâ€™s distributed infrastructure â€” no manual region selection needed. 

**OpenAI Background Mode**

Reasoning models can take minutes to solve complex problems.
With background mode, you can now run long-running tasks on models like `o3-pro` and `o1-pro` reliably, without worrying about timeouts or dropped connections.

Portkey now supports background mode for OpenAI requests.
Simply pass `background:True` as a parameter, and Portkey will handle the rest.

**Computer Use tool support**

Experiment confidently with Anthropicâ€™s new Computer Use tool by adding observability, fallback logic, and cost controls from day one. [Learn more ->](https://portkey.ai/docs/integrations/libraries/anthropic-computer-use)

**Support for additional Azure OpenAI endpoints**

Portkey now supports a wider range of Azure OpenAI endpoints including image generation, audio transcription, speech synthesis, file management, and batch operations.

**Anthropic prompt caching (tools)**

Anthropic tool-based interactions now benefit from prompt caching in Portkey, improving performance and reducing token use for repeated tool calls.

**To keeping up the pace!**
<Frame>
  <img src="/images/changelog/june-test.png" alt="June Test" width="100%" style={{ borderRadius: "12px" }} />
</Frame>

## New models and providers

<div style={{ display: "flex", gap: "2rem", flexWrap: "wrap" }}>
  <div style={{ flex: 1, minWidth: 300 }}>
    <ul>
      <li><b>Sutra</b>: Multilingual LLM with standout MMLU scores in Hindi & Gujarati</li>
      <li><b>Magistral</b>: Mistralâ€™s first reasoning model, built for multilingual and domain-specific logic</li>
      <li><b>o3-pro</b>: OpenAIâ€™s latest flagship model with strong reasoning and fast response times</li>
      <li><b>Gemini 2.5 (Flash, Pro, Flash-Lite)</b>: Now GA and supported with full observability</li>
    </ul>
  </div>
  <div style={{ flex: 1, minWidth: 300 }}>
    <ul>
      <li><b>Kluster AI</b>: Claude-compatible, MCP-enabled models optimized for low latency and high availability. </li>
      <li><b>Hyperbolic AI</b>: OpenAI-compatible models focused on cost efficiency and speed for production-scale usage. </li>
      <li><b>Featherless AI</b>: Serverless access to Hugging Face models with a lightweight setup.</li>
      <li><b>Groq</b>: Support for <code>service_tier</code> flag added</li>
    </ul>
  </div>
</div>

## Integrations
<CardGroup cols={2}>
  <Card
    title="OpenAI Agent SDK (TypeScript)"
    icon="tool"
    href="https://portkey.ai/docs/integrations/agents/openai-agents-sdk"
  >
    Portkey now works with OpenAIâ€™s new TypeScript Agents SDK. Add retries, observability, rate limits, and multi-provider support to your agent flows, without changing core logic.
  </Card>
  <Card
    title="Langroid"
    icon="integration"
    href="https://portkey.ai/docs/integrations/agents/langroid"
  >
    Langroid now supports Portkey natively â€” plug it in to get multi-provider routing, observability, fallback logic, and guardrails in your agentic Python apps.
  </Card>
  <Card
    title="Cursor"
    icon="integration"
  >
    Bring visibility and governance to Cursorâ€™s coding assistant with Portkey. Choose from 1600+ models, track requests, enforce rate limits, and log usage, making it easy for enterprises to govern Cursor across the org.
  </Card>
  <Card
    title="Agent Development Kit (ADK)"
    icon="integration"
  >
    Portkey now integrates with Googleâ€™s Agent Development Kit (ADK), bringing production-grade features like retries, fallback logic, and observability to ADK-based agents.
  </Card>
</CardGroup>


## Partnerships
<Frame>
  <img src="/images/changelog/image(32).png" width="100%" style={{ borderRadius: "12px" }} />
</Frame>
- **Prompt Security** â€“ Secure every prompt and response in real time by embedding Prompt Security directly into Portkeyâ€™s AI Gateway. [Read more here](https://portkey.ai/blog/why-llm-security-is-non-negotiable/)

- **Lasso Security** â€“ Combine infra-level controls and real-time behavioral monitoring to secure the entire LLM lifecycle â€” from access to output. [Read more here](https://portkey.ai/blog/how-to-secure-your-entire-llm-lifecycle)

- **FutureAGI** â€“ Use Portkey as the control layer and FutureAGI as the eval layer to automate output scoring across all model traffic. [See how you can implement this](https://portkey.ai/docs/integrations/tracing-providers/future-agi)

- **Arize AI** â€“ Connect Portkeyâ€™s routing and guardrails with Arizeâ€™s observability to monitor model drift, latency, cost, and quality in one flow. [Read more here](https://portkey.ai/docs/integrations/tracing-providers/arize)

## Portkey Live!

In partnership with [**Pangea**](https://pangea.cloud/), we hosted a live webinar on how to build scalable, secure GenAI infrastructure. Catch the replay here!.
<Frame>
  <iframe
    width="700"
    height="394"
    src="https://www.youtube.com/embed/gUz5caRvtes"
    title="Portkey Live Webinar"
    frameBorder="0"
    allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
    allowFullScreen
  ></iframe>
</Frame>

**Improvements**
- Renamed Model Whitelist to Allowed Models for clarity and consistency
- Improved error responses for webhook failures, making them easier to debug and handle programmatically

<Frame>
  <img src="/images/changelog/image(33).png" width="100%" style={{ borderRadius: "12px" }} />
</Frame>

## Resources
- Cookbook: [Optimizing Prompts with LLama Prompt Ops](https://portkey.ai/docs/guides/prompts/llama-prompts)
- Cookbook: [OpenAI Computer Use Tool](https://portkey.ai/docs/guides/llms/openai-computer-use-tool)
- Blog: [Building AI agent workflows with the help of an MCP gateway](https://portkey.ai/blog/building-ai-agent-workflows-with-the-help-of-an-mcp-gateway/)
- Blog: [Balancing AI model accuracy, performance, and costs](https://portkey.ai/blog/balancing-model-accuracy-performance-and-costs-with-an-ai-gateway/)


## Community Contributors

A special thanks to our community contributors this month:
- [Shubhwithai](https://github.com/Shubhwithai)
- [DarinVerheijke](https://github.com/DarinVerheijke)
- [jroberts2600](https://github.com/jroberts2600)

## Coming this month!
Struggling with unauthorized tool usage in MCP? Portkey is about to solve that. Stay tuned.

## Support

<CardGroup cols={2}>
<Card title="Need Help?" icon="bug" href="https://github.com/Portkey-AI/gateway/issues">
Open an issue on GitHub
</Card>
<Card title="Join Us" icon="discord" href="https://portkey.wiki/community">
Get support in our Discord
</Card>
</CardGroup>
