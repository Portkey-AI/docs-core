---
title: "September"  
---

This was an eventful month at Portkey. We hosted and joined incredible events, from community gatherings like **LLMs in Prod in San Francisco**, to enterprise conversations with **Palo Alto Networks and PG&E**. A highlight was seeing **Syngentaâ€™s teams use Portkey** in their Devcon 2025 hackathon, experimenting with new AI workflows.

Each of these moments reinforced how fast enterprise AI is evolving, and how important it is to build responsibly at scale. 

Alongside the events, we shipped some of our most anticipated product updates yet, including the **MCP Gateway** and a series of new guardrails, routing improvements, and provider integrations, all designed to make it easier for teams to run AI in production with security, governance, and flexibility built in.

## Summary

| Area | Key Highlights |  
| :----------- | :------------------------------------------------------------------------------------------------------------------------------------------- |  
| **Platform** | â€¢ MCP Gateway beta <br /> â€¢ Unified token counting endpoint |
| **Guardrails** | â€¢ Metadata-based model access guardrail <br /> â€¢ Regex Replace Guardrail |
| **Gateway & Providers** | â€¢ Unified finish_reason <br /> â€¢ Conditional routing enhancement <br /> â€¢ Provider updates and improvements|
| **Community & Events** | â€¢ Palo Alto Networks webinar <br /> â€¢ LLMs in Prod, San Francisco <br /> â€¢ PG&E Immersion Day <br /> â€¢ Syngenta Devcon 2025 <br /> â€¢ MCP Salon |

---

## Platform

### MCP Gateway (beta)

Enterprises can now bring MCP servers and tools into production with governance, observability, and access controls built-in. The Gateway helps teams avoid authentication sprawl, shadow tool usage, and fragmented monitoringâ€”giving enterprises a central way to manage how agents interact with external tools.

<Frame>
  <iframe
    width="700"
    height="394"
    src="https://www.youtube.com/embed/RH7dKg94EAg?si=C-UNkR3Tnog1wrDZ"
    title="Portkey MCP Gateway Overview"
    frameBorder="0"
    allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
    allowFullScreen
  ></iframe>
</Frame>

Key benefits include:
- Centralized MCP tool access
- Unified authentication and credentials handling
- Complete visibility into agent-tool interactions
- Access controls and usage limits by team

ðŸ‘‰ To try out the beta, [send us an email](support@portkey.ai) or [book a demo](https://portkey.sh/sept-email).

### Unified token counting endpoint

Token counting is now unified across AWS Bedrock, Vertex AI, and Anthropic.

With a single endpoint, you can:
- Estimate token usage before sending a request (avoid context limit errors)
- Standardize cost/usage logic across providers
- Enforce routing and quota rules directly inside your app

[Read more here](https://portkey.ai/docs/integrations/llms/anthropic/count-tokens)

<img src="/images/changelog/unfied tokens endpoint.png" alt="Unified Token Counting Endpoint" style={{maxWidth: "80%", borderRadius: "8px", margin: "24px 0"}} />

This goes beyond dashboard reporting so developers can make smarter runtime decisions with accurate, provider-agnostic token counts.

## Guardrails

### Metadata-based Model Access guardrail

<img src="/images/changelog/Metadata-based model access guardrail.png" alt="Metadata-based Model Access Guardrail" style={{maxWidth: "80%", borderRadius: "8px", margin: "24px 0"}} />


We introduced a new guardrail that lets you restrict model access based on metadata key-value pairs at runtime.
By evaluating metadata dynamically on every request, this guardrail provides granular, per-request governanceâ€”without requiring changes to your app logic.

### Regex-replace guardrail

<img src="/images/changelog/regex-replace-guardrails.png" alt="Regex Replace Guardrail" style={{maxWidth: "80%", borderRadius: "8px", margin: "24px 0"}} />

Our PII guardrail already covers common fields like emails, phone numbers, and credit cards. But many customers asked for a way to redact custom org-specific patterns.

With the Regex Replace Guardrail, you can now:
- Define your own regex patterns
- Replace matches with a chosen string (e.g., [masked_user])
- Enforce masking rules at runtime, before data reaches the model

This is particularly useful for internal IDs, employee codes, or project references that shouldn't leave your environment. [Read more here](https://portkey.ai/docs/integrations/guardrails/regex).

## Gateway and Providers

### Unified `finish_reason` parameter

We standardized the `finish_reason` field across all providers. By default, values are mapped to OpenAI-compatible outputs, ensuring consistent handling across multi-provider deployments.

If you prefer to keep the original provider-returned value, set `x-portkey-strict-openai-compliance = false`.

### Conditional router enhancement

Conditional routing now supports **parameter-based** routing in addition to metadata. Parameter-based routing enables dynamic, per-request optimizations, giving you better performance, cost efficiency, and control over user experience. [Read more about this here](https://portkey.ai/docs/product/ai-gateway/conditional-routing#structure-of-conditions-object)

### Gateway and Providers
### New Models

<Card title="Claude Sonnet 4.5">
Anthropic's latest model, now available via Portkey
</Card>

<Card title="GPT-5 Codex">
OpenAI's specialized coding model, supported with full observability
</Card>

### New Providers

<Card title="Meshy">
Specialized 3D generation and design workflows provider
</Card>

<Card title="Tripo3D">
Next-generation 3D modeling and visualization
</Card>

<Card title="Cerebras">
High-performance AI inference provider offering up to 70x faster speeds than GPU-based solutions
</Card>

<Card title="Nextbit256">
New provider focused on efficient inference for specialized workloads
</Card>

### Improvements and Fixes

* **DashScope** â†’ Updated supported parameters

* **Vertex AI**:
  * Added `timeRangeFilter` support for Google Search tool
  * Added support for Mistral models
  * Added support for `task_type` and `dimensions` parameters in Vertex AI batch embeddings
  * Handle empty responses returned by the provider
  * Support for `global` region

* **Fireworks** â†’ Better handling of non-ASCII characters in file uploads, plus removed unnecessary response transforms for faster performance

* **OpenAI & Azure OpenAI**:
  * Added new parameters for GPT-5 compatibility
  * Updated the tokenizer to support streaming request token calculation for latest gpt-5 models

* **AWS Bedrock**:
  * Added `video` support in chat completions
  * Added support for `APAC` cross region inference profiles
  * Added support for `performance_config` parameter which will be passed as-is to the provider as `performanceConfig` parameter
  * Support Inference Profiles when uploading files for batches & finetuning
  * KMS Support for file uploads to AWS

* **Azure Foundry and Github** â†’ Updated the parameter mapping to support all the latest OpenAI compatible chat completions parameters

* **Adding new models to Azure OpenAI** â†’ Now much simpler. You just need to add the target URI, and the model will be fetched and added directly to your integration.

* **Azure Foundry** â†’ You can now enable multiple models in a single Azure Foundry integration. This simplifies management and makes it easier to experiment with different models under the same integration, without repetitive setup.

* Support custom scope for entra auth to use with deprecated azure serverless models
* Custom Header support for OTEL Export of analytics data


ðŸš¨ **Vertex AI deprecation** â†’ Llama 3.1 + 3.2 models will be retired on Jan 15, 2026. Recommended migration: Llama 3.3 or Llama 4.

## Community & Events

### Protecting your AI platform with Palo Alto Networks and Portkey
<Frame>
  <iframe
    width="700"
    height="394"
    src="https://www.youtube.com/embed/M4DC0KUJRcA?si=p5xq6h2Kb2IBiSzJ"
    title="Portkey Live Webinar"
    frameBorder="0"
    allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
    allowFullScreen
  ></iframe>
</Frame>
We partnered with Palo Alto Networks for a joint webinar on securing enterprise AI platforms with guardrails + gateway. The session highlighted the most pressing AI security risks (prompt injections, data leakage, compliance gaps) and how combining a gateway with guardrails can address them without slowing down scale.

### LLMs in Prod, San Francisco

<iframe
  src="https://www.linkedin.com/embed/feed/update/urn:li:activity:7378313507869622272"
  height="570"
  width="100%"
  frameBorder="0"
  allowFullScreen=""
  title="Embedded LinkedIn Post"
  style={{
    borderRadius: "8px",
    marginBottom: "20px"
  }}
></iframe>

### PG&E Immersion Day

We had the privilege of joining Pacfic Gas & Electricity for their internal Immersion Day. It was a hands-on session with their teams, exploring how enterprises can adopt AI responsibly and scale across critical operations.

### Syngenta Devcon 2025
Our customer Syngenta hosted Devcon 2025, centered on AI.

<img src="/images/changelog/devcon.jpeg" alt="devcon" style={{maxWidth: "100%", borderRadius: "8px", margin: "24px 0"}} />

[Syngentaâ€™s](https://www.linkedin.com/posts/miragirahul_devcon2025-syngenta-hackathon-activity-7377593161826603008-X_bn) teams ran a hackathon using Portkey + n8n, building creative workflows and experimenting with how MCP servers and digital apps can transform grower experiences. 
It was inspiring to see Portkey embedded directly into their innovation process, powering hands-on experimentation and ideation at scale.

### MCP Salon

We hosted some of the most illustrious MCP builders for a closed-door roundtable. The group went deep into the technical challenges of building MCP servers and clients, serving them in production, and solving real-world adoption hurdles.

ðŸ‘‰ To stay updated on upcoming events, subscribe to our [event calendar](https://luma.com/portkey?k=c)

## Resources
* **Blog**: [MCP Message Types: Complete MCP JSON-RPC Reference Guide](https://portkey.ai/blog/mcp-message-types-complete-json-rpc-reference-guide)
* **Blog**: [Failover routing strategies for LLMs in production](https://portkey.ai/blog/failover-routing-strategies-for-llms-in-production)
* **Partnership Blog with Feedback Intelligence**: [Tracing Failures from the LLM Call to the User Experience](https://portkey.ai/blog/tracing-failures-from-the-llm-call-to-the-user-experience)
* **Blog**: [A Strategic Perspective on the MCP Registry for the Enterprise](https://portkey.ai/blog/mcp-registry)

## Community Contributors
A special thanks to our contributor this month:
* [MarcNB256](https://github.com/MarcNB256)

## Coming this month!
Webinar - **LibreChat in Production** [Register here â†’](https://luma.com/cywhfpko)

## Support

<CardGroup cols={2}>
<Card title="Need Help?" icon="bug" href="https://github.com/Portkey-AI/gateway/issues">
Open an issue on GitHub
</Card>
<Card title="Join Us" icon="discord" href="https://portkey.wiki/community">
Get support in our Discord
</Card>
</CardGroup>