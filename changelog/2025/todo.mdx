To write standalone docs or make updates to docs for the following or do the GTM for the following:
- update openapi spec for chat completions with reasoning_effort and more stuff
- custom webhooks mutation capability
- make a simple guide for enterprise users - this is what would be different for you. for owners, org admins, workspace manager, and workspace members, and then people with service keys
- google search tool
- fine-tuning, files, batches- 
- parnter & pro guardrails have configurable timeouts
- conditional routing can now be done using request params
- cache status filter in the cache page
- New docs for logs export and replace the existing ones
- In the instrumentation docs, add a note for how to do it on self-hosted Portkey

-----------

Feb changelog:

Here are the updates for this month:
Gateway
- add support for reasoning_effort param in openai
- Acuvity portkey guardrail
- the gateway now by default caches your vertex generated token
- suport for upload file endpoint on azure, openai, vertex, bedrock, fireworks
- add support for google search tool on the gateway (google_search is a separate tool from google_search_retrieval and the newer models like gemini2.0-flash don't support google_search_retrieval
- Handle inconsistent usage object in vertex and google streaming responses - vertex ai returns an empty usage object in the very first streaming chunk. Expected behaviour: only the last chunk should contain a usage object
- handle tool_calls mapping in gemini responses when there is one part tool call and one part text
- Custom webhooks on the Gateway now also let you mutate the request/response bodies. Your webhook just needs to send a new transformedData object along with the verdict - we'll look for request or response bodies in there and if they exist, we will override the existing request or response body with the transformedData that your webhook sends - <this is an amazing feature />
- All Partner & Pro Guardrails now have configurable timeouts
- Fix: Azure OpenAI streaming responses now include the last chunk with `stream_options` that has the `usage`
- Unified Fine-tuning, Files, Batches API for OpenAI, Azure OpenAI, Google Vertex AI, AWS Bedrock, Fireworks AI. Batches API will also work with any provider that Portkey supports
- You can now conditionally route based on any of your request params and also modify the param value to any random string and define that at the conditional router level
- Multimodal requests on Vertex can now use 'webm'
Cookbooks
- [Track Costs Using Metadata](/guides/use-cases/track-costs-using-metadata)
- [Deepseek R1](/guides/use-cases/deepseek-r1)
- Prompt Engineering Cookbooks: https://portkey.ai/docs/guides/prompts, building an ultimate AI SDR: https://portkey.ai/docs/guides/prompts/ultimate-ai-sdr
New feature:
- PII Redaction 
- Grounding for Vertex & Gemini
- Secure, observe, and govern your LLM interactions on Zed for your entire team
- Integration with AnythingLLM
- Integration with JanHQ
- Portkey is now on Azure Marketplace: https://azuremarketplace.microsoft.com/en-in/marketplace/apps/portkey.enterprise-saas?tab=Overview
- Auto instrumentation for CrewAI & LangGraph and still retain all of Portkey features for interoperability, metering, governance, routing, and more.
Docs improvement: 
- Added all the erorrs that originate from Portkey: https://portkey.ai/docs/api-reference/inference-api/error-codes
- Add Default Configs to API Keys
- Prompt Engineering Studio is the main highlight launch of this month
- Change member roles in UI
- Create User key from UI
- Allow multiple Owners in a single org - done through UI
New models
- o3 models from OpenAI
- gemini 2 flash thinking
- Claude 3.7 Sonnet
- openai gpt 4.5
Community
- We attended the AI Engineering Summit in NYC - https://x.com/aiDotEngineer/status/1886419969526919564
- We chatted with swyx and Alessio from the Latent Space podcast - https://www.youtube.com/watch?v=-rSbvS0qLqY | Here are the takeaways: https://x.com/PortkeyAI/status/1887495934789230759

User Stories
- <img src="/images/changelog/testimonial-analytical-monk.png" />
- "Describing Portkey as merely useful would be an understatement; it's a must-have." - @AManInTech <img src="/images/changelog/aman-testimonial.jpeg" />

Our Stories:
- The State of AI FinOps 2025: Key Insights from FinOps Foundation's Latest Report- https://portkey.ai/blog/the-state-of-ai-finops-2025-key-insights-from-finops-foundations-latest-report/

January was a record-breaking month for Portkey. We saw unprecedented enterprise adoption, closing more enterprise deals in January alone than in the final months of 2024 combined.

What's even more thrilling is the scale of AI adoption we're enabling: 

- processed ~250M LLM calls in just the past week
- ~60% calls have fallbacks configured
- ~39% calls have load balancing or A/B Testing enabled
- a large percentage have atleast one runtime guardrail check

Cheers to our amazing team making it possible. Here‚Äôs to even bigger wins ahead! üçªüî•



Beyon all of these, some updates that are unique to Enterprises:
Updated cache implementation to avoid redundant Redis calls to improve overall performance.

SDK updates:
We now have support for running our SDK in the Browser.
We have also enabled Cross-Origin access for our APIs.

Now, shout out to Community Contributors:
- https://github.com/ethanknights
- Matthias Endler, https://github.com/mre

-------------------

March

highlight
- generic and special hi
- prompt engineering studio <take content for this from the platform section below />

Summary
- <Claude to create the table />

Platform
- Prompt Engineering Studio
    <img src="/images/changelog/prompt-engineering-studio.png" />
    Prompt engineering studio lets teams build, test, and deploy AI prompts at scale. Includes prompt playground, version control, collaborative editing, and AI gateway integration supporting 1600+ models and AI agent frameworks.

    After seeing teams struggle with inconsistent prompt performance and scattered tooling, we built the solution prompt engineers have been waiting for.
    Last month we launched the world's most advanced prompt engineering toolkit at prompts.new üöÄ

    Why we built this: Working with AI models requires precision in prompting, but existing solutions leave teams guessing with slow, manual processes and no version control.

    What makes our Portkey‚Äôs Prompt Studio special:
    - Powerful playground with side-by-side model comparison
    - Test across 1600+ AI models instantly
    - Version control with labeled deployments
    - Collaborative prompt libraries
    - Mustache templating and reusable partials
    - A-powered prompt optimization
    - Native support for LangGraph, CrewAI and other agent frameworksI

    Our platform helps teams craft, optimize, and deploy AI prompts 75% faster, from experimentation to production.
    We've put a lot care into building Portkey, and we really hope you like the product !

    P.S. we call the IDE for Prompt Engineers

- Major Launches:
    - Bring your own encryption key with AWS KMS [/product/enterprise-offering/kms]
        Customers can bring their own encryption keys to Portkey AI to encrypt their data at storage. Currently, supported for AWS KMS.
    - Enforce Org Level or Workspace Level Guardrails [/product/administration/enforce-orgnization-level-guardrails] [/product/administration/enforce-workspace-level-guardials]
        Portkey enables organization owners to enforce request guardrails at the organization level. This feature ensures that all API requests made within the organization comply with predefined policies, enhancing security, compliance, and governance.
        >> Note for Claude - let's specially highlight this because this is a highly useful feature for enterprises that want to run network level guardrails for all of their AI use cases. On Potykey they can do it at both levels - Organisation and Workspace Also, they can also do it at API key level as well with default configs on API keys - [/product/administration/enforce-default-config]
    - Enforce Org Level Metadata Requirements for Workspaces and API Keys
        Portkey allows organisation owners to define mandatory [metadata](/product/observability/metadata) fields that must be included with every API request. This feature enables granular observability across your organisation's LLM usage.
        >> Note for Claude - they can enforce this when someone creates a Workspace or an API key and mandate that it must have the required metadata fields
        <img src="/images/product/org-level-metadata.png" />
    - SCIM for Okta & Azure Entra (AD)
        Azure Entra for SCIM Provisioning [/product/enterprise-offering/org-management/scim/azure-ad]
        Okta for SCIM Provisioning [/product/enterprise-offering/org-management/scim/okta]
        Scim Docs: [/product/enterprise-offering/org-management/scim/scim]
    - Add Emails that get notified on hitting rate/budget/usage limits - through both UI & API
        >> Crucial feature that lets you add any and all email addresses that should be notified when you hard/soft hit your budget/usage/rate limits: [/product/administration/enforce-budget-and-rate-limit#email-notifications]
        <img src="/images/product/email.png" />
    - New filter in logs & analytics to look for requests based on cache status: Cache Hit, Cache Miss, Cache Disabled, Cache Semantic Hit
        <img src="/images/cache-filter.png" />

- More Updates:
    - Support for sending metadata to Vertex AI with their native "label" param on strict_openai_complliance mode to False: (ref: https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/add-labels-to-api-calls#what-are-labels)
    - Thinking mode support for Anthropic (Bedrock, Vertex), OpenAI, and more - on Gateway, streaming, prompt, observability
        - [Anthropic](/integrations/llms/anthropic#extended-thinking-reasoning-models-beta)
        - [Vertex](/integrations/llms/vertex-ai#extended-thinking-reasoning-models-beta)
    - You can now send image URLs to Claude models on Anthropic or Vertex or Bedrock APIs
        <img src="/images/changelog/claude-image-url.png" />
    - You can also send PDFs to your Claude requests now [https://docs.anthropic.com/en/docs/build-with-claude/pdf-support?q=pdf+su#process-pdfs-with-claude]
    - Support for OpenAI's new `developer` role in chat.completions
    
Enterprise
- Added AWS CloudFormation template for deploying Portkey Gateway on EC2, which Enables 1-click deployment of Portkey Gateway to EC2 instances [/product/enterprise-offering/cloud-marketplace/aws]
- Updated Enterprise Architecute [/product/enterprise-offering/private-cloud-deployments/architecture]
- Our API docs now let you see the code for enterprise deployments as well [/api-reference/inference-api/introduction]
- Create User key from UI 
    You can now assign a key to a user directly from UI
    <img src="/images/user-api-keys.png" />

Customer Love
- "Describing Portkey as merely useful would be an understatement; it's a must-have." - @AManInTech <img src="/images/changelog/aman-testimonial.jpeg" />


Integrations
- models
     - ncompass
     - Snowflake Cortex
- guardrails
    - lasso

Community
- resources
    - Building LLM as as judge cookbook - [/guides/prompts/llm-as-a-judge]
    - We published case study on how we went about designing the Prompt Engineering Studio: https://portkey.ai/blog/portkey-prompt-engineering-studio-a-user-centric-design-facelift/
    - OpenAI's new launch around Responses API and Agents SDK and toher tools is a critical time for companies to think about their Gen AI strategy. Portkey team discussed internally - https://portkey.ai/blog/openai-new-agent-tools-making-sense-of-the-evolving-ai-infrastructure-landscape/
- events

Testimonials
- Portkey is being evaluated for AI Gateway byNew York University, Lehigh University, Bowdoin College, Cornell University, Harvard University, Princeton University, and the University of California, Berkeley. More info here: https://internet2.edu/new-net-service-evaluations-for-ai-services/

Improvements
- Platform:
    - Fix: Bedrock Guardrails: Regexes are not getting triggered and logic is too cluttered, simplifying the logic.
    - Fix: Add handling to detect all errors in retry handler
    - Fix: handle null content for tool use assistant message for bedrock, To support multi-turn conversation with tool_calls for Bedrock
    - Improvement: changes to support cached streaming in grounding request, Added GroundingMetadata interface to manage grounding-related data.
    - Improvement: support search mode param from pplx, `web_search_options` with `strict_openai_compliance` set to False
    - Fix: When retry is not configured for the request and the provider responds with any of the default status codes [429, 500, 502, 503, 504], the response shows x-portkey-retry-attempt count as -1. Ideally, this header should be marked as -1 only when retry is configured for the request and all of the attempts are exhausted.
    - Fix: ‚úÖ fix: give preference to provider error code over hooks failure response code
    - Fix: Enhanced the afterRequestHookHandler to return a 246 status code for streaming responses when hooks fail
    - Fix: add logprobs support for fireworks
    - Support for sending metadata to Vertex AI with their native "label" param on strict_openai_complliance mode to False: https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/add-labels-to-api-calls#what-are-labels
- Enterprise:
    - Added a new metric (llm_last_byte_diff_duration_milliseconds) to track LLM last byte latency for chunked JSON responses.
    - Added a new label (stream) for all metrics. Possible values: 0/1
    - Added support for internal POD to POD HTTPS communication.
    - Real-Time Model Pricing Sync
        - Model pricing configs are no longer coupled with gateway builds.
        - For hybrid deployments, model pricing configs will be fetched from the control plane.
- SDK
    - Python SDK you can now send headers with extra_headers param inside any method, such as chat.completions
    - If you want to trace Llamaindex/Langchain calls with our instrumentation but your Portkey deployment is private - this is also supported now.

Now, Contributors:
https://github.com/urbanonymous
https://github.com/vineye25
Ignacio - https://github.com/elentaure
Ajay Satish - https://github.com/Ajay-Satish-01