To write standalone docs or make updates to docs for the following or do the GTM for the following:
- update openapi spec for chat completions with reasoning_effort and more stuff
- custom webhooks mutation capability
- make a simple guide for enterprise users - this is what would be different for you. for owners, org admins, workspace manager, and workspace members, and then people with service keys
- google search tool
- fine-tuning, files, batches- 
- parnter & pro guardrails have configurable timeouts
- conditional routing can now be done using request params
- cache status filter in the cache page
- New docs for logs export and replace the existing ones
- In the instrumentation docs, add a note for how to do it on self-hosted Portkey

-----------

Feb changelog:

Here are the updates for this month:
Gateway
- add support for reasoning_effort param in openai
- Acuvity portkey guardrail
- the gateway now by default caches your vertex generated token
- suport for upload file endpoint on azure, openai, vertex, bedrock, fireworks
- add support for google search tool on the gateway (google_search is a separate tool from google_search_retrieval and the newer models like gemini2.0-flash don't support google_search_retrieval
- Handle inconsistent usage object in vertex and google streaming responses - vertex ai returns an empty usage object in the very first streaming chunk. Expected behaviour: only the last chunk should contain a usage object
- handle tool_calls mapping in gemini responses when there is one part tool call and one part text
- Custom webhooks on the Gateway now also let you mutate the request/response bodies. Your webhook just needs to send a new transformedData object along with the verdict - we'll look for request or response bodies in there and if they exist, we will override the existing request or response body with the transformedData that your webhook sends - <this is an amazing feature />
- All Partner & Pro Guardrails now have configurable timeouts
- Fix: Azure OpenAI streaming responses now include the last chunk with `stream_options` that has the `usage`
- Unified Fine-tuning, Files, Batches API for OpenAI, Azure OpenAI, Google Vertex AI, AWS Bedrock, Fireworks AI. Batches API will also work with any provider that Portkey supports
- You can now conditionally route based on any of your request params and also modify the param value to any random string and define that at the conditional router level
- Multimodal requests on Vertex can now use 'webm'
Cookbooks
- [Track Costs Using Metadata](/guides/use-cases/track-costs-using-metadata)
- [Deepseek R1](/guides/use-cases/deepseek-r1)
- Prompt Engineering Cookbooks: https://portkey.ai/docs/guides/prompts, building an ultimate AI SDR: https://portkey.ai/docs/guides/prompts/ultimate-ai-sdr
New feature:
- PII Redaction 
- Grounding for Vertex & Gemini
- Secure, observe, and govern your LLM interactions on Zed for your entire team
- Integration with AnythingLLM
- Integration with JanHQ
- Portkey is now on Azure Marketplace: https://azuremarketplace.microsoft.com/en-in/marketplace/apps/portkey.enterprise-saas?tab=Overview
- Auto instrumentation for CrewAI & LangGraph and still retain all of Portkey features for interoperability, metering, governance, routing, and more.
Docs improvement: 
- Added all the erorrs that originate from Portkey: https://portkey.ai/docs/api-reference/inference-api/error-codes
- Add Default Configs to API Keys
- Prompt Engineering Studio is the main highlight launch of this month
- Change member roles in UI
- Create User key from UI
- Allow multiple Owners in a single org - done through UI
New models
- o3 models from OpenAI
- gemini 2 flash thinking
- Claude 3.7 Sonnet
- openai gpt 4.5
Community
- We attended the AI Engineering Summit in NYC - https://x.com/aiDotEngineer/status/1886419969526919564
- We chatted with swyx and Alessio from the Latent Space podcast - https://www.youtube.com/watch?v=-rSbvS0qLqY | Here are the takeaways: https://x.com/PortkeyAI/status/1887495934789230759

User Stories
- <img src="/images/changelog/testimonial-analytical-monk.png" />
- "Describing Portkey as merely useful would be an understatement; it's a must-have." - @AManInTech <img src="/images/changelog/aman-testimonial.jpeg" />

Our Stories:
- The State of AI FinOps 2025: Key Insights from FinOps Foundation's Latest Report- https://portkey.ai/blog/the-state-of-ai-finops-2025-key-insights-from-finops-foundations-latest-report/

January was a record-breaking month for Portkey. We saw unprecedented enterprise adoption, closing more enterprise deals in January alone than in the final months of 2024 combined.

What's even more thrilling is the scale of AI adoption we're enabling: 

- processed ~250M LLM calls in just the past week
- ~60% calls have fallbacks configured
- ~39% calls have load balancing or A/B Testing enabled
- a large percentage have atleast one runtime guardrail check

Cheers to our amazing team making it possible. Here‚Äôs to even bigger wins ahead! üçªüî•



Beyon all of these, some updates that are unique to Enterprises:
Updated cache implementation to avoid redundant Redis calls to improve overall performance.

SDK updates:
We now have support for running our SDK in the Browser.
We have also enabled Cross-Origin access for our APIs.

Now, shout out to Community Contributors:
- https://github.com/ethanknights
- Matthias Endler, https://github.com/mre

-------------------

March Changelog:

- You can now send image URLs to Claude models on Anthropic or Vertex or Bedrock APIs
- Added AWS CloudFormation template for deploying Portkey Gateway on EC2, which Enables 1-click deployment of Portkey Gateway to EC2 instances
- New provider: ncompass
- Support for sending metadata to Vertex AI with their native "label" param on strict_openai_complliance mode to False: https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/add-labels-to-api-calls#what-are-labels
- New headers for default input and output guardrails: https://github.com/Portkey-AI/gateway/pull/986 supported over the API & SDK
- Thinking mode support for Anthropic (Bedrock, Vertex), OpenAI, and more - on Gateway, streaming, prompt, observability
- New provider: Snowflake Cortex
- Fix: Bedrock Guardrails: Regexes are not getting triggered and logic is too cluttered, simplifying the logic.
- Fix: Add handling to detect all errors in retry handler
- Fix: handle null content for tool use assistant message for bedrock, To support multi-turn conversation with tool_calls for Bedrock
- Improvement: changes to support cached streaming in grounding request, Added GroundingMetadata interface to manage grounding-related data.
- Improvement: support search mode param from pplx, `web_search_options` with `strict_openai_compliance` set to False
- Fix: Enhanced the afterRequestHookHandler to return a 246 status code for streaming responses when hooks fail
- Fix: add logprobs support for fireworks
- Fix: When retry is not configured for the request and the provider responds with any of the default status codes [429, 500, 502, 503, 504], the response shows x-portkey-retry-attempt count as -1. Ideally, this header should be marked as -1 only when retry is configured for the request and all of the attempts are exhausted.
- Fix: ‚úÖ fix: give preference to provider error code over hooks failure response code
- New Guardrails: AWS Bedrock, Acuvity
- Bring your own encryption key with AWS KMS
- Enforce Org Level or Workspace Level Guardrails
- Enforce Org Level Metadata Requirements for Workspaces and API Keys
- SCIM for Okta & Azure Entra (AD)
- Prompt Docs Revamp
Cookbook
- Building LLM as as judge
- Introduction to our Admin or Control Plane API: https://portkey.ai/docs/api-reference/admin-api/introduction
- Support for OpenAI's new developer role
- Add Emails that get notified on hitting rate/budget/usage limits - through both UI & API
- Send PDF to your Claude requests now supported on Portkey
- Mistral Moderations endpoint added as a guardrail
- New format for setting Guardrails with Input/Output Guardrails
- New Guardrail provider: Lasso
- Updated Enterprise Architecute: https://portkey.ai/docs/product/enterprise-offering/private-cloud-deployments/architecture
- Our API docs now let you see the code for enterprise deployments as well
- Create User key from UI
- New filter in logs & analytics to look for requests based on cache status: Cache Hit, Cache Miss, Cache Disabled, Cache Semantic Hit
- Prompt Engineering Studio is the main highlight launch of this month
- "Describing Portkey as merely useful would be an understatement; it's a must-have." - @AManInTech <img src="/images/changelog/aman-testimonial.jpeg" />
- Portkey is being evaluated for AI Gateway byNew York University, Lehigh University, Bowdoin College, Cornell University, Harvard University, Princeton University, and the University of California, Berkeley. More info here: https://internet2.edu/new-net-service-evaluations-for-ai-services/

Our Stories:
- We published case study on how we went about designing the Prompt Engineering Studio: https://portkey.ai/blog/portkey-prompt-engineering-studio-a-user-centric-design-facelift/
- OpenAI's new launch around Responses API and Agents SDK and toher tools is a critical time for companies to think about their Gen AI strategy. Portkey team discussed internall

Beyond all of these, some updates that are unique to Enterprise deployments only

Real-Time Model Pricing Sync
- Model pricing configs are no longer coupled with gateway builds.
- For hybrid deployments, model pricing configs will be fetched from the control plane.

Added a new metric (llm_last_byte_diff_duration_milliseconds) to track LLM last byte latency for chunked JSON responses.
Added a new label (stream) for all metrics. Possible values: 0/1
Added support for internal POD to POD HTTPS communication.

SDK Updates:
- Python SDK you can now send headers with extra_headers param inside any method, such as chat.completions
- If you want to trace Llamaindex/Langchain calls with our instrumentation but your Portkey deployment is private - this is also supported now.

Now, Contributors:
https://github.com/urbanonymous
https://github.com/vineye25
Ignacio - https://github.com/elentaure
Ajay Satish - https://github.com/Ajay-Satish-01