---
title: "Model Catalog"
description: "A single pane to view and manage every AI provider and model in your organization. It merges and supersedes Virtual Keys - providing centralized governance, discovery, and usage. "
sidebarTitle: "Model Catalog"
---

The Model Catalog is a centralized hub for viewing and managing all AI providers and models within your organization. It serves as the evolution of Virtual Keys, providing a more powerful and streamlined way to control your AI resources.

It abstracts raw API keys and scattered environment variables into governed Provider Integrations and Models.

<Tip>
  **Upgrading from Virtual Keys**\
  The Model Catalog upgrades the Virtual Key experience by introducing a centralized, organization-level management layer, offering advantages like:

  - Centralized provider and model management - no more duplicate configs across workspaces.
  - Fine-grained control: budgets, rate limits, and model allow-lists at both org and workspace level.
  - Inline usage: just pass `model="@provider/model_slug"`

  **Need help?** See our [Migration Guide ➜](/support/upgrade-to-model-catalog)
</Tip>

![Model Catalog - Provider and Models](/providersandmodels.gif)

<CardGroup cols={2}>
  <Card title="AI Providers" icon="sparkles" color="#444">
    AI Providers represent connections to AI services. Each AI Provider has:

    - 
    - ✔️ A unique slug (e.g., `@openai-prod`)
    - ✔️ Securely stored credentials
    - ✔️ Budget and rate limits
    - ✔️ Access to specific models
  </Card>
  <Card title="AI Models" icon="microchip-ai" iconType="regular" color="#444">
    The Models section is a gallery of all AI models available. Each Model entry includes:

    - 
    - ✔️ Model slug (`@openai-prod/gpt-4o`)
    - ✔️ Ready-to-use code snippets
    - ✔️ Input/output token limits
    - ✔️ Pricing information (where available)
  </Card>
</CardGroup>

## Adding an AI Provider

You can add providers via \*\*UI \*\*(follow the steps below) or [**API**](/api-reference/admin-api/introduction).

<Steps>
  <Step title="Go to AI Providers → Add Provider">
    ![Portkey Model Catalog - Add Provider](/Screenshot2025-07-21at5.29.57PM.png)
  </Step>
  <Step title="Select the AI Service to integrate">
    Choose from list (OpenAI, Anthropic, etc.) or _Self-hosted / Custom_.

    ![Portkey Model Catalog - Add Provider - Choose Service](/Screenshot2025-07-21at5.31.49PM.png)
  </Step>
  <Step title="Enter Credentials">
    Choose existing credentials or create new ones.

    ![Model Catalog - Add credentials](/Screenshot2025-07-21at5.34.13PM.png)
  </Step>
  <Step title="Enter provider details & save">
    Choose the name and slug for this provider. The slug cannot be changed later and will be used to reference the AI models.

    ![Model Catalog - Add Provider Details](/Screenshot2025-07-21at5.36.14PM.png)
  </Step>
</Steps>

## Using Provider Models

Once you have AI Providers set up, you can use their models in your applications through various methods.

### 1. Model String Composition (Recommended)

In Portkey, model strings follow this format:

`@provider_slug/model_name`

<img
  src="/Screenshot2025-07-21at5.39.59PM.png"
  alt="Screenshot2025 07 21at5 39 59PM Pn"
  title="Screenshot2025 07 21at5 39 59PM Pn"
  className="mr-auto"
  style={{ width:"59%" }}
/>

For example, `@openai-prod/gpt-4o`, `@anthropic/claude-sonnet-3.7`, `@bedrock-us/claude-3-sonnet-v1`

<CodeGroup>

```javascript Javascript SDK
import { Portkey } from 'portkey-ai';
const client = new Portkey({ apiKey: "PORTKEY_API_KEY" });

const resp = await client.chat.completions.create({
  model: '@openai-prod/gpt-4o',
  messages: [{ role: 'user', content: 'Hello!' }]
});
```


```python Python SDK
from portkey_ai import Portkey

portkey = Portkey(api_key="PORTKEY_API_KEY")

resp = portkey.chat.completions.create(
  model="@openai-prod/gpt-4o",
  messages=[{"role":"user","content":"Hello"}]
)
```


```python OpenAI Python SDK
from openai import OpenAI

client = OpenAI(api_key="PORTKEY_API_KEY", base_url="https://api.portkey.ai/v1")

client.chat.completions.create(
	model="@openai-prod/gpt-4o", 
	messages=[…]
)
```


```javascript OpenAI JS SDK
import OpenAI from 'openai';

const openai = new OpenAI({
    apiKey: "PORTKEY_API_KEY",
    baseURL: "https://api.portkey.ai/v1"
});

const completion = await openai.chat.completions.create({
    model: "@openai-prod/gpt-4o",
    messages: [{ role: 'user', content: 'Hello!' }]
});
```


```bash cURL
curl https://api.portkey.ai/v1/chat/completions \
  -H "Content-Type: application/json" \
  -H "x-portkey-api-key: $PORTKEY_API_KEY" \
  -d '{
    "model": "@openai-prod/gpt-4o",
    "messages": [{"role": "user", "content": "Hello!"}]
  }'
```

</CodeGroup>

### 2. Using the `provider` header

You can also specify the provider in the header instead of the model string like the earlier virtual keys approach. Remember to add the `@` before your provider slug.

<CodeGroup>

```javascript Javascript SDK
import { Portkey } from 'portkey-ai';
const client = new Portkey({ 
	apiKey: "PORTKEY_API_KEY",
	provider: "@openai-prod"
});

const resp = await client.chat.completions.create({
  model: 'gpt-4o',
  messages: [{ role: 'user', content: 'Hello!' }]
});
```


```python Python SDK
from portkey_ai import Portkey

portkey = Portkey(
	api_key="PORTKEY_API_KEY",
	provider="@openai-prod"
)

resp = portkey.chat.completions.create(
  model="gpt-4o",
  messages=[{"role":"user","content":"Hello"}]
)
```


```python OpenAI Python SDK
from openai import OpenAI
from portkey_ai import createHeaders

client = OpenAI(
	api_key="PORTKEY_API_KEY", 
	base_url="https://api.portkey.ai/v1",
	default_headers=createHeaders(
		provider="@openai-prod"
	)
)

client.chat.completions.create(
	model="gpt-4o", 
	messages=[…]
)
```


```javascript OpenAI JS SDK
import OpenAI from 'openai';
import { createHeaders } from 'portkey-ai'

const openai = new OpenAI({
    apiKey: "PORTKEY_API_KEY",
    baseURL: "https://api.portkey.ai/v1",
	defaultHeaders: {
		provider: "@openai-prod"
	}
});

const completion = await openai.chat.completions.create({
    model: "gpt-4o",
    messages: [{ role: 'user', content: 'Hello!' }]
});
```


```bash cURL
curl https://api.portkey.ai/v1/chat/completions \
  -H "Content-Type: application/json" \
  -H "x-portkey-api-key: $PORTKEY_API_KEY" \
  -H "x-portkey-provider": "@openai-prod" \
  -d '{
    "model": "gpt-4o",
    "messages": [{"role": "user", "content": "Hello!"}]
  }'
```

</CodeGroup>

### 3. Specify `provider` in the config

Specifying the `provider` in the [config](/product/ai-gateway/configs) will also set it for all requests using that config. You can also specify the model string format in the override params.

```json
// Specify provider in the config
{
	"provider": "@openai-prod"
}

// and/or specify the model string in "override_params"
{
	"strategy": { "mode": "fallback" },
	"targets": [{
		"override_params": { "model": "@openai-prod/gpt-4o" }
	}, {
		"override_params": { "model": "@anthropic/claude-sonnet-3.7" }
	}]
}
```

> **Ordering:** `config` (if provided) defines base; `override_params` merges on top (last write wins for scalars, deep merge for objects like `metadata`).

## Integrations and Workspaces

The Model Catalog enables seamless integration across your organization's structure:

- **Organization-Level**: Create and manage integrations centrally
- **Workspace-Level**: Provision specific integrations to workspaces
- **Developer-Level**: Use provisioned models through simple API calls

This hierarchical approach provides central governance while giving workspaces the flexibility they need.

<Card title="Learn More: Integrations">
  Admins can manage AI service credentials across workspaces easily through integrations. Click to learn more about using this.
</Card>

## Budgets & Limits

Portkey allows you to set and manage budget limits at various levels:

- **Workspace-Level**: Set specific budgets for each workspace
- **Provider-Level**: Set budgets for individual AI Providers

Budget limits can be:

- **Cost-Based**: Set a maximum spend in USD
- **Token-Based**: Set a maximum number of tokens that can be consumed
- **Rate-Based**: Set maximum requests per minute/hour/day

You can also configure periodic resets (weekly or monthly) for these limits, which is perfect for managing recurring team budgets.

[Learn more about Budgets and Limits here](/product/administration/enforce-budget-and-rate-limit).

## Model Management

<Card title="Custom Models">
  You can manage your own custom models in Model Catalog, including fine-tuned models, custom-hosted models and private models. Click to see how to create custom models.
</Card>

<Card title="Custom Pricing">
  For models with custom pricing arrangements, you can configure input and output token pricing at an integration level.. Click to see how to add custom pricing for models.
</Card>

## Self-hosted AI Providers

TBD