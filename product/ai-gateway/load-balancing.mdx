---
title: "Load Balancing"
description: Distribute traffic across multiple LLMs for high availability and optimal performance.
---

<Info>
Available on all Portkey [plans](https://portkey.ai/pricing).
</Info>

Distribute traffic across multiple LLMs to prevent any single provider from becoming a bottleneck.

## Examples

<CodeGroup>

```json Between Providers (model from request)
{
  "strategy": { "mode": "loadbalance" },
  "targets": [
    { "provider": "@openai-prod", "weight": 0.7 },
    { "provider": "@azure-prod", "weight": 0.3 }
  ]
}
```

```json Between Models
{
  "strategy": { "mode": "loadbalance" },
  "targets": [
    { "override_params": { "model": "@openai-prod/gpt-4o" }, "weight": 0.75 },
    { "override_params": { "model": "@anthropic-prod/claude-3-5-sonnet-20241022" }, "weight": 0.25 }
  ]
}
```

```json Multiple API Keys (same provider)
{
  "strategy": { "mode": "loadbalance" },
  "targets": [
    { "provider": "@openai-key-1", "weight": 1 },
    { "provider": "@openai-key-2", "weight": 1 },
    { "provider": "@openai-key-3", "weight": 1 }
  ]
}
```

```json Cost Optimization (cheap vs premium)
{
  "strategy": { "mode": "loadbalance" },
  "targets": [
    { "override_params": { "model": "@openai-prod/gpt-4o-mini" }, "weight": 0.8 },
    { "override_params": { "model": "@openai-prod/gpt-4o" }, "weight": 0.2 }
  ]
}
```

```json Gradual Migration (old to new model)
{
  "strategy": { "mode": "loadbalance" },
  "targets": [
    { "override_params": { "model": "@anthropic-prod/claude-3-5-sonnet-20241022" }, "weight": 0.9 },
    { "override_params": { "model": "@anthropic-prod/claude-sonnet-4-20250514" }, "weight": 0.1 }
  ]
}
```

</CodeGroup>

| Pattern | Use Case |
|---------|----------|
| **Between Providers** | Route to different providers; model comes from request |
| **Multiple API Keys** | Distribute load across rate limits from different accounts |
| **Cost Optimization** | Send most traffic to cheaper models, reserve premium for a portion |
| **Gradual Migration** | Test new models with small percentage before full rollout |

<Info>
The `@provider-slug/model-name` format automatically routes to the correct provider. Set up providers in [Model Catalog](https://app.portkey.ai/model-catalog).
</Info>

[Create](/product/ai-gateway/configs#creating-configs) and [use](/product/ai-gateway/configs#using-configs) configs in your requests.

## How It Works

1. **Define targets & weights** — Assign a `weight` to each target. Weights represent relative share of traffic.
2. **Weight normalization** — Portkey normalizes weights to sum to 100%. Example: weights 5, 3, 1 become 55%, 33%, 11%.
3. **Request distribution** — Each request routes to a target based on normalized probabilities.

<Info>
- Default `weight`: `1`
- Minimum `weight`: `0` (stops traffic without removing from config)
- Unset weights default to `1`
</Info>

## Considerations

- Ensure LLMs in your list are compatible with your use case
- Monitor usage per LLM—weight distribution affects spend
- Each LLM has different latency and pricing
