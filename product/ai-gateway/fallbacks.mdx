---
title: "Fallbacks"
description: Automatically switch to backup LLMs when the primary fails.
---

<Info>
Available on all Portkey [plans](https://portkey.ai/pricing).
</Info>

Specify a prioritized list of providers/models. If the primary LLM fails, Portkey automatically falls back to the next in line.

<Frame>
  <img src="/images/product/ai-gateway/ai-12.avif"/>
</Frame>

## Examples

<CodeGroup>

```json Between Models
{
  "strategy": { "mode": "fallback" },
  "targets": [
    { "override_params": { "model": "@openai-prod/gpt-4o" } },
    { "override_params": { "model": "@anthropic-prod/claude-3-5-sonnet-20241022" } }
  ]
}
```

```json Between Providers (model from request)
{
  "strategy": { "mode": "fallback" },
  "targets": [
    { "provider": "@openai-prod" },
    { "provider": "@azure-prod" }
  ]
}
```

```json On Rate Limit Only (429)
{
  "strategy": { "mode": "fallback", "on_status_codes": [429] },
  "targets": [
    { "provider": "@openai-prod" },
    { "provider": "@azure-prod" }
  ]
}
```

```json Multi-tier Fallback
{
  "strategy": { "mode": "fallback" },
  "targets": [
    { "override_params": { "model": "@openai-prod/gpt-4o" } },
    { "override_params": { "model": "@anthropic-prod/claude-3-5-sonnet-20241022" } },
    { "override_params": { "model": "@google-prod/gemini-1.5-pro" } }
  ]
}
```

</CodeGroup>

<Info>
The `@provider-slug/model-name` format automatically routes to the correct provider. Set up providers in [Model Catalog](https://app.portkey.ai/model-catalog).
</Info>

[Create](/product/ai-gateway/configs#creating-configs) and [use](/product/ai-gateway/configs#using-configs) configs in your requests.

## Trigger on Specific Status Codes

By default, fallback triggers on any **non-2xx** status code.

Customize with `on_status_codes`:

```json
{
  "strategy": { "mode": "fallback", "on_status_codes": [429, 503] },
  "targets": [
    { "provider": "@openai-prod" },
    { "provider": "@azure-prod" }
  ]
}
```

## Tracing Fallback Requests

Portkey logs all requests in a fallback chain. To trace:

1. Filter logs by `Config ID` to see all requests using that config
2. Filter by `Trace ID` to see all attempts for a single request

<Frame>
  <img src="/images/product/ai-gateway/ai-13.avif"/>
</Frame>

## Considerations

- Ensure fallback LLMs are compatible with your use case
- A single request may invoke multiple LLMs
- Each LLM has different latency and pricing
