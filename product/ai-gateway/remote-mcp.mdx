---
title: "Remote MCP"
description: Portkey's AI gateway has MCP server support that many foundational model providers offer.
---

[Model Context Protocol](https://modelcontextprotocol.io/introduction) (MCP) is an open protocol that standardizes how applications provide tools and context to LLMs. The MCP tool in the Responses API allows developers to give the model access to tools hosted on **Remote MCP servers**. These are MCP servers maintained by developers and organizations across the internet that expose these tools to MCP clients, like the Responses API.


Portkey Supports using MCP server via the Response API. Calling a remote MCP server with the Responses API is straightforward. For example, here's how you can use the [DeepWiki](https://deepwiki.com/) MCP server to ask questions about nearly any public GitHub repository.



Example MCP request
--------------


A Responses API request to OpenAI with MCP tools enabled.

<CodeGroup>
```bash cURL
curl https://api.portkey.ai/v1/responses \
  -H "Content-Type: application/json" \
  -H "x-portkey-api-key: $PORTKEY_API_KEY" \
  -d '{
    "model": "@OPENAI_PROVIDER/gpt-4.1",
    "tools": [
      {
        "type": "mcp",
        "server_label": "deepwiki",
        "server_url": "https://mcp.deepwiki.com/mcp",
        "require_approval": "never"
      }
    ],
    "input": "What transport protocols are supported in the 2025-03-26 version of the MCP spec?"
  }'
```

```javascript OpenAI Node SDK
import OpenAI from "openai";
import { PORTKEY_GATEWAY_URL, createHeaders } from 'portkey-ai'

const client = new OpenAI({
  apiKey: "PORTKEY_API_KEY",
  baseURL: PORTKEY_GATEWAY_URL
});

const resp = await client.responses.create({
  model: "@OPENAI_PROVIDER/gpt-4.1",
  tools: [
    {
      type: "mcp",
      server_label: "deepwiki",
      server_url: "https://mcp.deepwiki.com/mcp",
      require_approval: "never",
    },
  ],
  input: "What transport protocols are supported in the 2025-03-26 version of the MCP spec?",
});

console.log(resp.output_text);
```

```python OpenAI Python SDK
from openai import OpenAI
from portkey_ai import PORTKEY_GATEWAY_URL, createHeaders

client = OpenAI(
  api_key="PORTKEY_API_KEY",
  base_url=PORTKEY_GATEWAY_URL,
)

resp = client.responses.create(
  model="@OPENAI_PROVIDER/gpt-4.1",
  tools=[
    {
      "type": "mcp",
      "server_label": "deepwiki",
      "server_url": "https://mcp.deepwiki.com/mcp",
      "require_approval": "never",
    },
  ],
  input="What transport protocols are supported in the 2025-03-26 version of the MCP spec?",
)

print(resp.output_text)
```

```typescript Portkey Node SDK
import Portkey from 'portkey-ai';

const portkey = new Portkey({
  apiKey: "PORTKEY_API_KEY"
});

const resp = await portkey.responses.create({
  model: "@OPENAI_PROVIDER/gpt-4.1",
  tools: [
    {
      type: "mcp",
      server_label: "deepwiki",
      server_url: "https://mcp.deepwiki.com/mcp",
      require_approval: "never",
    },
  ],
  input: "What transport protocols are supported in the 2025-03-26 version of the MCP spec?",
});

console.log(resp.output_text);
```

```python Portkey Python SDK
from portkey_ai import Portkey

portkey = Portkey(
  api_key="PORTKEY_API_KEY"
)

resp = portkey.responses.create(
  model="@OPENAI_PROVIDER/gpt-4.1",
  tools=[
    {
      "type": "mcp",
      "server_label": "deepwiki",
      "server_url": "https://mcp.deepwiki.com/mcp",
      "require_approval": "never",
    },
  ],
  input="What transport protocols are supported in the 2025-03-26 version of the MCP spec?",
)

print(resp.output_text)
```
</CodeGroup>


<Frame caption="Example Log for the request on Portkey">
  <img src="/images/product/ai-gateway/mcp-remote-logs-image.png" />
</Frame>


MCP Server Authentication
--------------

Unlike the DeepWiki MCP server, most other MCP servers require authentication. The MCP tool in the Responses API gives you the ability to flexibly specify headers that should be included in any request made to a remote MCP server. These headers can be used to share API keys, oAuth access tokens, or any other authentication scheme the remote MCP server implements.

The most common header used by remote MCP servers is the `Authorization` header. This is what passing this header looks like:

Use Stripe MCP tool

<CodeGroup>
```bash cURL
curl https://api.portkey.ai/v1/responses \
  -H "Content-Type: application/json" \
  -H "x-portkey-api-key: $PORTKEY_API_KEY" \
  -d '{
    "model": "@OPENAI_PROVIDER/gpt-4.1",
    "input": "Create a payment link for $20",
    "tools": [
      {
        "type": "mcp",
        "server_label": "stripe",
        "server_url": "https://mcp.stripe.com",
        "headers": {
          "Authorization": "Bearer $STRIPE_API_KEY"
        }
      }
    ]
  }'
```

```javascript OpenAI Node SDK
import OpenAI from "openai";
import { PORTKEY_GATEWAY_URL } from 'portkey-ai'

const client = new OpenAI({
  apiKey: "PORTKEY_API_KEY",
  baseURL: PORTKEY_GATEWAY_URL
});

const resp = await client.responses.create({
  model: "@OPENAI_PROVIDER/gpt-4.1",
  input: "Create a payment link for $20",
  tools: [
    {
      type: "mcp",
      server_label: "stripe",
      server_url: "https://mcp.stripe.com",
      headers: {
        Authorization: "Bearer $STRIPE_API_KEY"
      }
    }
  ]
});

console.log(resp.output_text);
```

```python OpenAI Python SDK
from openai import OpenAI
from portkey_ai import PORTKEY_GATEWAY_URL

client = OpenAI(
  api_key="PORTKEY_API_KEY",
  base_url=PORTKEY_GATEWAY_URL
)

resp = client.responses.create(
  model="@OPENAI_PROVIDER/gpt-4.1",
  input="Create a payment link for $20",
  tools=[
    {
      "type": "mcp",
      "server_label": "stripe",
      "server_url": "https://mcp.stripe.com",
      "headers": {
        "Authorization": "Bearer $STRIPE_API_KEY"
      }
    }
  ]
)

print(resp.output_text)
```

```typescript Portkey Node SDK
import Portkey from 'portkey-ai';

const portkey = new Portkey({
  apiKey: "PORTKEY_API_KEY"
});

const resp = await portkey.responses.create({
  model: "@OPENAI_PROVIDER/gpt-4.1",
  input: "Create a payment link for $20",
  tools: [
    {
      type: "mcp",
      server_label: "stripe",
      server_url: "https://mcp.stripe.com",
      headers: {
        Authorization: "Bearer $STRIPE_API_KEY"
      }
    }
  ]
});

console.log(resp.output_text);
```

```python Portkey Python SDK
from portkey_ai import Portkey

portkey = Portkey(
  api_key="PORTKEY_API_KEY"
)

resp = portkey.responses.create(
  model="@OPENAI_PROVIDER/gpt-4.1",
  input="Create a payment link for $20",
  tools=[
    {
      "type": "mcp",
      "server_label": "stripe",
      "server_url": "https://mcp.stripe.com",
      "headers": {
        "Authorization": "Bearer $STRIPE_API_KEY"
      }
    }
  ]
)

print(resp.output_text)
```
</CodeGroup>

To prevent the leakage of sensitive keys, the Responses API does not store the values of **any** string you provide in the `headers` object. These values will also not be visible in the Response object created. Additionally, because some remote MCP servers generate authenticated URLs, we also discard the _path_ portion of the `server_url` in our responses (i.e. `example.com/mcp` becomes `example.com`). Because of this, you must send the full path of the MCP `server_url` and any relevant `headers` in every Responses API creation request you make.




---


## Using Remote MCP with Anthropic

Portkey now supports Remote MCP through Anthropic's Messages API format. This allows you to use MCP tools with models from **Anthropic, Amazon Bedrock, and Google Vertex** via the Portkey gateway.

You can enable MCP tools by passing a `tools` array in your request, just like with OpenAI. Portkey handles the translation to the appropriate provider format.

### Example MCP Request

An Anthropic Messages API request with the DeepWiki MCP tool enabled.

<CodeGroup>
```bash cURL
curl https://api.portkey.ai/v1/messages \
  -H "Content-Type: application/json" \
  -H "x-portkey-api-key: $PORTKEY_API_KEY" \
  -H "x-portkey-provider: anthropic" \
  -H "anthropic-beta: mcp-client-2025-04-04" \
  -d '{
    "model": "@your-provider-slug/model-name",
    "max_tokens": 1000,
    "messages": [{"role": "user", "content": "What tools do you have available?"}],
    "mcp_servers": [
      {
        "type": "url",
        "url": "https://example-server.modelcontextprotocol.io/sse",
        "name": "example-mcp",
        "authorization_token": "YOUR_TOKEN"
      }
    ]
  }'
```





```python Anthropic Python SDK
import anthropic

client = anthropic.Anthropic(
    api_key="dummy", # We will use portkey's provider slug
    base_url="https://api.portkey.ai/v1",
    default_headers={"x-portkey-api-key": "YOUR_PORTKEY_API_KEY"}
)

response = client.beta.messages.create(
    model="@your-provider-slug/your-model-name",
    max_tokens=1000,
    messages=[{
        "role": "user",
        "content": "What tools do you have available?"
    }],
    mcp_servers=[{
        "type": "url",
        "url": "https://mcp.example.com/sse",
        "name": "example-mcp",
        "authorization_token": "YOUR_TOKEN"
    }],
    betas=["mcp-client-2025-04-04"]
)

print(response.content)
```

```typescript Anthropic TS SDK
import Anthropic from '@anthropic-ai/sdk';

const anthropic = new Anthropic({
  apiKey: 'ANTHROPIC_API_KEY', // We will use portkey's provider slug
  baseURL: "https://api.portkey.ai/v1",
  defaultHeaders: { "x-portkey-api-key": "YOUR_PORTKEY_API_KEY" }
});

const msg = await anthropic.messages.create({
  model: "@your-provider-slug/your-model-name",
  max_tokens: 1024,
  messages: [{ role: "user", content: "What transport protocols are supported in the 2025-03-26 version of the MCP spec?" }],
  tools: [
    {
      type: "mcp",
      server_label: "deepwiki",
      server_url: "https://mcp.deepwiki.com/mcp",
      require_approval: "never",
    },
  ],
});
console.log(msg.content);
```
</CodeGroup>

### MCP Server Authentication

Authenticating with a private MCP server follows the same pattern. Simply include the `headers` object within the tool definition to pass credentials like an `Authorization` token.

Here is how you would use the Stripe MCP tool with an Anthropic model.

<CodeGroup>
```bash cURL
curl https://api.portkey.ai/v1/messages \
  -H "Content-Type: application/json" \
  -H "x-portkey-api-key: $PORTKEY_API_KEY" \
  -H "x-portkey-provider: anthropic" \
  -H "anthropic-beta: mcp-client-2025-04-04" \
  -d '{
    "model": "@your-provider-slug/model-name",
    "max_tokens": 1024,
    "messages": [
      {
        "role": "user",
        "content": "Create a payment link for $20"
      }
    ],
    "tools": [
      {
        "type": "mcp",
        "server_label": "stripe",
        "server_url": "https://mcp.stripe.com",
        "headers": {
          "Authorization": "Bearer $STRIPE_API_KEY"
        }
      }
    ]
  }'
```

```python Anthropic Python SDK
import anthropic

client = anthropic.Anthropic(
    api_key="ANTHROPIC_API_KEY", # Can be a dummy key
    default_headers={"x-portkey-api-key": "YOUR_PORTKEY_API_KEY"},
    base_url="https://api.portkey.ai/v1"
)

message = await anthropic.beta.messages.create({
  model: "claude-sonnet-4-20250514",
  max_tokens: 1000,
  messages: [
    {
      role: "user",
      content: "What tools do you have available?",
    },
  ],
  mcp_servers: [
    {
      type: "url",
      url: "https://example-server.modelcontextprotocol.io/sse",
      name: "example-mcp",
      authorization_token: "YOUR_TOKEN",
    },
  ],
  betas: ["mcp-client-2025-04-04"],
});

print(message.content)
```

```typescript Anthropic TS SDK
import Anthropic from '@anthropic-ai/sdk';

const anthropic = new Anthropic({
  apiKey: 'ANTHROPIC_API_KEY', // Can be a dummy key
  baseURL: "https://api.portkey.ai/v1",
  defaultHeaders: { "x-portkey-api-key": "YOUR_PORTKEY_API_KEY" }
});

const msg = await anthropic.messages.create({
  model: "claude-3-sonnet-20240229",
  max_tokens: 1024,
  messages: [{ role: "user", content: "Create a payment link for $20" }],
  tools: [
    {
      type: "mcp",
      server_label: "stripe",
      server_url: "https://mcp.stripe.com",
      headers: {
        Authorization: "Bearer $STRIPE_API_KEY"
      }
    }
  ]
});
console.log(msg.content);
```
</CodeGroup>

































---
title: "Remote MCP"
description: Portkey's AI gateway has MCP server support that many foundational model providers offer.
---

[Model Context Protocol](https://modelcontextprotocol.io/introduction) (MCP) is an open protocol that standardizes how applications provide tools and context to LLMs. The MCP tool in the Responses API allows developers to give the model access to tools hosted on **Remote MCP servers**. These are MCP servers maintained by developers and organizations across the internet that expose these tools to MCP clients, like the Portkey Gateway.

Portkey supports using a remote MCP server with both OpenAI-compatible and Anthropic-compatible models.

## Using Remote MCP with OpenAI

Calling a remote MCP server with OpenAI's API format is straightforward. For example, here's how you can use the [DeepWiki](https://deepwiki.com/) MCP server to ask questions about nearly any public GitHub repository.

### Example MCP Request

A Chat Completions request to an OpenAI model with MCP tools enabled.

<CodeGroup>
```bash cURL
curl https://api.portkey.ai/v1/chat/completions \
  -H "Content-Type: application/json" \
  -H "x-portkey-api-key: $PORTKEY_API_KEY" \
  -d '{
    "model": "gpt-4-turbo",
    "messages": [
        {
            "role": "user",
            "content": "What transport protocols are supported in the 2025-03-26 version of the MCP spec?"
        }
    ],
    "tools": [
      {
        "type": "mcp",
        "server_label": "deepwiki",
        "server_url": "https://mcp.deepwiki.com/mcp",
        "require_approval": "never"
      }
    ]
  }'
```

```javascript OpenAI Node SDK
import OpenAI from "openai";

const client = new OpenAI({
  apiKey: "PORTKEY_API_KEY",
  baseURL: "https://api.portkey.ai/v1"
});

const resp = await client.chat.completions.create({
  model: "gpt-4-turbo",
  messages: [
    {
      role: "user",
      content: "What transport protocols are supported in the 2025-03-26 version of the MCP spec?",
    },
  ],
  tools: [
    {
      type: "mcp",
      server_label: "deepwiki",
      server_url: "https://mcp.deepwiki.com/mcp",
      require_approval: "never",
    },
  ],
});

console.log(resp.choices[0].message);
```

```python OpenAI Python SDK
from openai import OpenAI

client = OpenAI(
  api_key="PORTKEY_API_KEY",
  base_url="https://api.portkey.ai/v1",
)

resp = client.chat.completions.create(
  model="gpt-4-turbo",
  messages=[
    {
      "role": "user",
      "content": "What transport protocols are supported in the 2025-03-26 version of the MCP spec?",
    },
  ],
  tools=[
    {
      "type": "mcp",
      "server_label": "deepwiki",
      "server_url": "https://mcp.deepwiki.com/mcp",
      "require_approval": "never",
    },
  ],
)

print(resp.choices[0].message)
```
</CodeGroup>


<Frame caption="Example Log for the request on Portkey">
  <img src="/images/product/ai-gateway/mcp-remote-logs-image.png" />
</Frame>


### MCP Server Authentication

Unlike the DeepWiki MCP server, most other MCP servers require authentication. The `tools` parameter gives you the ability to flexibly specify headers that should be included in any request made to a remote MCP server.

<CodeGroup>
```bash cURL
curl https://api.portkey.ai/v1/chat/completions \
  -H "Content-Type: application/json" \
  -H "x-portkey-api-key: $PORTKEY_API_KEY" \
  -d '{
    "model": "gpt-4-turbo",
    "messages": [
        {
            "role": "user",
            "content": "Create a payment link for $20"
        }
    ],
    "tools": [
      {
        "type": "mcp",
        "server_label": "stripe",
        "server_url": "https://mcp.stripe.com",
        "headers": {
          "Authorization": "Bearer $STRIPE_API_KEY"
        }
      }
    ]
  }'
```

```javascript OpenAI Node SDK
import OpenAI from "openai";

const client = new OpenAI({
  apiKey: "PORTKEY_API_KEY",
  baseURL: "https://api.portkey.ai/v1"
});

const resp = await client.chat.completions.create({
  model: "gpt-4-turbo",
  messages: [
    {
      "role": "user",
      "content": "Create a payment link for $20",
    },
  ],
  tools: [
    {
      type: "mcp",
      server_label: "stripe",
      server_url: "https://mcp.stripe.com",
      headers: {
        Authorization: "Bearer $STRIPE_API_KEY"
      }
    }
  ]
});

console.log(resp.choices[0].message);
```

```python OpenAI Python SDK
from openai import OpenAI

client = OpenAI(
  api_key="PORTKEY_API_KEY",
  base_url="https://api.portkey.ai/v1"
)

resp = client.chat.completions.create(
  model="gpt-4-turbo",
  messages=[
    {
      "role": "user",
      "content": "Create a payment link for $20",
    },
  ],
  tools=[
    {
      "type": "mcp",
      "server_label": "stripe",
      "server_url": "https://mcp.stripe.com",
      "headers": {
        "Authorization": "Bearer $STRIPE_API_KEY"
      }
    }
  ]
)

print(resp.choices[0].message)
```
</CodeGroup>

To prevent leakage of sensitive keys, Portkey does not store the values of **any** string you provide in the `headers` object. These values will also not be visible in the Response object created. 

---

## Using Remote MCP with Anthropic, Bedrock & Vertex

Portkey now supports Remote MCP through Anthropic's Messages API format, allowing you to connect to MCP servers with models from **Anthropic, Amazon Bedrock, and Google Vertex**.

This integration uses Anthropic's native MCP connector feature. To enable it, you must include the `mcp_servers` parameter in your request body and the `"anthropic-beta": "mcp-client-2025-04-04"` header.

### Example MCP Request

Hereâ€™s how to connect to a remote MCP server using the Messages API through Portkey.

<Info>
When using SDKs, the `anthropic-beta` header is passed via the `betas=["mcp-client-2025-04-04"]` parameter in the `create` method.
</Info>

<CodeGroup>
```bash cURL
# Use x-portkey-provider to specify anthropic, bedrock, or vertex
curl https://api.portkey.ai/v1/messages \
  -H "Content-Type: application/json" \
  -H "x-portkey-api-key: $PORTKEY_API_KEY" \
  -H "x-portkey-provider: anthropic" \
  -H "anthropic-beta: mcp-client-2025-04-04" \
  -d '{
    "model": "claude-3-sonnet-20240229",
    "max_tokens": 1000,
    "messages": [{"role": "user", "content": "What tools do you have available?"}],
    "mcp_servers": [
      {
        "type": "url",
        "url": "https://mcp.example.com/sse",
        "name": "example-mcp",
        "authorization_token": "YOUR_TOKEN"
      }
    ]
  }'
```

```python Anthropic Python SDK
import anthropic

# Use Portkey to connect to Anthropic, Bedrock, or Vertex
client = anthropic.Anthropic(
    api_key="dummy", 
    base_url="https://api.portkey.ai/v1",
    default_headers={
      "x-portkey-api-key": "YOUR_PORTKEY_API_KEY",
      # Set the target provider
      "x-portkey-provider": "anthropic" 
    }
)

response = client.beta.messages.create(
    model="claude-3-sonnet-20240229",
    max_tokens=1000,
    messages=[{
        "role": "user",
        "content": "What tools do you have available?"
    }],
    mcp_servers=[{
        "type": "url",
        "url": "https://mcp.example.com/sse",
        "name": "example-mcp",
        "authorization_token": "YOUR_TOKEN"
    }],
    betas=["mcp-client-2025-04-04"]
)

print(response.content)
```

```typescript Anthropic TS SDK
import Anthropic from '@anthropic-ai/sdk';

// Use Portkey to connect to Anthropic, Bedrock, or Vertex
const anthropic = new Anthropic({
  apiKey: 'dummy',
  baseURL: "https://api.portkey.ai/v1",
  defaultHeaders: { 
    "x-portkey-api-key": "YOUR_PORTKEY_API_KEY",
    // Set the target provider
    "x-portkey-provider": "anthropic"
  }
});

const msg = await anthropic.beta.messages.create({
  model: "claude-3-sonnet-20240229",
  max_tokens: 1024,
  messages: [{ role: "user", content: "What tools do you have available?" }],
  mcp_servers: [
    {
      "type": "url",
      "url": "https://mcp.example.com/sse",
      "name": "example-mcp",
      "authorization_token": "YOUR_TOKEN"
    },
  ],
  betas:["mcp-client-2025-04-04"]
});
console.log(msg.content);
```
</CodeGroup>

### MCP Server Configuration

Each object in the `mcp_servers` array supports the following fields:

| Property | Type | Required | Description |
| :--- | :--- | :--- | :--- |
| `type` | string | Yes | Currently, only `"url"` is supported. |
| `url` | string | Yes | The URL of the MCP server. Must start with `https://`. |
| `name`| string | Yes | A unique identifier for this MCP server. It will be used in `mcp_tool_use` blocks to identify the server. |
| `authorization_token` | string | No | OAuth authorization token if required by the MCP server. |
| `tool_configuration` | object | No | Configure tool usage from this server. |
| `tool_configuration.enabled` | boolean | No | Whether to enable tools from this server (default: `true`). |
| `tool_configuration.allowed_tools` | array | No | List of tool names to restrict usage to. By default, all tools are allowed. |

### Response Content Types

When the model uses an MCP tool, the response from the Messages API will include new content block types.

#### MCP Tool Use Block
This block indicates the model is calling a tool from a connected MCP server.

```json
{
  "type": "mcp_tool_use",
  "id": "mcptoolu_014Q35RayjACSWkSj4X2yov1",
  "name": "echo",
  "server_name": "example-mcp",
  "input": { "param1": "value1", "param2": "value2" }
}
```

#### MCP Tool Result Block
This block contains the result from the tool's execution.

```json
{
  "type": "mcp_tool_result",
  "tool_use_id": "mcptoolu_014Q35RayjACSWkSj4X2yov1",
  "is_error": false,
  "content": [
    {
      "type": "text",
      "text": "Hello"
    }
  ]
}
```