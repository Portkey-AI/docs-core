---
title: "Universal API"
sidebarTitle: "Overview"
description: "One API for 200+ LLMs across every major provider. Use OpenAI's Chat Completions, Responses API, or Anthropic's Messages format -- Portkey translates between them all."
---

<Info>
Available on all Portkey [plans](https://portkey.ai/pricing).
</Info>

Portkey's AI Gateway provides a single, unified API for 200+ models from every major provider. Write once in any format, switch providers by changing one parameter.

## Three API Formats, Any Provider

Portkey supports three API formats. Each works with **all providers** — Portkey handles translation automatically.

<CardGroup cols={3}>
  <Card title="Chat Completions" icon="comments" href="/product/ai-gateway/chat-completions">
    **OpenAI spec** · `POST /v1/chat/completions`

    Widest ecosystem compatibility
  </Card>
  <Card title="Responses API" icon="robot" href="/product/ai-gateway/responses-api">
    **OpenAI spec** · `POST /v1/responses`

    Agentic AI with built-in tool use
  </Card>
  <Card title="Messages API" icon="message" href="/product/ai-gateway/messages-api">
    **Anthropic spec** · `POST /v1/messages`

    Native Anthropic format across providers
  </Card>
</CardGroup>

### Chat Completions — `POST /v1/chat/completions`

OpenAI-compatible format with the widest ecosystem support. [Full guide →](/product/ai-gateway/chat-completions)

<CodeGroup>
```python Portkey Python
from portkey_ai import Portkey

portkey = Portkey(api_key="PORTKEY_API_KEY")

response = portkey.chat.completions.create(
    model="@anthropic-provider/claude-sonnet-4-5-20250514",
    messages=[{"role": "user", "content": "Hello!"}]
)

print(response.choices[0].message.content)
```

```javascript Portkey Node.js
import Portkey from 'portkey-ai';

const portkey = new Portkey({ apiKey: "PORTKEY_API_KEY" });

const response = await portkey.chat.completions.create({
    model: "@anthropic-provider/claude-sonnet-4-5-20250514",
    messages: [{ role: "user", content: "Hello!" }]
});

console.log(response.choices[0].message.content);
```

```python OpenAI Python
from openai import OpenAI

client = OpenAI(
    api_key="PORTKEY_API_KEY",
    base_url="https://api.portkey.ai/v1"
)

response = client.chat.completions.create(
    model="@anthropic-provider/claude-sonnet-4-5-20250514",
    messages=[{"role": "user", "content": "Hello!"}]
)

print(response.choices[0].message.content)
```

```javascript OpenAI Node.js
import OpenAI from 'openai';

const client = new OpenAI({
    apiKey: "PORTKEY_API_KEY",
    baseURL: "https://api.portkey.ai/v1"
});

const response = await client.chat.completions.create({
    model: "@anthropic-provider/claude-sonnet-4-5-20250514",
    messages: [{ role: "user", content: "Hello!" }]
});

console.log(response.choices[0].message.content);
```

```sh cURL
curl https://api.portkey.ai/v1/chat/completions \
  -H "Content-Type: application/json" \
  -H "x-portkey-api-key: $PORTKEY_API_KEY" \
  -d '{
    "model": "@anthropic-provider/claude-sonnet-4-5-20250514",
    "messages": [{"role": "user", "content": "Hello!"}]
  }'
```
</CodeGroup>

### Responses API — `POST /v1/responses`

OpenAI's next-gen format for agentic AI with built-in tool use and reasoning. [Full guide →](/product/ai-gateway/responses-api)

<CodeGroup>
```python Portkey Python
from portkey_ai import Portkey

portkey = Portkey(api_key="PORTKEY_API_KEY")

response = portkey.responses.create(
    model="@anthropic-provider/claude-sonnet-4-5-20250514",
    input="Hello!"
)

print(response.output_text)
```

```javascript Portkey Node.js
import Portkey from 'portkey-ai';

const portkey = new Portkey({ apiKey: "PORTKEY_API_KEY" });

const response = await portkey.responses.create({
    model: "@anthropic-provider/claude-sonnet-4-5-20250514",
    input: "Hello!"
});

console.log(response.output_text);
```

```python OpenAI Python
from openai import OpenAI

client = OpenAI(
    api_key="PORTKEY_API_KEY",
    base_url="https://api.portkey.ai/v1"
)

response = client.responses.create(
    model="@anthropic-provider/claude-sonnet-4-5-20250514",
    input="Hello!"
)

print(response.output_text)
```

```javascript OpenAI Node.js
import OpenAI from 'openai';

const client = new OpenAI({
    apiKey: "PORTKEY_API_KEY",
    baseURL: "https://api.portkey.ai/v1"
});

const response = await client.responses.create({
    model: "@anthropic-provider/claude-sonnet-4-5-20250514",
    input: "Hello!"
});

console.log(response.output_text);
```

```sh cURL
curl https://api.portkey.ai/v1/responses \
  -H "Content-Type: application/json" \
  -H "x-portkey-api-key: $PORTKEY_API_KEY" \
  -d '{
    "model": "@anthropic-provider/claude-sonnet-4-5-20250514",
    "input": "Hello!"
  }'
```
</CodeGroup>

### Messages API — `POST /v1/messages`

Anthropic-native format using the Anthropic SDK pointed at Portkey's base URL. [Full guide →](/product/ai-gateway/messages-api)

<CodeGroup>
```python Python
import anthropic

client = anthropic.Anthropic(
    api_key="PORTKEY_API_KEY",
    base_url="https://api.portkey.ai"
)

message = client.messages.create(
    model="@anthropic-provider/claude-sonnet-4-5-20250514",
    max_tokens=1024,
    messages=[{"role": "user", "content": "Hello!"}]
)

print(message.content[0].text)
```

```typescript TypeScript
import Anthropic from '@anthropic-ai/sdk';

const client = new Anthropic({
    apiKey: "PORTKEY_API_KEY",
    baseURL: "https://api.portkey.ai"
});

const message = await client.messages.create({
    model: "@anthropic-provider/claude-sonnet-4-5-20250514",
    max_tokens: 1024,
    messages: [{ role: "user", content: "Hello!" }]
});

console.log(message.content[0].text);
```

```sh cURL
curl https://api.portkey.ai/v1/messages \
  -H "Content-Type: application/json" \
  -H "x-portkey-api-key: $PORTKEY_API_KEY" \
  -d '{
    "model": "@anthropic-provider/claude-sonnet-4-5-20250514",
    "max_tokens": 1024,
    "messages": [{"role": "user", "content": "Hello!"}]
  }'
```
</CodeGroup>

<Info>
The Portkey SDK is a superset of the OpenAI SDK. The OpenAI SDK also works directly — point `base_url` at `https://api.portkey.ai/v1`. For the Messages API, use the Anthropic SDK. See [Model Catalog](/product/model-catalog) for provider setup.
</Info>

## Switching Providers

Change the `@provider/model` string to switch between any provider. The API format stays the same.

<CodeGroup>
```python Portkey Python
from portkey_ai import Portkey

portkey = Portkey(api_key="PORTKEY_API_KEY")

# OpenAI
response = portkey.chat.completions.create(
    model="@openai-provider/gpt-4o",
    messages=[{"role": "user", "content": "Hello"}]
)

# Switch to Anthropic -- same client, different model string
response = portkey.chat.completions.create(
    model="@anthropic-provider/claude-sonnet-4-5-20250514",
    messages=[{"role": "user", "content": "Hello"}]
)

# Switch to Gemini
response = portkey.chat.completions.create(
    model="@google-provider/gemini-2.0-flash",
    messages=[{"role": "user", "content": "Hello"}]
)
```

```javascript Portkey Node.js
import Portkey from 'portkey-ai';

const portkey = new Portkey({ apiKey: "PORTKEY_API_KEY" });

// OpenAI
const response = await portkey.chat.completions.create({
    model: "@openai-provider/gpt-4o",
    messages: [{ role: "user", content: "Hello" }]
});

// Switch to Anthropic -- same client, different model string
const response2 = await portkey.chat.completions.create({
    model: "@anthropic-provider/claude-sonnet-4-5-20250514",
    messages: [{ role: "user", content: "Hello" }]
});
```

```python OpenAI Python
from openai import OpenAI

client = OpenAI(
    api_key="PORTKEY_API_KEY",
    base_url="https://api.portkey.ai/v1"
)

# OpenAI
response = client.chat.completions.create(
    model="@openai-provider/gpt-4o",
    messages=[{"role": "user", "content": "Hello"}]
)

# Switch to Anthropic -- same client, different model string
response = client.chat.completions.create(
    model="@anthropic-provider/claude-sonnet-4-5-20250514",
    messages=[{"role": "user", "content": "Hello"}]
)

# Switch to Gemini
response = client.chat.completions.create(
    model="@google-provider/gemini-2.0-flash",
    messages=[{"role": "user", "content": "Hello"}]
)
```

```javascript OpenAI Node.js
import OpenAI from 'openai';

const client = new OpenAI({
    apiKey: "PORTKEY_API_KEY",
    baseURL: "https://api.portkey.ai/v1"
});

// OpenAI
const response = await client.chat.completions.create({
    model: "@openai-provider/gpt-4o",
    messages: [{ role: "user", content: "Hello" }]
});

// Switch to Anthropic -- same client, different model string
const response2 = await client.chat.completions.create({
    model: "@anthropic-provider/claude-sonnet-4-5-20250514",
    messages: [{ role: "user", content: "Hello" }]
});
```

```sh cURL
# OpenAI
curl https://api.portkey.ai/v1/chat/completions \
  -H "Content-Type: application/json" \
  -H "x-portkey-api-key: $PORTKEY_API_KEY" \
  -d '{"model": "@openai-provider/gpt-4o", "messages": [{"role": "user", "content": "Hello"}]}'

# Switch to Anthropic -- same endpoint, different model string
curl https://api.portkey.ai/v1/chat/completions \
  -H "Content-Type: application/json" \
  -H "x-portkey-api-key: $PORTKEY_API_KEY" \
  -d '{"model": "@anthropic-provider/claude-sonnet-4-5-20250514", "messages": [{"role": "user", "content": "Hello"}]}'
```
</CodeGroup>

<Info>
Set up providers and credentials in the [Model Catalog](/product/model-catalog). The `@provider-slug` in the model string routes requests to the correct provider automatically.
</Info>

## Routing, Fallbacks, and Load Balancing

[Configs](/product/ai-gateway/configs) enable routing strategies, fallbacks, and load balancing across providers.

<CodeGroup>
```python Portkey Python
from portkey_ai import Portkey

config = {
    "strategy": {"mode": "fallback"},
    "targets": [
        {"override_params": {"model": "@openai-provider/gpt-4o"}},
        {"override_params": {"model": "@anthropic-provider/claude-sonnet-4-5-20250514"}}
    ]
}

portkey = Portkey(api_key="PORTKEY_API_KEY", config=config)

# Automatically falls back to Anthropic if OpenAI fails
response = portkey.chat.completions.create(
    messages=[{"role": "user", "content": "Hello"}]
)
```

```javascript Portkey Node.js
import Portkey from 'portkey-ai';

const config = {
    strategy: { mode: "fallback" },
    targets: [
        { override_params: { model: "@openai-provider/gpt-4o" } },
        { override_params: { model: "@anthropic-provider/claude-sonnet-4-5-20250514" } }
    ]
};

const portkey = new Portkey({ apiKey: "PORTKEY_API_KEY", config });

// Automatically falls back to Anthropic if OpenAI fails
const response = await portkey.chat.completions.create({
    messages: [{ role: "user", content: "Hello" }]
});
```

```python OpenAI Python
from openai import OpenAI

# Use a saved config ID from Portkey dashboard
client = OpenAI(
    api_key="PORTKEY_API_KEY",
    base_url="https://api.portkey.ai/v1",
    default_headers={"x-portkey-config": "pp-config-xxx"}
)

# Automatically falls back to Anthropic if OpenAI fails
response = client.chat.completions.create(
    model="gpt-4o",
    messages=[{"role": "user", "content": "Hello"}]
)
```

```javascript OpenAI Node.js
import OpenAI from 'openai';

// Use a saved config ID from Portkey dashboard
const client = new OpenAI({
    apiKey: "PORTKEY_API_KEY",
    baseURL: "https://api.portkey.ai/v1",
    defaultHeaders: { "x-portkey-config": "pp-config-xxx" }
});

// Automatically falls back to Anthropic if OpenAI fails
const response = await client.chat.completions.create({
    model: "gpt-4o",
    messages: [{ role: "user", content: "Hello" }]
});
```

```sh cURL
curl https://api.portkey.ai/v1/chat/completions \
  -H "Content-Type: application/json" \
  -H "x-portkey-api-key: $PORTKEY_API_KEY" \
  -H "x-portkey-config: pp-config-xxx" \
  -d '{"messages": [{"role": "user", "content": "Hello"}]}'
```
</CodeGroup>

Configs work with all three API formats -- Chat Completions, Responses, and Messages.

For more details, see [Configs](/product/ai-gateway/configs) and [Conditional Routing](/product/ai-gateway/conditional-routing).

## Local and Private Models

Route to local or private models with `custom_host`. The model must be compatible with a supported provider format.

<CodeGroup>
```python Portkey Python
from portkey_ai import Portkey

portkey = Portkey(
    api_key="PORTKEY_API_KEY",
    provider="mistral-ai",
    custom_host="http://MODEL_URL/v1/"
)

response = portkey.chat.completions.create(
    model="mixtral-8x22b",
    messages=[{"role": "user", "content": "Hello"}]
)
```

```javascript Portkey Node.js
import Portkey from 'portkey-ai';

const portkey = new Portkey({
    apiKey: "PORTKEY_API_KEY",
    provider: "mistral-ai",
    customHost: "http://MODEL_URL/v1/"
});

const response = await portkey.chat.completions.create({
    model: "mixtral-8x22b",
    messages: [{ role: "user", content: "Hello" }]
});
```

```python OpenAI Python
from openai import OpenAI

client = OpenAI(
    api_key="PORTKEY_API_KEY",
    base_url="https://api.portkey.ai/v1",
    default_headers={
        "x-portkey-provider": "mistral-ai",
        "x-portkey-custom-host": "http://MODEL_URL/v1/"
    }
)

response = client.chat.completions.create(
    model="mixtral-8x22b",
    messages=[{"role": "user", "content": "Hello"}]
)
```

```javascript OpenAI Node.js
import OpenAI from 'openai';

const client = new OpenAI({
    apiKey: "PORTKEY_API_KEY",
    baseURL: "https://api.portkey.ai/v1",
    defaultHeaders: {
        "x-portkey-provider": "mistral-ai",
        "x-portkey-custom-host": "http://MODEL_URL/v1/"
    }
});

const response = await client.chat.completions.create({
    model: "mixtral-8x22b",
    messages: [{ role: "user", content: "Hello" }]
});
```

```sh cURL
curl https://api.portkey.ai/v1/chat/completions \
  -H "Content-Type: application/json" \
  -H "x-portkey-api-key: $PORTKEY_API_KEY" \
  -H "x-portkey-provider: mistral-ai" \
  -H "x-portkey-custom-host: http://MODEL_URL/v1/" \
  -d '{"model": "mixtral-8x22b", "messages": [{"role": "user", "content": "Hello"}]}'
```
</CodeGroup>

<Note>
Include the version identifier (e.g., `/v1`) in the `custom_host` URL. Portkey appends the endpoint path (`/chat/completions`, `/responses`, etc.) automatically. For Ollama, see the [Ollama integration](/integrations/llms/ollama).
</Note>

## Supported Endpoints

### Core Endpoints

- **[Chat Completions](/product/ai-gateway/chat-completions)** — OpenAI-compatible text generation with streaming, function calling, and multimodal inputs
- **[Responses API](/product/ai-gateway/responses-api)** — Next-gen format with built-in tool use and reasoning
- **[Messages API](/product/ai-gateway/messages-api)** — Anthropic-compatible endpoint across all providers
- **[Images](/api-reference/inference-api/images/create-image)** — Generate, edit, and create image variations (DALL-E, gpt-image-1, Stable Diffusion)
- **[Audio](/api-reference/inference-api/audio/create-speech)** — Speech-to-text and text-to-speech

### Advanced Capabilities

- **[Fine-tuning](/product/ai-gateway/fine-tuning)** — Customize models on specific datasets
- **[Batch Processing](/product/ai-gateway/batches)** — Process large request volumes efficiently
- **[Files](/product/ai-gateway/files)** — Upload and manage files for fine-tuning and batch operations
- **[Moderations](/api-reference/inference-api/moderations)** — Content safety and compliance checks

### Additional Endpoints

- **[Gateway to Other APIs](/api-reference/inference-api/gateway-for-other-apis)** — Proxy requests to any provider endpoint
- **[Assistants API](/api-reference/inference-api/assistants-api/assistants/create-assistant)** — OpenAI Assistants with persistent threads
- **[Completions](/api-reference/inference-api/completions)** — Legacy text completion endpoint

### Multimodal Capabilities

Multimodal inputs work across all three API formats:

- **[Vision](/product/ai-gateway/multimodal-capabilities/vision)** — Image understanding across providers
- **[Function Calling](/product/ai-gateway/multimodal-capabilities/function-calling)** — Tool use and function calling
- **[Image Generation](/product/ai-gateway/multimodal-capabilities/image-generation)** — Text-to-image generation
- **[Speech-to-Text](/product/ai-gateway/multimodal-capabilities/speech-to-text)** — Audio transcription
- **[Text-to-Speech](/product/ai-gateway/multimodal-capabilities/text-to-speech)** — Audio generation
- **[Thinking / Reasoning](/product/ai-gateway/multimodal-capabilities/thinking-mode)** — Extended reasoning modes

<Info>
Not all providers support every endpoint or modality. See the [provider compatibility matrix](/api-reference/inference-api/supported-providers) for details.
</Info>
