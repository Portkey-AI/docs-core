---
title: "Unified Batch Inference"
description: "Run large‚Äëscale inference jobs through one consistent endpoint"
---

<Info>
  **Enterprise Feature** <br/> Batch inference is available on **Enterprise** plans only. Contact the Portkey team to enable it for your workspace.
</Info>

Portkey‚Äôs AI Gateway lets you send a single request that fan‚Äëouts to hundreds‚Äîor millions‚Äîof completions. Choose the mode that best fits cost, latency, and provider support.

## Choose Your Batching Mode

| Mode                   | When to pick it | Works with |
| ---------------------- | -------------------------- | ------------ |
| **[Provider¬†Batch¬†API](#provider-batch-api-mode)** | *Cheapest* for overnight or offline jobs. Uses the provider‚Äôs native batch endpoint & limits.| `OpenAI`, `Azure¬†OpenAI`, `Bedrock`, `Vertex`, `Fireworks` |
| **[Portkey¬†Batch¬†API](#portkey-batch-api-mode)**  | *Fastest* and provider‚Äëagnostic. Batches at the Gateway layer; ideal when a provider has no native batch support or you need cross‚Äëprovider jobs. | Any provider supported by Portkey |

<Tip>
<strong>Quick rule of thumb¬†‚Üí</strong> <br/> Need low latency or multi‚Äëprovider batching? **Portkey¬†Batch¬†API**. Otherwise, stick with the provider‚Äôs native batch for cost savings.
</Tip>

---

## Before You Start

Have the following ready to start making batch requests:
1. **Portkey account & API key**.
2. **Provider credentials** for each downstream model (OpenAI key, Bedrock IAM role, etc.).
3. A **Portkey File** (`input_file_id`) - **required only when using the Portkey Batch API (Mode¬†#2)**. See [Files](/product/ai-gateway/files) to upload one.
4. Optional: Familiarity with the [Create¬†Batch OpenAPI spec](https://portkey.ai/docs/api-reference/inference-api/batch/create-batch).

---

## Provider¬†Batch¬†API Mode
Used to run batch jobs with the provider's native batch endpoint. Providers usually offer a cheaper rate for batch jobs, but you'll be limited by the provider's quota and limits. Most completion windows are about 24 hours.

### Quickstart (OpenAI example)


<CodeGroup>

```bash curl
curl -X POST https://api.portkey.ai/v1/batches \
  -H "Authorization: Bearer $PORTKEY_API_KEY" \
  -H "Content-Type: application/json" \
  -H "x-portkey-provider: $PORTKEY_PROVIDER" \
  -d '{
    "input_file_id": "file_abc123", 
    "completion_window": "24h",  
    "endpoint": "/v1/chat/completions",
}'
```

```javascript Typescript
import Portkey from 'portkey-ai';

const client = new Portkey({
  apiKey: 'PORTKEY_API_KEY',
  virtualKey: 'PROVIDER_VIRTUAL_KEY'
});

async function main() {
  const batch = await client.batches.create({
    input_file_id: "file-abc123",
    endpoint: "/v1/chat/completions",
    completion_window: "24h"
  });

  console.log(batch);
}

main();

```

```python Python
from portkey_ai import Portkey

client = Portkey(
  api_key = "PORTKEY_API_KEY",
  virtual_key = "PROVIDER_VIRTUAL_KEY"
)

client.batches.create(
  input_file_id="file-abc123",
  endpoint="/v1/chat/completions",
  completion_window="24h"
)
```


</CodeGroup>    

> üîó¬†Full schema: see the [OpenAPI reference](/api-reference/inference-api/batch/create-batch).

### Supported Providers & Endpoints

| Provider                                                | Endpoints                                       |
| ------------------------------------------------------- | ----------------------------------------------- |
| [OpenAI](/integrations/llms/openai/batches)             | `completions`, `chat completions`, `embeddings` |
| [Azure¬†OpenAI](/integrations/llms/azure-openai/batches) | `completions`, `chat completions`, `embeddings` |
| [Bedrock](/integrations/llms/bedrock/batches)           | `chat completions`                              |
| [Vertex¬†AI](/integrations/llms/vertex-ai/batches)       | `chat completions`, `embeddings`                |
| [Fireworks](/integrations/llms/fireworks/batches)       | `completions`                                   |

### Defaults & Limits

| Property            | Default          | Notes                                          |
| ------------------- | ---------------- | ---------------------------------------------- |
| `completion_window` | `24h`            | Set by provider (cannot be shorter).           |
| Provider quota      | Per provider     | e.g., OpenAI¬†‚â§¬†50k jobs/day.                   |
| Retries             | Provider‚Äëdefined | Portkey surfaces job status; no Gateway retry. |

---

## Portkey¬†Batch¬†API Mode ‚≠êÔ∏è

### How It Works

Set `completion_window` to `immediate` and Portkey aggregates your requests in memory, then fires them to the target provider in fixed buckets.

| Gateway default | Value                                                   |
| --------------- | ------------------------------------------------------- |
| Batch size      | **25** requests                                         |
| Batch interval  | **5¬†s** between flushes                                 |
| Retries         | **3** per request (configurable via `x-portkey-config`) |

*Coming soon*: configurable `batch_size`, `batch_interval`, and `max_retries`.

### Quickstart (provider‚Äëagnostic)

```bash
curl -X POST https://api.portkey.ai/v1/batches \
  -H "Authorization: Bearer <PORTKEY_KEY>" \
  -H "Content-Type: application/json" \
  -d '{
    "input_file_id": "pk_file_...",
    "completion_window": "immediate",
    "endpoint": "/v1/chat/completions"
}'
```

Because Portkey orchestrates the batching, this works even for providers without a native batch endpoint.

### Response & Monitoring

Identical to Provider mode; the difference is that `provider_job_id` is absent and cost is computed from individual calls.

### About Portkey Files

Portkey Files are files uploaded to Portkey that are then automatically uploaded to the provider. They're useful when you want to make multiple batch completions using the same file. Portkey will:

- Automatically upload the file to the provider on your behalf
- Reuse the content in your batch requests
- Check batch progress and provide post-batch analysis including token and cost calculations
- Make batch outputs available via the `GET /batches/<batch_id>/output` endpoint

---

## Error Handling & Retries

| Layer                   | What Portkey does                 | How to override                                    |
| ----------------------- | --------------------------------- | -------------------------------------------------- |
| Gateway (Portkey Batch) | Retries **3√ó** on network/429/5xx | `x-portkey-config: {"retry": {"max_attempts": 5}}` |
| Provider (native batch) | Provider rules                    | Not configurable via Portkey                       |

---

## Security & IAM

* **Files** are encrypted at rest (AES‚Äë256) and deleted from provider storage once the batch succeeds or after 7¬†days, whichever is earlier.
* Portkey uploads on your behalf using *least‚Äëprivilege* scoped credentials; no long‚Äëlived secrets are stored.
* Access to batch status & outputs is gated by your workspace role (`batch.read`).

---

## Glossary

| Term                               | Meaning                                                                                                   |
| ---------------------------------- | --------------------------------------------------------------------------------------------------------- |
| **Batch¬†Job**                      | A collection of completion requests executed asynchronously.                                              |
| **Portkey File** (`input_file_id`) | Files uploaded to Portkey that are automatically uploaded to the provider for batch processing. Useful for reusing the same file across multiple batch completions. |
| **Virtual Key**                    | A logical provider credential stored in Portkey; referenced by ID, not secret.                            |
| **Completion¬†Window**              | Time frame in which the job must finish. `immediate` ‚Üí handled by Portkey; `24h` ‚Üí delegated to provider. |

---

## Roadmap

* Custom `batch_size`, `batch_interval`, `max_retries` (Q3¬†2025)
* Real‚Äëtime progress webhooks
* UI for canceling or pausing jobs
