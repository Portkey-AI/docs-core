---
title: "Observability"
description: "Gain real-time insights, track key metrics, and streamline debugging with our comprehensive observability suite."
---

If you're working with an LLM - visibility across all your requests can be a BIG pain. How do you trace and measure cost, latency, accuracy of your requests?

Portkey's OpenTelemetry-compliant observability suite gives you complete control over all your requests. And Portkey's analytics dashboards provide the insights you're looking for. Fast.

<Frame>
  <img src="/images/product/product-1.png"/>
</Frame>

## Features

<CardGroup columns={3}>
  <Card title="Logs" href="/product/observability/logs">
    <p>Portkey records all your multimodal requests and responses, making it easy to view, monitor, and debug interactions.</p>
  </Card>

  <Card title="Tracing" href="/product/observability/traces">
    <p>Portkey supports request tracing to help you monitor your applications throughout the lifecycle of a request.</p>
  </Card>

  <Card title="Analytics" href="/product/observability/analytics">
    <p>A comprehensive view of 21+ key metrics. Use it to analyze data, spot trends, and make informed decisions.</p>
  </Card>

  <Card title="Filters" href="/product/observability/filters">
    <p>Streamline your data view with customizable filters. Zero in on data that matters most.</p>
  </Card>

  <Card title="Custom Metadata" href="/product/observability/metadata">
    <p>Enrich your LLM APIs with custom metadata. Assign unique tags for swift grouping and troubleshooting.</p>
  </Card>

  <Card title="Feedback" href="/product/observability/feedback">
    <p>Add feedback values and weights to complete the loop.</p>
  </Card>

  <Card title="Budget Limits" href="/product/ai-gateway/virtual-keys/budget-limits">
    <p>Set up budget limits for your provider API keys and gain confidence over your application's costs.</p>
  </Card>
</CardGroup>
