---
title: "Open Source"
---

## [Portkey AI Gateway](https://github.com/portkey-ai/rubeus)

We have open sourced our battle-tested AI Gateway to the community - it connects to 200+ LLMs with a unified interface and a single endpoint, and lets you effortlessly setup fallbacks, load balancing, retries, and more.

This gateway is in production at Portkey processing billions of tokens every day.

#### [Contribute here](https://github.com/portkey-ai/rubeus).

---

## [AI Grants Finder](https://grantsfinder.portkey.ai/)

Community resource for AI builders to find `**GPU credits**`, `**grants**`, `**AI accelerators**`, or `**investments**` \- all in a single place. Continuously updated, and sometimes also featuring [exclusive deals](https://twitter.com/PortkeyAI/status/1692463628514156859).

Access the data [here](https://airtable.com/appUjtBcdLQIgusqW/shrAU1e4M5twTmRal).

---

## [Gateway Reports](https://portkey.ai/blog/tag/benchmarks/)

We collaborate with the community to dive deep into how the LLMs & their inference providers are performing at scale, and publish gateway reports. We track latencies, uptime, cost changes, fluctuations across various modalitites like time-of-day, regions, token-lengths, and more.

#### Report 1: [GPT-4 is getting faster](https://portkey.ai/blog/gpt-4-is-getting-faster/)

<Frame>
  <img src="/images/guardrails/g13.avif"/>
</Frame>

#### Please reach out** [**on Discord**](https://discord.gg/9sfE5ZYv) **to collaborate on our next report!

---

## Collaborations

Portkey supports various open source projects with additional production capabilities through its custom integrations, and the list is always growing:

* [Langchain](https://python.langchain.com/docs/integrations/providers/portkey/) \- Monitor and trace your Langchain queries
* [Llamaindex](https://gpt-index.readthedocs.io/en/latest/examples/llm/portkey.html) - Monitor & trace your requests, and also set up automated fallbacks & load balancing
* [GPT Prompt Engineer](https://github.com/mshumer/gpt-prompt-engineer) \- Log all the prompt engineer runs and debug issues easily
* [Instructor](/integrations/libraries/instructor) \- Extract structured outputs from LLMs and get full-stack observability over everything
* [Promptfoo](/integrations/libraries/promptfoo) \- Use Portkey prompts with Promptfoo to run evals and manage and version your prompt templates
* [Route to OSS LLMs](/integrations/llms/byollm) using [Ollama](/integrations/llms/ollama) or [LocalAI](/integrations/llms/local-ai) \- Connect Portkey to your locally hosted models
* [Autogen](/integrations/libraries/autogen) \- Bring LLM interoperability and Portkey's reliability to your Autogen agents
