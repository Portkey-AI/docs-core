---
title: "Integrations"
description: "Securely store and manage AI provider credentials across your organization with centralized governance controls"
---

<Note>
Integrations are designed for **organization admins and managers** who need to manage AI provider access across teams. If you're  looking to use AI models, see the [Model Catalog](/product/model-catalog) documentation.
</Note>


Integrations are the secure foundation for AI provider management in Portkey. Think of them as your organization's credential vault - a centralized place where you store API keys, configure access controls, and set usage policies that cascade throughout your entire AI infrastructure.

When you create an Integration, you're not just storing credentials - you're establishing a governance layer that controls:
- **Who** can access these AI services (through workspace provisioning)
- **What** models they can use (through model provisioning)
- **How much** they can spend (through budget limits)
- **How fast** they can consume resources (through rate limits)


## Why Integrations Matter

In enterprise AI deployments, raw API keys scattered across teams create security risks and make cost control impossible. Integrations solve this by:

1. **Centralizing Credentials**: Store API keys once, use everywhere through secure references
2. **Enabling Governance**: Apply organization-wide policies that automatically enforce compliance
3. **Simplifying Management**: Update credentials, limits, or access in one place
4. **Maintaining Security**: Never expose raw API keys to end users or applications
5. **Granular Observabilty**: Get complete end-to-end observability and track 40+ crucial metric for every single LLM call

## Creating an Integration

Let's walk through creating an Integration for AWS Bedrock as an example:

<Steps>
<Step title="Navigate to Integrations">
From your admin panel, go to **Integrations** and click **Create New Integration**.
</Step>

<Step title="Select Your AI Provider">
Choose from 200+ supported providers. Each provider may have different credential requirements.

<Frame>
  <img src="/images/product/model-catalog/integrations-page.png" alt="Creating Bedrock Integration" />
</Frame>
</Step>

<Step title="Configure Integration Details">
- **Name**: A descriptive name for this integration (e.g., "Bedrock Production")
- **Slug**: A unique identifier used in API calls (e.g., "bedrock-prod")
- **Description**: Optional context about this integration's purpose
- **Endpoint Type**: Choose between Public or Private endpoints
</Step>

<Step title="Enter Provider Credentials">
Each provider requires different credentials:


**For OpenAI:**
<Frame>
  <img src="/images/product/model-catalog/integrations-page-opnai.png" alt="OpenAI Integration Setup" />
</Frame>

- API Key
- Optional: Organization ID, Project ID


**For AWS Bedrock:**
- AWS Access Key
- AWS Secret Access Key
- AWS Access Key ID
- AWS Region

<Card href="" title="Connect Bedrock with Amazon Assumed Role">
    How to integrate Bedrock using Amazon Assumed Role Authentication
</Card>

Similarly for:
- Azure OpenAI
- Google Vertex AI
- Anthropic
- Gemini
and more...
</Step>
</Steps>


## Workspace Provisioning

Workspace provisioning determines which teams and projects can access this Integration. This is crucial for maintaining security boundaries and ensuring teams only access approved AI resources.

### How It Works

When you provision an Integration to a workspace:
1. That workspace can create AI Providers using this Integration's credentials
2. All usage is tracked at the workspace level for accountability
3. Budget and rate limits can be applied per workspace
4. Access can be revoked instantly if needed

### Setting Up Workspace Provisioning

<Frame>
  <img src="/images/product/model-catalog/workspace-provisioning-page.png" alt="Workspace Provisioning Configuration" />
</Frame>

1. In your Integration settings, navigate to **Workspace Provisioning**
2. Select which workspaces should have access:
   - **All Workspaces**: Grants access to every workspace in your organization
   - **Specific Workspaces**: Choose individual workspaces that need access
3. For each workspace, optionally set:
   - Custom budget limits (overrides Integration-level limits)
   - Custom rate limits
   - Specific model access

### Best Practices

- **Principle of Least Privilege**: Only provision to workspaces that genuinely need access
- **Environment Separation**: Create separate Integrations for dev/staging/production
- **Regular Audits**: Review workspace provisioning quarterly to remove unnecessary access


## Model Provisioning

Model provisioning gives you fine-grained control over which AI models are accessible through an Integration. This is essential for:
- Controlling costs by restricting access to expensive models
- Ensuring compliance by limiting models to approved ones
- Maintaining consistency by standardizing model usage across teams

### Configuration Options

<Frame>
  <img src="/images/product/model-catalog/model-provisioning-page.png" alt="Model Provisioning Settings" />
</Frame>

**1. Allow All Models** (Default)
- Provides access to all models offered by the provider
- Suitable for research/experimentation environments

**2. Allow Specific Models**
- Create an allowlist of approved models
- Example: Only allow `gpt-4`, `gpt-3.5-turbo` for an OpenAI Integration

### Adding Custom Models

For fine-tuned or custom-hosted models:

1. Click **Add Custom Model**
2. Provide:
   - Model identifier (must match provider's model name)
   - Display name for the Model Catalog
   - Optional: Custom pricing if different from standard
   - Optional: Context window and other parameters

[Learn more about model provisioning →](/product/administration/model-provisioning)




## Budget Limits

**Budget Limits on Integrations** provide a simple way to manage your spending on AI providers (and LLMs) - giving you confidence and control over your application's costs. They act as financial guardrails, preventing unexpected AI costs across your organization. These limits cascade down to all AI Providers created from this Integration.

### Setting Budget Limits

#### Cost-Based Limits

Set a budget limit in USD that, once reached, will automatically expire the key to prevent further usage and overspending.

#### Token-Based Limits

Set a maximum number of tokens that can be consumed, allowing you to control usage independent of cost fluctuations.

<Frame>
  <img src="/images/product/model-catalog/budget-and-limits-page-model-catalog.png" alt="Budget Limit Configuration" />
</Frame>


> #### Key Considerations
>
> * Budget limits can be set as either cost-based (USD) or token-based
> * The minimum cost limit you can set is **\$1**
> * The minimum token limit you can set is **100 tokens**
> * Budget limits apply until exhausted or reset
> * Budget limits are applied only to requests made after the limit is set; they do not apply retroactively
> * Once set, budget limits **cannot be edited** by any organization member
> * Budget limits work for **all AI provider** created on Portkey using the given **integration**


### Alert Thresholds

You can now set alert thresholds to receive notifications before your budget limit is reached:

* For cost-based budgets, set thresholds in USD
* For token-based budgets, set thresholds in tokens
* Receive email notifications when usage reaches the threshold
* Continue using the key until the full budget limit is reached


### Periodic Reset Options

You can configure budget limits to automatically reset at regular intervals:

<Frame>
  <img src="https://mintlify.s3.us-west-1.amazonaws.com/portkey-docs/images/product/periodic-reset.png" />
</Frame>

#### Reset Period Options

* **No Periodic Reset**: The budget limit applies until exhausted with no automatic renewal
* **Reset Weekly**: Budget limits automatically reset every week
* **Reset Monthly**: Budget limits automatically reset every month


#### Reset Timing

* Weekly resets occur at the beginning of each week (Sunday at 12 AM UTC)
* Monthly resets occur on the **1st** calendar day of the month, at **12 AM UTC**, irrespective of when the budget limit was set prior


## Monitoring Your Spending and Usage

You can track your spending and token usage for any specific AI provider by navigating to the Analytics tab and filtering by the **desired key** and **timeframe**.

### Pricing Support and Limitations

Budget limits currently apply to all providers and models for which Portkey has pricing support. If a specific request log shows `0 cents` in the COST column, it means that Portkey does not currently track pricing for that model, and it will not count towards the integrtions's budget limit.

For token-based budgets, Portkey tracks both input and output tokens across all supported models.

It's important to note that budget limits cannot be applied retrospectively. The spend counter starts from zero only after you've set a budget limit for a key.


### Cascading Behavior
Integration budget limits work hierarchically:
1. **Integration Level**: Total budget for all usage through this Integration
2. **Workspace Level**: Optional per-workspace budgets (must be ≤ Integration budget)
3. **API Key Level**: Optional per-key budgets (must be ≤ Workspace budget)











## Rate Limits

Rate limits control the velocity of API usage, protecting against runaway processes and ensuring fair resource distribution across teams.

### Configuration Options

<Frame>
  <img src="/images/product/model-catalog/budget-and-limits-page-model-catalog.png" alt="Rate Limit Settings" />
</Frame>

**Limit Types:**
- **Request-based**: Limit number of API calls (e.g., 1000 requests/minute)
- **Token-based**: Limit token consumption rate (e.g., 1M tokens/hour)

**Time Windows:**
- **Per Minute**: For high-frequency applications
- **Per Hour**: For sustained workloads
- **Per Day**: For batch processing

### Rate Limit Strategy

Effective rate limiting requires balancing:
- **Performance**: Ensure limits don't throttle legitimate usage
- **Protection**: Prevent abuse and accidental loops
- **Fair Access**: Distribute capacity across teams equitably

Example configuration for a production environment:
- 10,000 requests/minute at Integration level
- 1,000 requests/minute per workspace
- 100 requests/minute per API key

[Learn more about rate limiting →](/product/administration/rate-limits)

## Next Steps

Now that you understand Integrations, you can:

1. [Create AI Providers](/product/model-catalog) using your Integrations
2. [Configure workspace access](/product/administration/workspace-provisioning) for your teams
3. [Set up monitoring](/product/observability) for usage tracking
4. [Implement budget controls](/product/administration/budget-limits) for cost management

For API-based Integration management, see our [Admin API documentation](/api-reference/admin-api/integrations).
