---
title: 'OpenAI Codex'
description: 'Add usage tracking, cost controls, and security guardrails to Codex with Portkey'
---

**Codex** is OpenAI’s coding agent that runs in the terminal, IDE extension, and CLI. It uses a shared config (user-level `~/.codex/config.toml` and optional project-level `.codex/config.toml`) to set the default model, approval policies, sandbox settings, and provider details. Add Portkey to get:

- **1600+ LLMs** through one interface — switch providers by changing the model in config
- **Observability** — track costs, tokens, and latency for every request
- **Reliability** — automatic fallbacks, retries, and caching
- **Governance** — budget limits, usage tracking, and team access controls

This guide shows how to configure Codex with Portkey in a few minutes.

<Note>
  For enterprise deployments across teams, see [Enterprise Governance](#3-enterprise-governance).
</Note>

## 1. Setup

<Steps>
<Step title="Add Provider">
Go to [Model Catalog](https://app.portkey.ai/model-catalog) → **Add Provider**.

<Frame>
<img src="/Screenshot2025-07-21at5.29.57PM.png" width="500"/>
</Frame>
</Step>

<Step title="Configure Credentials">
Select your provider (OpenAI, Anthropic, etc.), enter your API key, and create a slug like `openai-prod`.

<Frame>
<img src="/images/product/model-catalog/create-provider-page.png" width="500"/>
</Frame>
</Step>

<Step title="Get Portkey API Key">
Go to [API Keys](https://app.portkey.ai/api-keys) and generate your Portkey API key.
</Step>
</Steps>

## 2. Configure Portkey in Codex

Codex reads configuration from **TOML** files. User-level config lives at `~/.codex/config.toml`; you can override per project with `.codex/config.toml` in the repo (see [Config basics](https://developers.openai.com/codex/config-basic) for precedence).

Create or edit `~/.codex/config.toml` and add Portkey as the provider. Use `model_provider` (the provider id from `model_providers`) and define Portkey under `[model_providers.portkey]` with `base_url` and `env_key` per the [Config reference](https://developers.openai.com/codex/config-reference):

```toml
model_provider = "portkey"
model = "@openai-prod/gpt-4o"

[model_providers.portkey]
name = "Portkey"
base_url = "https://api.portkey.ai/v1"
env_key = "PORTKEY_API_KEY"
```

Set the API key in your environment:

```shell
export PORTKEY_API_KEY="your-portkey-api-key"
```

<Note>
Add to `~/.zshrc` or `~/.bashrc` for persistence.
</Note>

Test the integration:

```shell
codex "explain this repository to me"
```

Monitor usage in the [Portkey Dashboard](https://app.portkey.ai/dashboard).

## 3. Change providers and models

Codex uses the `model` value in `config.toml` to decide which model to call. With Portkey, the model is a **virtual key** in the form `@<provider-slug>/<model-id>`. Change providers or models by updating `model` in your config.

**Examples:**

```toml
# OpenAI
model = "@openai-prod/gpt-4o"

# Anthropic
model = "@anthropic-prod/claude-3-5-sonnet-20241022"

# Google
model = "@google-prod/gemini-2.0-flash-exp"
```

Use the provider slugs you created in the [Model Catalog](https://app.portkey.ai/model-catalog). After editing `~/.codex/config.toml` (or `.codex/config.toml` in the project), the next Codex run uses the new model.

<Note>
**Fallbacks, load balancing, or caching?** Create a [Portkey Config](/product/ai-gateway/configs), attach it to your API key, and set `model` to the config’s virtual model. See [Enterprise Governance](#3-enterprise-governance) for examples.
</Note>

import AdvancedFeatures from '/snippets/portkey-advanced-features.mdx';

<AdvancedFeatures />
