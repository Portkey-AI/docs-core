---
title: 'OpenAI Codex'
description: 'Add usage tracking, cost controls, and security guardrails to Codex with Portkey'
---

**Codex** is OpenAI’s coding agent that runs in the terminal, IDE extension, and CLI. It uses a shared config (user-level `~/.codex/config.toml` and optional project-level `.codex/config.toml`) to set the default model, approval policies, sandbox settings, and provider details. Add Portkey to get:

- **1600+ LLMs** through one interface — switch providers by changing the model in config
- **Observability** — track costs, tokens, and latency for every request
- **Reliability** — automatic fallbacks, retries, and caching
- **Governance** — budget limits, usage tracking, and team access controls

This guide shows how to configure Codex with Portkey in a few minutes.

<Note>
  For enterprise deployments across teams, see [Enterprise Governance](#3-enterprise-governance).
</Note>

## 1. Setup

<Steps>
<Step title="Add Provider">
Go to [Model Catalog](https://app.portkey.ai/model-catalog) → **Add Provider**.

<Frame>
<img src="/Screenshot2025-07-21at5.29.57PM.png" width="500"/>
</Frame>
</Step>

<Step title="Configure Credentials">
Select your provider (OpenAI, Anthropic, etc.), enter your API key, and create a slug like `openai-prod`.

<Frame>
<img src="/images/product/model-catalog/create-provider-page.png" width="500"/>
</Frame>
</Step>

<Step title="Get Portkey API Key">
Go to [API Keys](https://app.portkey.ai/api-keys) and generate your Portkey API key.
</Step>
</Steps>

## 2. Configure Portkey in Codex

Codex reads configuration from **TOML** files. User-level config lives at `~/.codex/config.toml`; you can override per project with `.codex/config.toml` in the repo (see [Config basics](https://developers.openai.com/codex/config-basic) for precedence).

Create or edit `~/.codex/config.toml` and add Portkey as the provider. Use `model_provider` (the provider id from `model_providers`) and define Portkey under `[model_providers.portkey]` with `base_url` and `env_key` per the [Config reference](https://developers.openai.com/codex/config-reference):

```toml
model_provider = "portkey"
model = "@openai-prod/gpt-4o"

[model_providers.portkey]
name = "Portkey"
base_url = "https://api.portkey.ai/v1"
env_key = "PORTKEY_API_KEY"
# wire_api = "chat"   # optional: "chat" (default) or "responses"
```

Set the API key in your environment:

```shell
export PORTKEY_API_KEY="your-portkey-api-key"
```

<Note>
Add to `~/.zshrc` or `~/.bashrc` for persistence.
</Note>

Test the integration:

```shell
codex "explain this repository to me"
```

Monitor usage in the [Portkey Dashboard](https://app.portkey.ai/dashboard).

## 3. Change providers and models

Codex uses the `model` value in `config.toml` to decide which model to call. With Portkey, the model is a **virtual key** in the form `@<provider-slug>/<model-id>`. Change providers or models by updating `model` in your config.

Example:

```toml
model = "@openai-prod/gpt-4o"
```

In the [Model Catalog](https://app.portkey.ai/model-catalog), use the copy button next to a provider or virtual model to copy its virtual key to the clipboard, then paste it into `model`. After editing `~/.codex/config.toml` (or `.codex/config.toml` in the project), the next Codex run uses the new model.

### Provider and model options

Tune the Portkey provider and the active model with these options in `config.toml`. All keys follow the [Codex config reference](https://developers.openai.com/codex/config-reference).

**Provider protocol (`wire_api`)**  
Under `[model_providers.portkey]`, set `wire_api` to choose which API protocol Codex uses when talking to the provider:

| Value       | Description |
| ----------- | ----------- |
| `"chat"`    | Chat Completions API (default if omitted). Use for standard chat/completion models. |
| `"responses"` | [Responses API](https://developers.openai.com/docs/guides/responses). Use for models that support structured reasoning and tool use via the Responses API. |

Example with protocol and optional retry/timeout tuning:

```toml
[model_providers.portkey]
name = "Portkey"
base_url = "https://api.portkey.ai/v1"
env_key = "PORTKEY_API_KEY"
wire_api = "responses"
# request_max_retries = 4
# stream_idle_timeout_ms = 300000
```

**Reasoning and behavior (top-level)**  
These apply to the current session model; they are especially relevant when using reasoning-capable models or the Responses API:

| Key | Values | Description |
| --- | ------ | ----------- |
| `model_reasoning_effort` | `minimal`, `low`, `medium`, `high`, `xhigh` | How much reasoning effort the model uses (Responses API). Higher values can improve quality and latency. `xhigh` is model-dependent. |
| `model_reasoning_summary` | `auto`, `concise`, `detailed`, `none` | How much reasoning summary to include or to disable summaries. |
| `personality` | `none`, `friendly`, `pragmatic` | Default communication style for models that support it. Overridable per thread or via `/personality` in-session. |

Example:

```toml
model_provider = "portkey"
model = "@openai-prod/gpt-4o"

model_reasoning_effort = "high"
model_reasoning_summary = "concise"
personality = "pragmatic"

[model_providers.portkey]
name = "Portkey"
base_url = "https://api.portkey.ai/v1"
env_key = "PORTKEY_API_KEY"
wire_api = "chat"
```

Use `wire_api = "responses"` when your Portkey virtual model is backed by a Responses API–capable model; pair it with `model_reasoning_effort` and `model_reasoning_summary` as needed.

<Note>
**Fallbacks, load balancing, or caching?** Create a [Portkey Config](/product/ai-gateway/configs), attach it to your API key, and set `model` to the config’s virtual model. See [Enterprise Governance](#3-enterprise-governance) for examples.
</Note>

import AdvancedFeatures from '/snippets/portkey-advanced-features.mdx';

<AdvancedFeatures />
