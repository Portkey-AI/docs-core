---
title: "Any OpenAI-Compatible Project"
description: "Add Portkey to any OpenAI-compatible app—just change 2 settings."
---

Portkey works with any tool or application that supports OpenAI-compatible APIs. Add enterprise features—observability, reliability, cost controls—with just 2 configuration changes.

## Quick Start

Find your project's OpenAI settings and update:

1. **Base URL:** `https://api.portkey.ai/v1`
2. **API Key:** Your Portkey API key

That's it! Your app now routes through Portkey.

<Frame caption="All requests appear in Portkey logs">
  <img src="/images/integrations/observability.png" width="600"/>
</Frame>

You now get:
- ✅ Full observability (costs, latency, logs)
- ✅ Access to 250+ LLM providers
- ✅ Automatic fallbacks and retries
- ✅ Budget controls per team/project

## Why Add Portkey?

<CardGroup cols={2}>
  <Card title="Enterprise Observability" icon="chart-line">
    Every request logged with costs, latency, tokens. Track usage across teams.
  </Card>
  <Card title="Multi-Provider Access" icon="shuffle">
    Switch between OpenAI, Anthropic, Google, and 250+ models without code changes.
  </Card>
  <Card title="Production Reliability" icon="shield-check">
    Automatic fallbacks, retries, load balancing—configured once, works everywhere.
  </Card>
  <Card title="Cost & Access Control" icon="dollar-sign">
    Budget limits per team. Rate limiting. Centralized credential management.
  </Card>
</CardGroup>

## Setup

### 1. Add Provider in Model Catalog

1. Go to [**Model Catalog → Add Provider**](https://app.portkey.ai/model-catalog/providers)
2. Select your provider (OpenAI, Anthropic, Google, etc.)
3. Choose existing credentials or create new by entering your API keys
4. Name your provider (e.g., `openai-prod`)

Your provider slug will be **`@openai-prod`** (or whatever you named it).

<Card title="Complete Model Catalog Guide →" href="/product/model-catalog">
  Set up budgets, rate limits, and manage credentials
</Card>

### 2. Get Portkey API Key

Create your Portkey API key at [app.portkey.ai/api-keys](https://app.portkey.ai/api-keys)

### 3. Configure Your Application

Most OpenAI-compatible apps have settings for:

**Base URL / Endpoint**
```
https://api.portkey.ai/v1
```

**API Key**
```
Your Portkey API Key (from step 2)
```

**Model** (if configurable)
```
@openai-prod/gpt-4o
```

If your app requires the OpenAI format (just model name), use a Portkey config with your default model.

## Common Integration Patterns

### Pattern 1: Direct Configuration (Recommended)

If your app allows custom base URL and API key:

```
Base URL: https://api.portkey.ai/v1
API Key: PORTKEY_API_KEY
Model: @openai-prod/gpt-4o
```

### Pattern 2: With Config

If your app only accepts model names like `gpt-4o`:

1. Create a config in [Portkey dashboard](https://app.portkey.ai/configs):

```json
{
  "override_params": {
    "model": "@openai-prod/gpt-4o"
  }
}
```

2. Use the config in your app:

```
Base URL: https://api.portkey.ai/v1
API Key: PORTKEY_API_KEY
Model: gpt-4o  (the config will override this)
```

Add config to API key defaults or pass via header.

### Pattern 3: Environment Variables

Many apps use environment variables:

```bash
OPENAI_API_BASE=https://api.portkey.ai/v1
OPENAI_API_KEY=PORTKEY_API_KEY
OPENAI_MODEL=@openai-prod/gpt-4o
```

## Switching Providers

Change the model string to switch providers:

```
@openai-prod/gpt-4o        # OpenAI
@anthropic-prod/claude-sonnet-4    # Anthropic
@google-prod/gemini-2.0-flash      # Google
```

All without changing your application code!

## Advanced Features via Configs

For production features like fallbacks, caching, and load balancing:

1. Create a config in [Portkey dashboard](https://app.portkey.ai/configs)
2. Attach it to your API key defaults
3. Or pass via header: `x-portkey-config: your-config-id`

**Example config with fallbacks:**
```json
{
  "strategy": {"mode": "fallback"},
  "targets": [
    {"override_params": {"model": "@openai-prod/gpt-4o"}},
    {"override_params": {"model": "@anthropic-prod/claude-sonnet-4"}}
  ]
}
```

<Card title="Learn About Configs →" href="/product/ai-gateway/configs">
  Fallbacks, retries, caching, load balancing, and more
</Card>

import AdvancedFeatures from '/snippets/portkey-advanced-features.mdx';

<AdvancedFeatures />