---
title: "Any OpenAI-Compatible Project"
description: "Add Portkey to any OpenAI-compatible app—just change 2 settings."
---

Portkey works with any tool or application that supports OpenAI-compatible APIs. Add enterprise features—observability, reliability, cost controls—with just 2 configuration changes.

## Quick Start

Find your project's OpenAI settings and update:

1. **Base URL:** `https://api.portkey.ai/v1`
2. **API Key:** Your Portkey API key

That's it! Your app now routes through Portkey.

<Frame caption="All requests appear in Portkey logs">
  <img src="/images/integrations/observability.png" width="600"/>
</Frame>

You now get:
- ✅ Full observability (costs, latency, logs)
- ✅ Access to 250+ LLM providers
- ✅ Automatic fallbacks and retries
- ✅ Budget controls per team/project

## Why Add Portkey?

<CardGroup cols={2}>
  <Card title="Enterprise Observability" icon="chart-line">
    Every request logged with costs, latency, tokens. Track usage across teams.
  </Card>
  <Card title="Multi-Provider Access" icon="shuffle">
    Switch between OpenAI, Anthropic, Google, and 250+ models without code changes.
  </Card>
  <Card title="Production Reliability" icon="shield-check">
    Automatic fallbacks, retries, load balancing—configured once, works everywhere.
  </Card>
  <Card title="Cost & Access Control" icon="dollar-sign">
    Budget limits per team. Rate limiting. Centralized credential management.
  </Card>
</CardGroup>

## Setup

### 1. Add Provider in Model Catalog

1. Go to [**Model Catalog → Add Provider**](https://app.portkey.ai/model-catalog/providers)
2. Select your provider (OpenAI, Anthropic, Google, etc.)
3. Choose existing credentials or create new by entering your API keys
4. Name your provider (e.g., `openai-prod`)

Your provider slug will be **`@openai-prod`** (or whatever you named it).

<Card title="Complete Model Catalog Guide →" href="/product/model-catalog">
  Set up budgets, rate limits, and manage credentials
</Card>

### 2. Get Portkey API Key

Create your Portkey API key at [app.portkey.ai/api-keys](https://app.portkey.ai/api-keys)

### 3. Configure Your Application

Most OpenAI-compatible apps have settings for:

**Base URL / Endpoint**
```
https://api.portkey.ai/v1
```

**API Key**
```
Your Portkey API Key (from step 2)
```

**Model** (if configurable)
```
@openai-prod/gpt-4o
```

If your app requires the OpenAI format (just model name), use a Portkey config with your default model.

## Common Integration Patterns

### Pattern 1: Direct Configuration (Recommended)

If your app allows custom base URL and API key:

```
Base URL: https://api.portkey.ai/v1
API Key: PORTKEY_API_KEY
Model: @openai-prod/gpt-4o
```

### Pattern 2: With Config

If your app only accepts model names like `gpt-4o`:

1. Create a config in [Portkey dashboard](https://app.portkey.ai/configs):

```json
{
  "override_params": {
    "model": "@openai-prod/gpt-4o"
  }
}
```

2. Use the config in your app:

```
Base URL: https://api.portkey.ai/v1
API Key: PORTKEY_API_KEY
Model: gpt-4o  (the config will override this)
```

Add config to API key defaults or pass via header.

### Pattern 3: Environment Variables

Many apps use environment variables:

```bash
OPENAI_API_BASE=https://api.portkey.ai/v1
OPENAI_API_KEY=PORTKEY_API_KEY
OPENAI_MODEL=@openai-prod/gpt-4o
```

## Switching Providers

Change the model string to switch providers:

```
@openai-prod/gpt-4o        # OpenAI
@anthropic-prod/claude-sonnet-4    # Anthropic
@google-prod/gemini-2.0-flash      # Google
```

All without changing your application code!

## Advanced Features via Configs

For production features like fallbacks, caching, and load balancing:

1. Create a config in [Portkey dashboard](https://app.portkey.ai/configs)
2. Attach it to your API key defaults
3. Or pass via header: `x-portkey-config: your-config-id`

**Example config with fallbacks:**
```json
{
  "strategy": {"mode": "fallback"},
  "targets": [
    {"override_params": {"model": "@openai-prod/gpt-4o"}},
    {"override_params": {"model": "@anthropic-prod/claude-sonnet-4"}}
  ]
}
```

<Card title="Learn About Configs →" href="/product/ai-gateway/configs">
  Fallbacks, retries, caching, load balancing, and more
</Card>

## Enterprise Governance

For teams and organizations:

### Budget Controls

Set budget limits per team or project:

1. Go to [Model Catalog](https://app.portkey.ai/model-catalog)
2. Create separate providers for each team
3. Set budget and rate limits
4. Distribute team-specific API keys

<Card title="Budget Limits Guide →" href="/product/model-catalog/integrations#3-budget-%26-rate-limits">
  Set up cost controls and alerts
</Card>

### Access Management

Control who can access which models:

- **Model provisioning** - Enable/disable models per provider
- **API key scopes** - Limit what each key can do
- **Team workspaces** - Isolate teams with separate workspaces

<Card title="Access Control Guide →" href="/product/enterprise-offering/access-control-management">
  Set up RBAC and team permissions
</Card>

### Usage Tracking

Track usage by team, project, or user:

- Add metadata to API keys
- Filter logs by metadata tags
- Monitor costs per team
- Set up alerts for unusual usage

<Card title="Metadata Guide →" href="/product/observability/metadata">
  Track and filter by custom tags
</Card>

## Common Applications

Portkey works with:

✅ **AI Tools** - Cursor, Windsurf, Cline, Continue  
✅ **Chat UIs** - Open WebUI, LibreChat, AnythingLLM  
✅ **Automation** - n8n, Make, Zapier  
✅ **No-Code** - LangFlow, Flowise, Dify  
✅ **Frameworks** - Any OpenAI-compatible SDK  
✅ **Custom Apps** - Your own applications

See specific integration guides:

<CardGroup cols={2}>
  <Card title="Cursor" href="/integrations/libraries/cursor">
    AI code editor integration
  </Card>
  <Card title="Cline" href="/integrations/libraries/cline">
    VS Code AI assistant
  </Card>
  <Card title="Open WebUI" href="/integrations/libraries/openwebui">
    Self-hosted chat interface
  </Card>
  <Card title="n8n" href="/integrations/libraries/n8n">
    Workflow automation
  </Card>
</CardGroup>

## Troubleshooting

### Can't find OpenAI settings?

Look for sections labeled:
- "OpenAI Compatible"
- "Custom Endpoint"  
- "API Configuration"
- "LLM Settings"
- "Model Provider"

### Model not working?

Make sure you're using the full provider slug:
- ✅ `@openai-prod/gpt-4o` (correct)
- ❌ `gpt-4o` (needs provider slug)

Or use a config to set the default model.

### Getting authentication errors?

1. Verify your Portkey API key is correct
2. Check that base URL is exactly: `https://api.portkey.ai/v1`
3. Make sure provider slug matches what's in Model Catalog

## Next Steps

<CardGroup cols={2}>
  <Card title="Model Catalog" icon="database" href="/product/model-catalog">
    Set up providers and budgets
  </Card>
  <Card title="Configs" icon="gear" href="/product/ai-gateway/configs">
    Configure fallbacks and routing
  </Card>
  <Card title="Observability" icon="chart-line" href="/product/observability">
    Track costs and performance
  </Card>
  <Card title="Guardrails" icon="shield" href="/product/guardrails">
    Add PII detection and filtering
  </Card>
</CardGroup>

**Questions?** Join our [Discord Community](https://portkey.sh/discord-report) or check out [more integrations](/integrations).
