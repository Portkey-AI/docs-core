---
title: "GitHub Copilot"
description: "Use Portkey with GitHub Copilot for centralized governance, observability, and cost controls."
---

GitHub Copilot is a popular AI pair programmer. By routing Copilot through Portkey, you gain enterprise-grade controls: unified model access, spend governance, observability, guardrails, and reliability features â€” without changing your editors or workflows.

<Note>
This Experimental integration. You can only use Github Copilot Chat with Portkey. Advanced Copilot features are not available yet.
</Note>


# 1. Set up Portkey

Portkey lets you use 1600+ LLMs with GitHub Copilot via a simple OpenAI-compatible endpoint. Weâ€™ll create a model routing config and attach it to a Portkey API key.

<Steps>
<Step title="Create or verify your Integration">
Go to <a href="https://app.portkey.ai/integrations">Integrations</a> and connect your provider (e.g., OpenAI, Anthropic, etc.).

1. Click **Connect** on your provider
2. Enter a **Name** and **Slug**
3. Provide provider credentials (API key and other required details)
4. Finish model provisioning

<Frame>
  <img src="/images/product/model-catalog/integrations-page.png" width="520" />
</Frame>

<Note>
On the provisioning screen, you can keep default model selections or customize them.
</Note>
</Step>

<Step title="Copy the model slug">
1. Open <a href="https://app.portkey.ai/model-catalog">Model Catalog</a> â†’ **Models**
2. Click your desired model (example: OpenAIâ€™s GPTâ€‘4o)
3. Copy its **slug** (e.g., <code>@openai-dev/gpt-4o</code>)

<Frame>
  <img src="/images/product/model-catalog/model-integration-model-box.png" width="520" />
</Frame>

<Note>
You can click **Run Test Request** here to validate your integration. If you see a permissions error, create a **User API Key** first under <a href="https://app.portkey.ai/api-keys">API Keys</a>.
</Note>
</Step>

<Step title="Create a Config in Portkey">
Create a routing config that pins your Copilot traffic to the model from the previous step.

1. Go to <a href="https://app.portkey.ai/configs">Configs</a>
2. Create a new config with:

```json
{
	"override_params": {
		"model": "@YOUR_SLUG_FROM_PREVIOUS_STEP" 
	}
}
```

3. Give it a **Name** and **Save**

<Frame>
  <img src="/images/product/model-catalog/default-config-model-catalog.png" width="640" />
</Frame>
</Step>

<Step title="Attach the Config to a Portkey API Key">
Create an API key and attach your default config.

1. Go to <a href="https://app.portkey.ai/api-keys">API Keys</a>
2. Click **Create**
3. Choose the **Config** you created above
4. Save your API key securely

<Frame>
  <img src="/images/integrations/api-key.png" width="320" />
</Frame>

<Card title="Enforce Default Configs" href="/product/administration/enforce-default-config">
Learn how to enforce the attached config and optionally disable overrides.
</Card>
</Step>
</Steps>

<Check>
ðŸŽ‰ Step 1 complete! You now have a Portkey API key with a default config that selects your model.
</Check>


# 2. Integrate Portkey with GitHub Copilot

Copilot lets you manage models by provider. Weâ€™ll configure it via the **Azure** provider option and point it to Portkeyâ€™s OpenAI-compatible endpoint.

<Steps>
<Step title="Open Manage Models in Copilot">
1. In the Copilot chat view, click the **CURRENT-MODEL** dropdown
2. Click **Manage Modelsâ€¦**


<Frame>
  <img src="/images/libraries/github-copilot-1.webp" width="520" />
</Frame>




Youâ€™ll see a list of providers.


<Frame>
  <img src="/images/libraries/github-copilot-providers.png" width="520" />
</Frame>

</Step>

<Step title="Select Azure and configure a new model">
1. Choose **Azure**
2. Click the **gear** icon next to Azure
3. Click **Configure models** â†’ **Add a new model**

Fill in the details:

- **Identifier**: a unique key for this model, e.g., <code>portkey-model</code>
- **Display name**: e.g., <code>Custom Portkey Model</code>
- **API endpoint URL**: <code>https://api.portkey.ai/v1/chat/completions</code>
- **Capabilities**: enable <em>Tools</em>, <em>Vision</em>, and <em>Thinking</em> (as needed for your use)
- **Maximum context tokens**: use your providerâ€™s documented limit; keep defaults if unsure
- **Maximum output tokens**: set per your usage; adjust later if needed

<Frame>
  <img src="/images/libraries/github-copilot-2.png" width="520" />
</Frame>




After saving, you should see your **Custom Portkey Model** in Copilotâ€™s model list.
</Step>

<Step title="Provide your Portkey API key">
1. From **Manage Modelsâ€¦**, select **Azure**
2. Pick the model you just created
3. In the **API Keys** section, paste your **Portkey Workspace API Key** (the one with the default config from Step 1)
4. Save


<Frame>
  <img src="/images/libraries/github-copilot-2.png" width="520" />
</Frame>




<Frame>
  <img src="/images/libraries/github-copilot-models.png" width="520" />
</Frame>




You can now use your Portkey-routed model in Copilot chat.
</Step>
</Steps>

<Check>
âœ… Copilot is now integrated with Portkey. Your requests will go through Portkey with the configured routing, guardrails, and analytics.
</Check>




# Portkey Features
Now that you have enterprise-grade Github Copilot Chat setup, let's explore the comprehensive features Portkey provides to ensure secure, efficient, and cost-effective AI agent operations.

### 1. Comprehensive Metrics
Using Portkey you can track 40+ key metrics including cost, token usage, response time, and performance across all your LLM providers in real time. You can also filter these metrics based on custom metadata that you can set in your configs. Learn more about metadata here.

<Frame>
  <img src="/images/integrations/observability.png" width="600"/>
</Frame>

### 2. Advanced Logs
Portkey's logging dashboard provides detailed logs for every request made by Github Copilot Chat. These logs include:
- Complete request and response tracking for debugging
- Metadata tags for filtering by team or project
- Cost attribution per task
- Complete conversation history with the AI agent

<Frame>
<img src="/images/llms/openai/logs.png"></img>
</Frame>




### 3. Unified Access to 1600+ LLMs

You can easily switch between 1600+ LLMs. Call various LLMs such as Anthropic, Gemini, Mistral, Azure OpenAI, Google Vertex AI, AWS Bedrock, and many more by simply changing the `virtual key` in your default `config` object.


### 4. Advanced Metadata Tracking
Using Portkey, you can add custom metadata to your LLM requests for detailed tracking and analytics. Use metadata tags to filter logs, track usage, and attribute costs across engineering teams and projects.

<Card title="Custom Metadata" icon="coins" href="/product/observability/metadata">
</Card>



### 5. Enterprise Access Management

<CardGroup cols={2}>
<Card title="Budget Controls" icon="coins" href="/product/ai-gateway/virtual-keys/budget-limits">
Set and manage spending limits across teams and departments. Control costs with granular budget limits and usage tracking.
</Card>

<Card title="Single Sign-On (SSO)" icon="key" href="/product/enterprise-offering/org-management/sso">
Enterprise-grade SSO integration with support for SAML 2.0, Okta, Azure AD, and custom providers for secure authentication.
</Card>

<Card title="Organization Management" icon="building" href="/product/enterprise-offering/org-management">
Hierarchical organization structure with workspaces, teams, and role-based access control for enterprise-scale deployments.
</Card>

<Card title="Access Rules & Audit Logs" icon="shield-check" href="/product/enterprise-offering/access-control-management#audit-logs">
Comprehensive access control rules and detailed audit logging for security compliance and usage tracking.
</Card>
</CardGroup>


### 6. Reliability Features
<CardGroup cols={3}>
  <Card title="Fallbacks" icon="life-ring" href="/product/ai-gateway/fallbacks">
    Automatically switch to backup targets if the primary target fails.
  </Card>
  <Card title="Conditional Routing" icon="route" href="/product/ai-gateway/conditional-routing">
    Route requests to different targets based on specified conditions.
  </Card>
  <Card title="Load Balancing" icon="key" href="/product/ai-gateway/load-balancing">
      Distribute requests across multiple targets based on defined weights.
  </Card>
  <Card title="Caching" icon="database" href="/product/ai-gateway/cache-simple-and-semantic">
    Enable caching of responses to improve performance and reduce costs.
  </Card>
  <Card title="Smart Retries" icon="database" href="/product/ai-gateway/automatic-retries">
Automatic retry handling with exponential backoff for failed requests
</Card>
<Card title="Budget Limits" icon="shield-check" href="/product/ai-gateway/virtual-keys/budget-limits">
    Set and manage budget limits across teams and departments. Control costs with granular budget limits and usage tracking.
</Card>
</CardGroup>



### 7. Advanced Guardrails

Protect your codebase and enhance reliability with real-time checks on LLM inputs and outputs. Leverage guardrails to:
- Prevent sensitive code or API key leaks
- Enforce compliance with coding standards
- PII detection and masking in generated code
- Content filtering for inappropriate code generation
- Custom security rules for your organization
- Compliance checks for internal coding policies

<Card title="Guardrails" icon="shield-check" href="/product/guardrails">
Implement real-time protection for your AI agent interactions with automatic detection and filtering of sensitive content, PII, and custom security rules. Enable comprehensive data protection while maintaining compliance with organizational policies.
</Card>



# FAQs

<AccordionGroup>
  <Accordion title="What endpoint should I use in Copilot?">
Use <code>https://api.portkey.ai/v1/chat/completions</code> for OpenAI-compatible chat completions.
  </Accordion>
  <Accordion title="Which capabilities should I enable?">
Enable the ones your workflows need (Tools/Function Calling, Vision, Thinking). You can change them later.
  </Accordion>
  <Accordion title="How do I enforce a fixed model or routing policy?">
Attach a default config to the API key and optionally disable overrides. See <a href="/product/administration/enforce-default-config">Enforcing Default Configs</a>.
  </Accordion>
  <Accordion title="How do I track costs per team?">
Issue separate API keys or use metadata. Monitor in the analytics dashboard.
  </Accordion>
</AccordionGroup>


# Next Steps

 **Join our Community**
- [Discord Community](https://portkey.sh/discord-report)
- [GitHub Repository](https://github.com/Portkey-AI)

<Note>
For enterprise support or custom features, contact our <a href="https://calendly.com/portkey-ai">enterprise team</a>.
</Note>


