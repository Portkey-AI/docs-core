---
title: 'OpenClaw'
description: 'Connect OpenClaw to Portkey for observability, cost tracking, and production reliability'
---

[OpenClaw](https://openclaw.ai) is an open-source AI assistant with persistent memory and multi-platform access (Telegram, WhatsApp, Discord, CLI). When you route OpenClaw through Portkey, every LLM request gets logged with full prompt/response details, real-time costs, and latency metrics—all in one dashboard. You also get automatic failovers, caching, and guardrails without changing how OpenClaw works.

**What you get immediately:** Connect OpenClaw once. See every request, track costs, and switch between 200+ models from a single config. No code changes to OpenClaw itself.

---

## Quick Start

<Steps>
<Step title="Install OpenClaw">
```bash
# macOS
brew install openclaw/tap/openclaw

# Other platforms
curl -fsSL https://openclaw.ai/install.sh | sh
```

Verify:
```bash
openclaw --version
```
</Step>

<Step title="Get your Portkey API key">
[Create an API key](https://app.portkey.ai/api-keys) from the Portkey dashboard. Copy it—you'll need it in Step 4.
</Step>

<Step title="Add your LLM provider to Portkey">
Go to [Providers](https://app.portkey.ai/providers) → **Add Provider** → paste your Anthropic or OpenAI credentials.

Note the **slug** you create (e.g., `anthropic`). This identifies the provider in your OpenClaw config.

<Frame caption="Create a provider with a memorable slug">
<img src="/images/product/model-catalog/create-provider-page.png" width="500"/>
</Frame>
</Step>

<Step title="Configure OpenClaw to use Portkey">
Open your OpenClaw config:

```bash
openclaw config edit
```

Add Portkey as a provider:

```json5
{
  agents: {
    defaults: { 
      model: { primary: "portkey/@anthropic/claude-sonnet-4-5" }
    }
  },
  models: {
    mode: "merge",
    providers: {
      portkey: {
        baseUrl: "https://api.portkey.ai/v1",
        apiKey: "${PORTKEY_API_KEY}",
        api: "openai-completions",
        models: [
          { id: "@anthropic/claude-sonnet-4-5", name: "Claude Sonnet 4.5" },
          { id: "@openai/gpt-4o", name: "GPT-4o" }
        ]
      }
    }
  }
}
```

<Note>
**Model format:** `@provider-slug/model-id`. The slug must match what you created in Providers.
</Note>
</Step>

<Step title="Set your Portkey API key">
```bash
openclaw config set env.PORTKEY_API_KEY "pk-xxx"
```

Replace `pk-xxx` with your actual Portkey API key from Step 2.
</Step>

<Step title="Test the connection">
```bash
openclaw
> What's 2+2?
```

Open [Portkey Logs](https://app.portkey.ai/logs). You should see your request with cost, latency, and full prompt/response.

<Frame caption="Every OpenClaw request now appears in Portkey with full observability">
<img src="/images/openclaw-1.png" />
</Frame>
</Step>
</Steps>

---

## Debugging Requests

Portkey captures every request OpenClaw makes—the exact prompt sent (including memory context), the response received, token counts, latency, and cost. When something goes wrong, you have full visibility into what happened.

### View request details

Open [Portkey Logs](https://app.portkey.ai/logs) and click any request to see:
- **Prompt sent** — Complete messages array with system instructions and memory
- **Response** — Full completion with finish reason
- **Tokens** — Input, output, and total counts
- **Latency** — Time to first token and total duration
- **Cost** — Calculated from token usage and model pricing

### Filter logs

Find specific requests:
- **Time range** — Last hour, today, or custom range
- **Model** — Compare Claude vs GPT requests
- **Status** — Filter for errors (4xx, 5xx)
- **Metadata** — Custom tags (see below)

### Tag requests for easier debugging

Add metadata to identify specific OpenClaw features or users. Edit your Portkey config in OpenClaw:

```json5
{
  models: {
    providers: {
      portkey: {
        baseUrl: "https://api.portkey.ai/v1",
        apiKey: "${PORTKEY_API_KEY}",
        api: "openai-completions",
        headers: {
          "x-portkey-metadata": "{\"feature\":\"memory-recall\",\"user\":\"${USER}\"}"
        },
        models: [
          { id: "@anthropic/claude-sonnet-4-5", name: "Claude Sonnet 4.5" }
        ]
      }
    }
  }
}
```

Now filter logs in the dashboard by `feature:memory-recall` to isolate memory-related requests.

---

## Managing Costs

Portkey calculates costs in real-time based on token usage and model pricing. No waiting for end-of-month provider bills.

### View costs

The [Portkey dashboard](https://app.portkey.ai/analytics) shows:
- **Per-request costs** — Exact spend for each OpenClaw interaction
- **Daily/weekly/monthly totals** — Track spend over time
- **By model** — Compare Claude vs GPT costs
- **By metadata** — Filter by feature or user (if you added tags)

### Set budget limits

Prevent runaway costs:

1. Go to [API Keys](https://app.portkey.ai/api-keys)
2. Click your key → **Edit** → **Budget Limits**
3. Set a monthly limit (e.g., $50)

When the limit is reached, requests return a `429` error and OpenClaw will show an error message. You won't get a surprise bill.

---

## Improving Reliability

Portkey adds automatic retries, failovers, and caching to make OpenClaw more reliable—without changing OpenClaw's code.

### Create a reliability config

1. Go to [Configs](https://app.portkey.ai/configs) in Portkey
2. Click **Create Config**
3. Add retries and fallback:

```json
{
  "retry": {
    "attempts": 3,
    "on_status_codes": [429, 500, 502, 503, 504]
  },
  "strategy": { "mode": "fallback" },
  "targets": [
    { "provider": "anthropic" },
    { "provider": "openai" }
  ]
}
```

4. Save and copy the **config ID** (e.g., `pc-config-abc123`)

### Apply the config to your OpenClaw

Add the config to your Portkey provider in OpenClaw:

```json5
{
  models: {
    providers: {
      portkey: {
        baseUrl: "https://api.portkey.ai/v1",
        apiKey: "${PORTKEY_API_KEY}",
        api: "openai-completions",
        headers: {
          "x-portkey-config": "pc-config-abc123"  // Your config ID
        },
        models: [
          { id: "@anthropic/claude-sonnet-4-5", name: "Claude Sonnet 4.5" }
        ]
      }
    }
  }
}
```

**What this does:**
- Retries failed requests 3 times on transient errors
- If Anthropic fails, automatically routes to OpenAI
- OpenClaw keeps working even during provider outages

### Add caching

OpenClaw's memory recall often sends similar queries. Cache semantically similar requests to cut costs:

```json
{
  "cache": { "mode": "semantic", "max_age": 3600 }
}
```

Add this to your config in Portkey. Repeated questions like "What did we discuss about deployment?" will return cached responses. Typical savings: 30-50% on read-heavy workflows.

<Card title="Learn more about Configs" icon="route" href="/product/ai-gateway/configs">
Combine retries, fallbacks, caching, and load balancing in a single config.
</Card>

---

## Managing Teams

When multiple people use OpenClaw, you'll want separate access, budgets, and visibility.

### Create separate API keys

Issue one API key per person or team:

1. Go to [API Keys](https://app.portkey.ai/api-keys) → **Create Key**
2. Name it after the user/team (e.g., "Alice - Engineering")
3. Set a budget limit (e.g., $50/month)
4. Give this key to the user for their OpenClaw config

Each key has its own budget, and logs filter by key so you see who made which requests. Revoke a key instantly without affecting others.

### Use workspaces for larger teams

[Workspaces](/product/enterprise-offering/workspaces) isolate:
- API keys
- Provider credentials
- Configs and guardrails
- Logs and analytics

Create a workspace per team. Each team sees only their own data.

---

## Adding Guardrails

Guardrails check inputs and outputs before they reach the LLM or your users—blocking PII, filtering toxic content, or validating structured outputs.

### Common guardrails for OpenClaw

| Guardrail | Use case |
|-----------|----------|
| **PII Detection** | Block or redact personal information |
| **Content Moderation** | Filter harmful or off-topic outputs |
| **JSON Validation** | Ensure structured agent outputs match your schema |
| **Topic Restriction** | Keep conversations within allowed subjects |

### Apply guardrails

1. Create guardrails in the [Portkey dashboard](/product/guardrails)
2. Add them to your config:

```json
{
  "before_request_hooks": [{ "id": "guardrail-pii-check" }],
  "after_request_hooks": [{ "id": "guardrail-content-filter" }]
}
```

3. Attach the config to your API key or add it to OpenClaw's headers (same as the reliability config above)

Requests that fail guardrails return a `446` status. You can configure fallback behavior—for example, route PII-containing requests to an on-premises model instead.

<Card title="Guardrails documentation" icon="shield-halved" href="/product/guardrails">
See all available guardrails and setup instructions.
</Card>

---

## Troubleshooting

| Symptom | Likely cause | Fix |
|---------|--------------|-----|
| Requests not in logs | Wrong `baseUrl` | Use `https://api.portkey.ai/v1` |
| `401 Unauthorized` | Invalid API key | Regenerate key in Portkey dashboard |
| `Model not found` | Wrong model format | Use `@slug/model-id` format |
| `429 Too Many Requests` | Budget or rate limit hit | Check API key limits |
| High latency | No caching enabled | Add semantic cache to your config |

### Debug commands

Test your Portkey connection:
```bash
curl -H "x-portkey-api-key: $PORTKEY_API_KEY" \
  https://api.portkey.ai/v1/models
```

View your OpenClaw config:
```bash
openclaw config get models.providers.portkey
```

Check Portkey logs:
```bash
open https://app.portkey.ai/logs
```

---

## Next Steps

<CardGroup cols={2}>
  <Card title="Configs" icon="route" href="/product/ai-gateway/configs">
    Advanced routing: retries, fallbacks, caching, load balancing
  </Card>
  <Card title="Guardrails" icon="shield-halved" href="/product/guardrails">
    Input/output validation and content filtering
  </Card>
  <Card title="Analytics" icon="chart-line" href="/product/observability">
    Custom dashboards, alerts, and cost analysis
  </Card>
  <Card title="Enterprise" icon="server" href="/product/enterprise-offering">
    Self-hosted gateway, SSO, and compliance
  </Card>
</CardGroup>
