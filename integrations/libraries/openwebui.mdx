---
title: 'Open WebUI'
description: 'Cost tracking, observability, and more for Open WebUI'
---
This guide will help you implement enterprise-grade security, observability, and governance for OpenWebUI using Portkey. While OpenWebUI supports various provider plugins, Portkey provides a unified interface for all your LLM providers, offering comprehensive features for model management, cost tracking, observability, and metadata logging.

For IT administrators deploying centralized instances of OpenWebUI, Portkey enables essential enterprise features including usage tracking, access controls, and budget management. Let's walk through implementing these features step by step.

# Understanding the Implementation
When implementing Portkey with OpenWebUI in your organization, we'll follow these key steps:

1. Basic OpenWebUI integration with Portkey
2. Setting up organizational governance using Virtual Keys and Configs
3. Managing user access and permissions
4. Implementing observability and analytics

<Note>
If you're an individual user just looking to use Portkey with OpenWebUI, you only need to complete Steps 1 and 2 to get started.
</Note>

## Step 1: Basic Integration 

Let's start by integrating Portkey with your OpenWebUI installation. This integration uses OpenWebUI's pipeline functionality to route all requests through Portkey's Platform.

### Installing the Portkey Plugin
1. Start your OpenWebUI server
2. Navigate to `Workspace` and then go to the `Functions` section
3. Click on the `+` button in UI
4. Copy and paste the [Portkey plugin](https://openwebui.com/f/nath/portkey/) code

## Step 2: Setting Up Portkey Pipeline

To use OpenWebUI with Portkey, you'll need to configure three key components:

### 1. Portkey API Key
Get your Portkey API key from [here](https://app.portkey.ai/api-keys). You'll need this for authentication with Portkey's services.

### 2. Virtual Keys
Virtual Keys are Portkey's secure way to manage your LLM provider API keys. They provide essential controls like:
- Budget limits for API usage
- Rate limiting capabilities
- Secure API key storage


Craeate a [Virtual Key](https://app.portkey.ai/virtual-keys) in your Portkey dashboard and save it for future use.

<Frame>
<img src="/images/integrations/openai/virtual-key-2.png" width="500"/>
</Frame>

For detailed information on budget limits, [refer to this documentation](/product/ai-gateway/virtual-keys/budget-limits)

### 3. Using Configs (Optional)
Configs in Portkey enhance your implementation with features like advanced routing, fallbacks, and retries. Here's a simple config example that implements 5 retry attempts on server errors:

```json
{
    "retry": {
        "attempts": 5
    },
    "virtual_key": "virtual-key-xxx"
}
```

Configs are highly flexible and can be customized for various use cases. Learn more in our [Configs documentation](https://docs.portkey.ai/configs).


### Step 3: Configure Pipeline Variables

The pipeline setup involves configuring both credentials and model access in OpenWebUI.

#### Credentials Setup
1. In OpenWebUI, navigate to `Workspace` â†’ `Functions`
2. Click the `Valves` button to open the configuration interface 
3. Add the following credentials:
   - Your Portkey API Key
   - Config slug (if using Configs)
   - Base URL (only needed for Open Source Gateway users)

#### Model Configuration
1. In the Functions section, click the `...` button and select `Edit`
2. Find the virtual keys JSON object in the Portkey function code
3. Update it with your virtual keys:
   ```json
   "virtual_keys": {
       "openai": "YOUR_OPENAI_VIRTUAL_KEY",
       "anthropic": "YOUR_ANTHROPIC_VIRTUAL_KEY"
   }
   ```
4. Configure model names using this format:
   ```json
   {provider_slug_from_portkey}/{model_id_from_provider}
   ```
   Example: `openai/gpt-3.5-turbo`
5. Save your changes

### Step 3: Setting Up Organizational Governance (Enterprises)
Many enterprises are choosing OpenWebUI as a secure, self-hosted alternative to ChatGPT. While OpenWebUI handles core functionality and data privacy compliance, enterprise deployments face several challenges:

- Cost control across multiple teams
- Model access management for different teams
- Usage tracking and attribution
- Security and compliance standards

Portkey adds a governance layer to address these challenges. Here's how to implement it:

#### 1: Set Budget Controls and Rate Limits 

Virtual Keys provide granular control over LLM access. Create different Virtual Keys for different departments/teams:

1. Go to the [Portkey AI dashboard](https://app.portkey.ai/virtual-keys) Virtual Keys page and creat your virtual keys
3. Set appropriate limits:
   - Monthly budget (e.g., $100 for development, $1000 for production)
   - Rate limits (e.g., 100 requests/minute for development)
4. Create and save your Virtual Key

<Frame>
<img src="/images/integrations/openai/virtual-key-2.png" width="500"/>
</Frame>

#### 2: Configure Model Routing Rules

As your OpenWebUI usage scales, you'll need rules for model access and request routing. Portkey Configs provide this control layer through JSON configuration:

```json
{
    "strategy": {
        "mode": "conditional",
        "conditions": [
            {
                "query": { "metadata.environment": { "$eq": "production" } },
                "then": "prod-gpt4"
            },
            {
                "query": { "metadata.environment": { "$eq": "development" } },
                "then": "dev-gpt35"
            }
        ]
    },
    "targets": [
        {
            "name": "prod-gpt4",
            "virtual_key": "vk-prod-xxx",
            "model": "gpt-4"
        },
        {
            "name": "dev-gpt35",
            "virtual_key": "vk-dev-xxx",
            "model": "gpt-3.5-turbo"
        }
    ],
    "metadata": {
        "required": ["email", "department", "environment"]
    }
}
```

Create your config on the [Configs page](https://app.portkey.ai/configs) in your Portkey dashboard. You'll need the config ID for connecting to OpenWebUI.



#### 3: Set Up Team Access Controls

Now that you have budget controls and routing rules, you'll need to create API keys for teams to use OpenWebUI with these controls. These keys will:
- Track usage automatically
- Apply the correct configs
- Collect metadata for each request

Create API keys through:
- [Portkey App](https://app.portkey.ai/api-keys)
- [API Key Management API](https://docs.portkey.ai/api-reference/api-keys/create)

The Portkey plugin automatically captures user email as `_user` in metadata. You can add fields like department, project, or environment for detailed tracking.

Example using Python SDK:
```python
from portkey_ai import Portkey

portkey = Portkey(api_key="YOUR_ADMIN_API_KEY")

api_key = portkey.api_keys.create(
    name="engineering-team",
    type="organisation",
    workspace_id="YOUR_WORKSPACE_ID",
    defaults={
        "config_id": "your-config-id",
        "metadata": {
            "environment": "production",
            "department": "engineering"
        }
    },
    scopes=["logs.view", "configs.read"]
)
```

For detailed key management instructions, see our [API Keys documentation](/api-reference/api-keys).

### Step 4: Deploy to OpenWebUI Instances

Apply your governance setup to OpenWebUI instances using the Basic setup steps described earlier. Your OpenWebUI deployment now has enterprise-grade governance.

# Portkey Features

### Observability and Analytics
Once integrated, Portkey provides comprehensive visibility through our analytics dashboard:

- Real-time cost tracking
- Token usage monitoring
- Response time analytics
- 40+ key metrics
- Detailed Logs and Traces

<Frame>
  <img src="/images/product/product-1.png"/>
</Frame>

### Request Logging and Tracing
Track every request with detailed metadata and performance metrics:



### Organization Management and Access Control
Portkey's hierarchical organization structure includes:

- **Organizations**: Enterprise-level containers
- **Workspaces**: Team or project-specific environments
- **Users**: Team members with defined roles

This enables:
- Centralized organizational control
- Team-level access management
- Project isolation
- Resource usage tracking
- [SSO integration](/docs-core/product/enterprise-offering.mdx)

### Central Guardrails
Security is a major concern for companies using AI. Portkey implements organization-wide safety measures using built-in Guardrails, supporting both deterministic and LLM-based guardrails:
- PII detection and redaction
- Content filtering
- Regex Check, NSFW control
- Custom prompt validation

Learn more about [guardrails here](/docs-core/product/guardrails).

## Support

Need help with integration?
- Join our [Portkey Community](https://portkey.wiki/community)
- Enterprise Support: enterprise-support@portkey.ai

This completes our comprehensive guide to implementing Portkey with OpenWebUI. For any specific questions or advanced configurations, please reach out to our support team.


















This guide will help you implement enterprise-grade security, observability, and governance for OpenWebUI using Portkey. While OpenWebUI supports various provider plugins, Portkey provides a unified interface for all your LLM providers, offering comprehensive features for model management, cost tracking, observability, and metadata logging.

For IT administrators deploying centralized instances of OpenWebUI, Portkey enables essential enterprise features including usage tracking, access controls, and budget management. Let's walk through implementing these features step by step.


# Understanding the Implementation
When implementing Portkey with OpenWebUI in your organization, we'll follow these key steps:

1. Basic OpenWebUI integration with Portkey
2. Setting up organizational governance using Virtual Keys and Configs
3. Managing user access and permissions

<Note> If you're an individual user just looking to use Portkey with OpenWebUI, you only need to complete Steps 1 and 2 to get started.</Note>

## Step 1: Basic Integration

First, we'll integrate [Portkey plugin](https://openwebui.com/f/nath/portkey/) with your OpenWebUI installation. The integration uses OpenWebUI's pipeline functionality to route all requests through Portkey's Platform.
For detailed plugin information, see the [Open WebUI plugins documentation](https://docs.openwebui.com/tutorials/plugin/functions).

### Installing the Portkey Plugin
1. Start your OpenWebUI server
2. Navigate to `Workspace` and then go to the `Functions` section
3. Click on the `+` button in UI
4. Copy and Paste the [Portkey plugin](https://openwebui.com/f/nath/portkey/) code


## Step 2: Setting Up Portkey Pipeline with Open WebUI

Before we can start using OpenWebUI with Portkey, we need to set up 3 key components:


### Portkey API Key
Get your Portkey API keys from [here](https://app.portkey.ai/api-keys) and save it for further use.

### Virtual Keys

Virtual Keys are Portkey's secure way to manage your LLM provider API keys. They act as a secure vault for your API keys while providing essential controls like budget limits and rate limits for your API usage.

To create a Virtual Key, head over to the [Virtual Keys page](https://app.portkey.ai/virtual-keys) in your Portkey dashboard ancrete. 

<Frame>
<img src="/images/integrations/openai/virtual-key-2.png" width="500"/>
</Frame>

For more information on budget limits, [refer to this documentation](/product/ai-gateway/virtual-keys/budget-limits)



### Using Configs (Optional)

While not required for basic integration, Configs in Portkey can enhance your implementation with features like advanced routing, fallbacks, retreis and more. Here's an example of a simple config that retries a request 5 times if there's an error on server:

```json
{
    "retry": {
        "attempts": 5
    },
    "virtual_key": "virtual-key-xxx"
}
```

This is just one way to use configs - they're highly flexible and can be customized for various use cases. Learn more about the possibilities in our [Configs documentation](https://docs.portkey.ai/configs).




Let me rewrite that section to be clearer and more structured while maintaining the same information.

### Step 3: Configure Pipeline Variables

Setting up your pipeline variables involves two main parts: configuring credentials and setting up model access.

**Part 1: Adding Credentials**
1. Navigate to `Workspace` â†’ `Functions` section
2. Look for the `Valves` button - this opens the UI input interface
3. Add your `Portkey API Key`
4. (Optional) If you're using the Open Source Gateway:
   - Add your Config slug
   - Add your base URL (for Open Source Gateway)

**Part 2: Configuring Models and Virtual Keys**

In the same Functions section:
1. Click the `...` (more) button
2. Select `Edit` to access the Portkey function code
3. Locate the virtual keys JSON object:
   ```json
   "virtual_keys": {
       "openai": "YOUR_OPENAI_VIRTUAL_KEY",
       "anthropic": "YOUR_ANTHROPIC_VIRTUAL_KEY"
   }
   ```
4. Add your virtual keys following this format:
   ```json
   {"provider-slug": "provider_virtual_key"}
   ```
   **Note**: Make sure to use the correct provider slug from Portkey docs

5. Configure your model names in the pipes function using:
   ```json
   {provider_slug_from_portkey}/{model_id_from_provider}
   ```
   For example: `openai/gpt-3.5-turbo`

6. Save your changes
Your pipeline is now configured with your credentials and model access settings.


### Step 3: Setting Up Organizational Governance (Enterprises)
As enterprises adopt AI, many are turning to OpenWebUI as a secure, self-hosted Open soruce alternative to ChatGPT. While OpenWebUI solves the core challenge of providing teams with a familiar chat interface while taking care of data privacy laws. Still enterprise deployments face several critical challenges:

How do you prevent cost overruns when multiple teams are using AI?
How do you ensure different teams use appropriate models for their needs?
How do you track and attribute AI usage across departments?
How do you maintain security and compliance standards?

Portkey adds a governance layer to OpenWebUI that addresses these challenges. Here's sample workflow to do the same:



#### 1: Set Budget Controls and Rate Limits 

Virtual Keys provide granular control over LLM access, enforcing budget limits and usage quotas. Create different Virtual Keys for different departments/users in your organization to manage their API usage:

Navigate to the Virtual Keys page in your [Portkey AI dashboard](https://app.portkey.ai/virtual-keys)
Select your preferred LLM provider (OpenAI, Anthropic, etc.)
Set appropriate monthly budget limits (e.g., $100 for development, $1000 for production)
Configure rate limits to prevent usage spikes (e.g., 100 requests/minute for development)
Create and save your Virtual Key

You can create multiple Virtual Keys with different limits for various teams, environments, or use cases. Each key will track its own usage and automatically enforce the set limits.
<Frame>
<img src="/images/integrations/openai/virtual-key-2.png" width="500"/>
</Frame>


#### 2: Configure Model Routing Rules

As organizations scale their OpenWebUI usage, you'll need rules to determine which teams use which models, how requests are routed, and what metadata to track. Portkey Configs provide this control layer through a simple yet powerful JSON configuration.

While Configs can handle complex routing logic, retries, fallbacks, and transformations (learn more in our [Config documentation](https://docs.portkey.ai/configs)), let's start with a basic setup that routes requests based on environment:

```json
{
    "strategy": {
        "mode": "conditional",
        "conditions": [
            {
                "query": { "metadata.environment": { "$eq": "production" } },
                "then": "prod-gpt4"
            },
            {
                "query": { "metadata.environment": { "$eq": "development" } },
                "then": "dev-gpt35"
            }
        ]
    },
    "targets": [
        {
            "name": "prod-gpt4",
            "virtual_key": "vk-prod-xxx",
            "model": "gpt-4"
        },
        {
            "name": "dev-gpt35",
            "virtual_key": "vk-dev-xxx",
            "model": "gpt-3.5-turbo"
        }
    ],
    "metadata": {
        "required": ["email", "department", "environment"]
    }
}
```

Head to the [Configs page](https://app.portkey.ai/configs) in your Portkey dashboard, create a new config with these rules, and save it. You'll need the config ID for connecting it to your OpenWebUI instances in the next step.


#### 3: Set Up Team Access Controls

Now that you have budget controls and routing rules in place, you'll need to create API keys for your teams to use OpenWebUI with these controls. These keys will automatically track usage, apply the right configs, and collect metadata for each request.

Portkey provides multiple ways to create these API keys:
- Through the [Portkey App](https://app.portkey.ai/api-keys)
- Programmatically using our [API Key Management API](https://docs.portkey.ai/api-reference/api-keys/create)

When creating keys, you can attach metadata like department and environment, which helps in:
- Usage tracking per team
- Cost allocation
- Request filtering
- Access control

Note: The Portkey plugin for OpenWebUI automatically captures the user's email as `_user` in metadata. You can add additional metadata fields like department, project, or environment for more detailed tracking.

Here's an example using our Python SDK:
```python
from portkey_ai import Portkey

portkey = Portkey(api_key="YOUR_ADMIN_API_KEY")

api_key = portkey.api_keys.create(
    name="engineering-team",
    type="organisation",
    workspace_id="YOUR_WORKSPACE_ID",
    defaults={
        "config_id": "your-config-id",
        "metadata": {
            "environment": "production",
            "department": "engineering"
        }
    },
    scopes=["logs.view", "configs.read"]
)
```

For detailed instructions on key creation and management, check our [API Keys documentation](/api-reference/api-keys).


### Step 4: Deploy to OpenWebUI Instances

Now that you have all the governance components set up, connect them to your OpenWebUI instances as mentionied in the Basic setup.

That's it! Your OpenWebUI deployment now has enterprise-grade governance.



# Portkey Features
### Observabiltiy and Analytics

Once set up, Portkey provides comprehensive visibility into your AI usage through our analytics dashboard:

Portkey provides comprehensive monitoring capabilities:
- Real-time cost tracking
- Token usage monitoring
- Response time analytics
- 40+ key metrics
- Detailed Logs and Traces

<Frame>
  <img src="/images/product/product-1.png"/>
</Frame>


### Request Logging and Tracing
Track every request with detailed metadata and performance metrics:

[Screenshot: Logs interface showing detailed request information with metadata]

### Organization Management and Access Control
Portkey provides a hierarchical organization structure:

- **Organizations**: Enterprise-level containers
- **Workspaces**: Team or project-specific environments
- **Users**: Team members with defined roles

This enables:
- Centralized organizational control
- Team-level access management
- Project isolation
- Resource usage tracking
- [SSO integration](/docs-core/product/enterprise-offering.mdx) with providers like Okta, Microsoft Azure AD, and Google Workspace

### Central Guardrails
Securty is a big concern for companies utilisng AI. Portkey solves this by Implementing organization-wide safety measures using built-in Guradrails. You can have both deterministic or llm based guradraisl for your resoponses:
- PII detection and redaction
- Content filtering
- Regex Check, NSFW control
- Custom prompt validation
Learn more about [guardrails here](/docs-core/product/guardrails).



## Support

For integration assistance:
- Community Forum: [Portkey Community](https://portkey.wiki/community)
- Enterprise Support: enterprise-support@portkey.ai