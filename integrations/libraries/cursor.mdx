---
title: 'Cursor'
description: "Add observability, governance, and reliability to Cursor with Portkey."
---

[Cursor](https://cursor.sh) is an AI-first code editor. Add Portkey to get:

- **1600+ LLMs** — Not just OpenAI & Anthropic
- **Observability** — Track costs, tokens, latency for every request
- **Governance** — Budget limits, rate limits, RBAC
- **Guardrails** — PII detection, content filtering

<Note>
When using Portkey, Cursor-specific features (autocomplete, Apply from Chat, inline refactoring) require Cursor Pro/Enterprise plans.
</Note>

<Note>
For enterprise governance, see [Enterprise Governance](#enterprise-governance).
</Note>

## 1. Setup

<Steps>
<Step title="Add Provider">
Go to [Model Catalog](https://app.portkey.ai/model-catalog) → **Add Provider**.

<Frame>
<img src="/Screenshot2025-07-21at5.29.57PM.png" width="500"/>
</Frame>
</Step>

<Step title="Configure Credentials">
Select your provider (OpenAI, Anthropic, etc.), enter your API key, and create a slug like `openai-prod`.

<Frame>
<img src="/images/product/model-catalog/create-provider-page.png" width="500"/>
</Frame>
</Step>

<Step title="Create Config">
Go to [Configs](https://app.portkey.ai/configs) and create:

```json
{
  "override_params": {
    "model": "@openai-prod/gpt-4o"
  }
}
```

Save and note the Config ID.
</Step>

<Step title="Get Portkey API Key">
Go to [API Keys](https://app.portkey.ai/api-keys) → Create new key → Attach your config → Save.
</Step>
</Steps>

## 2. Configure Cursor

1. Open **Cursor → Settings → Cursor Settings → Models**
2. Scroll to **API Keys** section
3. Enable **OpenAI API Key** toggle and enter your **Portkey API Key**
4. Enable **Override OpenAI Base URL** and enter: `https://api.portkey.ai/v1`
5. Click **Verify**

<Frame>
<img src="/images/libraries/cursor.png"/>
</Frame>

Done! Monitor usage in the [Portkey Dashboard](https://app.portkey.ai/dashboard).

---

## Enterprise Governance

For organizations using Cursor, Portkey adds governance controls:

<AccordionGroup>
<Accordion title="Budget & Rate Limits">
Create providers with spending limits per team:

1. Go to [Model Catalog](https://app.portkey.ai/model-catalog)
2. Create provider for each team with budget/rate limits

<Frame>
<img src="/images/product/model-catalog/create-provider-page.png" width="500"/>
</Frame>
</Accordion>

<Accordion title="Model Access Rules">
Control which models teams can access at the integration level.

<Frame>
<img src="/images/product/model-catalog/model-provisioning-page.png" width="500"/>
</Frame>
</Accordion>

<Accordion title="Routing Configuration">
Use Configs for fallbacks, load balancing, caching:

```json
{
  "strategy": { "mode": "loadbalance" },
  "targets": [
    { "override_params": { "model": "@openai-prod/gpt-4o" } },
    { "override_params": { "model": "@anthropic-prod/claude-sonnet-4-20250514" } }
  ]
}
```
</Accordion>

<Accordion title="Team API Keys">
Create team-specific API keys with metadata:

```python
from portkey_ai import Portkey

portkey = Portkey(api_key="YOUR_ADMIN_API_KEY")

api_key = portkey.api_keys.create(
    name="frontend-team",
    workspace_id="YOUR_WORKSPACE_ID",
    defaults={
        "config_id": "your-config-id",
        "metadata": {"team": "frontend", "environment": "production"}
    },
    scopes=["logs.view", "configs.read"]
)
```
</Accordion>
</AccordionGroup>

---

## Features

### Observability

Track 40+ metrics: cost, tokens, latency, performance. Filter by custom metadata.

<Frame>
<img src="/images/integrations/observability.png" width="600"/>
</Frame>

### Logs

Complete request/response tracking with metadata tags and cost attribution.

<Frame>
<img src="/images/llms/openai/logs.png"/>
</Frame>

### Reliability

<CardGroup cols={3}>
<Card title="Fallbacks" icon="life-ring" href="/product/ai-gateway/fallbacks">
Auto-switch on failure
</Card>
<Card title="Load Balancing" icon="balance-scale" href="/product/ai-gateway/load-balancing">
Distribute across providers
</Card>
<Card title="Caching" icon="database" href="/product/ai-gateway/cache-simple-and-semantic">
Reduce costs and latency
</Card>
<Card title="Retries" icon="rotate" href="/product/ai-gateway/automatic-retries">
Exponential backoff
</Card>
<Card title="Conditional Routing" icon="route" href="/product/ai-gateway/conditional-routing">
Route by conditions
</Card>
<Card title="Budget Limits" icon="coins" href="/product/model-catalog/integrations#3-budget-%26-rate-limits">
Control spending
</Card>
</CardGroup>

### Guardrails

Protect code and data with real-time checks:
- PII detection and masking
- Content filtering
- Custom security rules

<Card title="Guardrails" icon="shield-check" href="/product/guardrails">
Configure input/output protection
</Card>

---

## FAQs

<AccordionGroup>
<Accordion title="Can I use multiple providers with one API key?">
Yes. Create multiple providers and attach them to a single config. The config connects to your API key.
</Accordion>

<Accordion title="How do I track costs per team?">
Create separate providers per team, use metadata tags, or set up team-specific API keys.
</Accordion>

<Accordion title="What happens when budget limit is reached?">
Requests are blocked. Admins get notified. Limits can be adjusted anytime.
</Accordion>
</AccordionGroup>

---

## Next Steps

<CardGroup cols={2}>
<Card title="Model Catalog" icon="list" href="/product/model-catalog">
Set up providers and budgets
</Card>
<Card title="Configs" icon="gear" href="/product/ai-gateway/configs">
Configure routing and reliability
</Card>
</CardGroup>

**Community:** [Discord](https://portkey.sh/discord-report) · [GitHub](https://github.com/Portkey-AI)
