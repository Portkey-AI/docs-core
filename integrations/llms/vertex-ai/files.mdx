---
title: Files
description: Upload files to Google Cloud Storage for Vertex AI fine-tuning and batch inference
---

To perform fine-tuning or batch inference with Vertex AI, you need to upload files to Google Cloud Storage.
With Portkey, you can easily upload files to GCS and use them for fine-tuning or batch inference with Vertex AI models.

## Uploading Files
<Tabs>
 <Tab title="Python">
```python
from portkey_ai import Portkey

# Initialize the Portkey client
portkey = Portkey(
    api_key="PORTKEY_API_KEY",  # Replace with your Portkey API key
    provider="@VERTEX_PROVIDER",   
    vertex_storage_bucket_name="your_bucket_name", # Specify the GCS bucket name
    provider_file_name="your_file_name.jsonl", # Specify the file name in GCS
    provider_model="gemini-1.5-flash-001" # Specify the model to use
)

upload_file_response = portkey.files.create(
  purpose="fine-tune", # Can be "fine-tune" or "batch"
  file=open("dataset.jsonl", "rb")
)

print(upload_file_response)
```
  </Tab>
    <Tab title="NodeJS">
```js
import { Portkey } from 'portkey-ai';
import * as fs from 'fs';

// Initialize the Portkey client
const portkey = Portkey({
    apiKey: "PORTKEY_API_KEY",  // Replace with your Portkey API key
    provider:"@VERTEX_PROVIDER",   
    vertexStorageBucketName: "your_bucket_name", // Specify the GCS bucket name
    providerFileName: "your_file_name.jsonl", // Specify the file name in GCS
    providerModel: "gemini-1.5-flash-001" // Specify the model to use
});

const uploadFile = async () => {
  const file = await portkey.files.create({
    purpose: "fine-tune", // Can be "fine-tune" or "batch"
    file: fs.createReadStream("dataset.jsonl")
  });

  console.log(file);
}

uploadFile();
```
  </Tab>
<Tab title="REST">
```sh
curl -X POST --header 'x-portkey-api-key: <portkey_api_key>' \
 --header 'x-portkey-provider: <vertex_virtual_key>' \
 --header 'x-portkey-vertex-storage-bucket-name: <bucket_name>' \
 --header 'x-portkey-provider-file-name: <file_name>.jsonl' \
 --header 'x-portkey-provider-model: <model_name>' \
 --form 'purpose="fine-tune"' \
 --form 'file=@dataset.jsonl' \
 'https://api.portkey.ai/v1/files'
```
</Tab>
  <Tab title="OpenAI NodeJS">
```js
import OpenAI from 'openai';
import { PORTKEY_GATEWAY_URL, createHeaders } from 'portkey-ai';
import * as fs from 'fs';

const openai = new OpenAI({
  apiKey: 'OPENAI_API_KEY',
  baseURL: PORTKEY_GATEWAY_URL,
  defaultHeaders: createHeaders({
    provider:"@VERTEX_PROVIDER",
    apiKey: "PORTKEY_API_KEY",
    vertexStorageBucketName: "your_bucket_name",
    providerFileName: "your_file_name.jsonl",
    providerModel: "gemini-1.5-flash-001"
  })
});

const uploadFile = async () => {
  const file = await openai.files.create({
    purpose: "fine-tune", // Can be "fine-tune" or "batch"
    file: fs.createReadStream("dataset.jsonl")
  });

  console.log(file);
}

uploadFile();
```
  </Tab>
    <Tab title="OpenAI Python">
```python
from openai import OpenAI
from portkey_ai import PORTKEY_GATEWAY_URL, createHeaders

openai = OpenAI(
    api_key='OPENAI_API_KEY',
    base_url=PORTKEY_GATEWAY_URL,
    default_headers=createHeaders(
        provider="@VERTEX_PROVIDER",
        api_key="PORTKEY_API_KEY",
        vertex_storage_bucket_name="your_bucket_name",
        provider_file_name="your_file_name.jsonl",
        provider_model="gemini-1.5-flash-001"
    )
)

upload_file_response = openai.files.create(
  purpose="fine-tune", # Can be "fine-tune" or "batch"
  file=open("dataset.jsonl", "rb")
)

print(upload_file_response)
```
  </Tab>
 
</Tabs>


## Get File
<Tabs>
 <Tab title="Python">
```python
from portkey_ai import Portkey

# Initialize the Portkey client
portkey = Portkey(
    api_key="PORTKEY_API_KEY",  # Replace with your Portkey API key
    provider="@VERTEX_PROVIDER"   
)

file = portkey.files.retrieve(file_id="file_id")

print(file)
```
  </Tab>
    <Tab title="NodeJS">
```js
import { Portkey } from 'portkey-ai';

// Initialize the Portkey client
const portkey = Portkey({
    apiKey: "PORTKEY_API_KEY",  // Replace with your Portkey API key
    provider:"@VERTEX_PROVIDER"   
});

const getFile = async () => {
  const file = await portkey.files.retrieve("file_id");

  console.log(file);
}

getFile();
```
  </Tab>
<Tab title="REST">
```sh
curl -X GET --header 'x-portkey-api-key: <portkey_api_key>' \
--header 'x-portkey-provider: <vertex_virtual_key>' \
'https://api.portkey.ai/v1/files/<file_id>'
```
</Tab>
  <Tab title="OpenAI NodeJS">
```js
import OpenAI from 'openai';
import { PORTKEY_GATEWAY_URL, createHeaders } from 'portkey-ai';

const openai = new OpenAI({
  apiKey: 'OPENAI_API_KEY',
  baseURL: PORTKEY_GATEWAY_URL,
  defaultHeaders: createHeaders({
    provider:"@VERTEX_PROVIDER",
    apiKey: "PORTKEY_API_KEY"
  })
});

const getFile = async () => {
  const file = await openai.files.retrieve("file_id");

  console.log(file);
}

getFile();
```
  </Tab>
    <Tab title="OpenAI Python">
```python
from openai import OpenAI
from portkey_ai import PORTKEY_GATEWAY_URL, createHeaders

openai = OpenAI(
    api_key='OPENAI_API_KEY',
    base_url=PORTKEY_GATEWAY_URL,
    default_headers=createHeaders(
        provider="@VERTEX_PROVIDER",
        api_key="PORTKEY_API_KEY"
    )
)

file = openai.files.retrieve(file_id="file_id")

print(file)
```
  </Tab>
 
</Tabs>


## Get File Content
<Tabs>
 <Tab title="Python">
```python
from portkey_ai import Portkey

# Initialize the Portkey client
portkey = Portkey(
    api_key="PORTKEY_API_KEY",  # Replace with your Portkey API key
    provider="@VERTEX_PROVIDER"   
)

file_content = portkey.files.content(file_id="file_id")

print(file_content)
```
  </Tab>
    <Tab title="NodeJS">
```js
import { Portkey } from 'portkey-ai';

// Initialize the Portkey client
const portkey = Portkey({
    apiKey: "PORTKEY_API_KEY",  // Replace with your Portkey API key
    provider:"@VERTEX_PROVIDER"   
});

const getFileContent = async () => {
  const fileContent = await portkey.files.content("file_id");

  console.log(fileContent);
}

getFileContent();
```
  </Tab>
<Tab title="REST">
```sh
curl -X GET --header 'x-portkey-api-key: <portkey_api_key>' \
--header 'x-portkey-provider: <vertex_virtual_key>' \
'https://api.portkey.ai/v1/files/<file_id>/content'
```
</Tab>
  <Tab title="OpenAI NodeJS">
```js
import OpenAI from 'openai';
import { PORTKEY_GATEWAY_URL, createHeaders } from 'portkey-ai';

const openai = new OpenAI({
  apiKey: 'OPENAI_API_KEY',
  baseURL: PORTKEY_GATEWAY_URL,
  defaultHeaders: createHeaders({
    provider:"@VERTEX_PROVIDER",
    apiKey: "PORTKEY_API_KEY"
  })
});

const getFileContent = async () => {
  const fileContent = await openai.files.content("file_id");

  console.log(fileContent);
}

getFileContent();
```
  </Tab>
    <Tab title="OpenAI Python">
```python
from openai import OpenAI
from portkey_ai import PORTKEY_GATEWAY_URL, createHeaders

openai = OpenAI(
    api_key='OPENAI_API_KEY',
    base_url=PORTKEY_GATEWAY_URL,
    default_headers=createHeaders(
        provider="@VERTEX_PROVIDER",
        api_key="PORTKEY_API_KEY"
    )
)

file_content = openai.files.content(file_id="file_id")

print(file_content)
```
  </Tab>
 
</Tabs>


Note: The `ListFiles` endpoint is not supported for Vertex AI. 


### Uploading Files for Inference

Vertex AI supports inference with files that are uploaded to GCS, to upload files for inference i.e not for `batch` and `fine-tune`, please check below code snippet.

<Tabs>
 <Tab title="Python">
```python
from portkey_ai import Portkey

# Initialize the Portkey client
portkey = Portkey(
    api_key="PORTKEY_API_KEY",  # Replace with your Portkey API key
    provider="@VERTEX_PROVIDER",   
    vertex_storage_bucket_name="your_bucket_name", # Specify the GCS bucket name
    provider_file_name="your_file_name.png", # Specify the file name in GCS
    provider_model="gemini-1.5-flash-001" # Specify the model to use
)

upload_file_response = portkey.files.create(
  purpose="upload",
  file=open("your_file_name.png", "rb"),
  extra_headers={"x-portkey-file-purpose": "upload"}
)

print(upload_file_response)
```
  </Tab>
    <Tab title="NodeJS">
```js
import { Portkey } from 'portkey-ai';
import * as fs from 'fs';

// Initialize the Portkey client
const portkey = Portkey({
    apiKey: "PORTKEY_API_KEY",  // Replace with your Portkey API key
    provider:"@VERTEX_PROVIDER",   
    vertexStorageBucketName: "your_bucket_name", // Specify the GCS bucket name
    providerFileName: "your_file_name.png", // Specify the file name in GCS
    providerModel: "gemini-1.5-flash-001" // Specify the model to use
});

const uploadFile = async () => {
  const file = await portkey.files.create({
    purpose: "upload",
    file: fs.createReadStream("your_file_name.png"),
    extra_headers: {"x-portkey-file-purpose": "upload"}
  });

  console.log(file);
}

uploadFile();
```
  </Tab>
<Tab title="REST">
```sh
curl -X POST --header 'x-portkey-api-key: <portkey_api_key>' \
 --header 'x-portkey-provider: <vertex_virtual_key>' \
 --header 'x-portkey-vertex-storage-bucket-name: <bucket_name>' \
 --header 'x-portkey-provider-file-name: <file_name>.png' \
 --header 'x-portkey-provider-model: <model_name>' \
 --header 'x-portkey-file-purpose: "upload"' \
 --form 'purpose="upload"' \
 --form 'file=@your_file_name.png' \
 'https://api.portkey.ai/v1/files'
```
</Tab>
  <Tab title="OpenAI NodeJS">
```js
import OpenAI from 'openai';
import { PORTKEY_GATEWAY_URL, createHeaders } from 'portkey-ai';
import * as fs from 'fs';

const openai = new OpenAI({
  apiKey: 'OPENAI_API_KEY',
  baseURL: PORTKEY_GATEWAY_URL,
  defaultHeaders: createHeaders({
    provider:"@VERTEX_PROVIDER",
    apiKey: "PORTKEY_API_KEY",
    vertexStorageBucketName: "your_bucket_name",
    providerFileName: "your_file_name.png",
    providerModel: "gemini-1.5-flash-001"
  })
});

const uploadFile = async () => {
  const file = await openai.files.create({
    purpose: "upload",
    file: fs.createReadStream("your_file_name.png"),
    extraHeaders: {"x-portkey-file-purpose": "upload"}
  });

  console.log(file);
}

uploadFile();
```
  </Tab>
    <Tab title="OpenAI Python">
```python
from openai import OpenAI
from portkey_ai import PORTKEY_GATEWAY_URL, createHeaders

openai = OpenAI(
    api_key='OPENAI_API_KEY',
    base_url=PORTKEY_GATEWAY_URL,
    default_headers=createHeaders(
        provider="@VERTEX_PROVIDER",
        api_key="PORTKEY_API_KEY",
        vertex_storage_bucket_name="your_bucket_name",
        provider_file_name="your_file_name.png",
        provider_model="gemini-1.5-flash-001"
    )
)

upload_file_response = openai.files.create(
  purpose="upload",
  file=open("your_file_name.png", "rb"),
  extra_headers={"x-portkey-file-purpose": "upload"}
)

print(upload_file_response)
```
  </Tab>
 
</Tabs>