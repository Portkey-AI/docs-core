---
title: "Bring Your Own LLM"
---

Portkey supports integration with privately hosted LLMs, allowing you to seamlessly incorporate your private deployments into your AI infrastructure. This feature enables you to manage both private and commercial LLMs through a unified interface while leveraging Portkey's full suite of management and reliability features.

## Key Benefits
- Unified API access across private and commercial LLMs
- Full compatibility with Portkey's reliability features
- Advanced load balancing capabilities
- Comprehensive monitoring and analytics
- Team-specific access controls
- Secure credential management

## Integrate

<Note>
**Prerequisites**

Your private LLM should be following the API specification of any of the existing Portkey-supported providers (e.g., OpenAI's `/chat/completions`, Anthropic's `/messages`, etc.).
</Note>

#### Add Deployment Details
<Frame>
<img src="/images/private-llm.png" />
</Frame>
Navigate to the Virtual Keys section in your Portkey dashboard and create a new Virtual Key. Here, enable the "Local/Privately hosted provider" toggle.

Now, configure your deployment:
  - Select the matching provider API specification (typically, this may be `OpenAI`)
  - Enter your model's base URL in the `Custom Host` field
  - Add required authentication headers and their values

That's it! Portkey will generate a virtual key for your private LLM deployment that you can use anywhere.

<Info>
If you do not want to add your private LLM details to Portkey vault, you can also pass them while instantiating the Portkey client.

**[More on that here](#making-requests-without-virtual-keys).**
</Info>



#### Sample Request

Now, you can make requests to Portkey using the newly generated virtual key:

<CodeGroup>

```js NodeJS
import Portkey from 'portkey-ai'

const portkey = new Portkey({
    apiKey: "PORTKEY_API_KEY",
    virtualKey: "VIRTUAL_KEY"
})

async function main() {
  const response = await portkey.chat.completions.create({
    messages: [{ role: "user", content: "You are a helpful assistant." }],
    model: "MODEL_NAME",
  });

  console.log(response.choices[0]);
}

main();
```

```py Python
from portkey_ai import Portkey

portkey = Portkey(
    api_key="PORTKEY_API_KEY",
    virtual_key="VIRTUAL_KEY"
)

response = portkey.chat.completions.create(
  model="MODEL_NAME",
  messages=[
    {"role": "user", "content": "Hello!"}
  ]
)

print(response.choices[0].message)
```

```sh cURL
curl https://api.portkey.ai/v1/chat/completions \
  -H "Content-Type: application/json" \
  -H "x-portkey-api-key: $PORTKEY_API_KEY" \
  -H "x-portkey-virtual-key: $PORTKEY_PROVIDER_VIRTUAL_KEY" \
  -d '{
    "model": "MODEL_NAME",
    "messages": [
      { "role": "user", "content": "Hello!" }
    ]
  }'
```
</CodeGroup>


## Advanced Features

### Load Balancing
Deploy your private LLM across multiple servers and let Portkey handle load distribution:
- Automatic request distribution
- Health checking
- Failover management

### Reliability Features
Private LLMs are fully compatible with Portkey's reliability suite:
- Automatic retries
- Request timeouts
- Fallback configurations
- Conditional routing
- Rate limiting
- Budget management

### Access Control
Manage access to your private LLMs:
- Team-specific provisioning
- Role-based access control
- Usage monitoring per team/user

## Monitoring & Analytics

### Available Metrics
- Request volume
- Response latency
- Success rates
- User feedback
- Cost tracking (based on target provider specifications)
- Custom pricing support (coming soon)

### Usage Tracking
Monitor your private LLM usage alongside commercial providers:
- Unified dashboard
- Comparative analytics
- Cost optimization insights

## Security

### Credential Management
- Authentication credentials stored securely in Portkey vault
- Access managed exclusively through virtual keys
- No direct exposure of private LLM credentials

### Network Security
- No special network configuration required
- Secure request routing
- End-to-end request encryption


## Best Practices

1. **Testing & Validation**
   - Test your private LLM integration in a staging environment
   - Verify API compatibility before full deployment
   - Monitor initial requests for expected behavior

2. **Load Balancing**
   - Start with conservative load distribution
   - Monitor server performance
   - Adjust balancing rules based on usage patterns

3. **Access Control**
   - Implement principle of least privilege
   - Regularly review team access permissions
   - Monitor usage patterns for security anomalies

## Making Requests Without Virtual Keys

You can also pass your private LLM details directly without using virutal keys.

Instead of using a `virtualKey` referring to the deployment, you can specify a `provider` \+ `custom_host` pair while instantiating the Portkey client.

<Note>
`custom_host` here refers to the URL where your custom model is hosted, including the API version identifier.

More on `custom_host` [here](/product/ai-gateway/universal-api#integrating-local-or-private-models).
</Note>

<CodeGroup>

```js NodeJS
import Portkey from 'portkey-ai'

const portkey = new Portkey({
    apiKey: "PORTKEY_API_KEY",
    provider: "PROVIDER_NAME", // This can be openai, mistral-ai, openai, or anything else
    customHost: "http://MODEL_URL/v1/", // Your custom URL with version identifier
    Authorization: "AUTH_KEY",
    xApiKey: "API_KEY",
    forwardHeaders: ["Authorization","xApiKey"] // Directly forward these headers
})

async function main() {
  const response = await portkey.chat.completions.create({
    messages: [{ role: "user", content: "You are a helpful assistant." }]
  });

  console.log(response.choices[0]);
}

main();
```

```py Python
from portkey_ai import Portkey

portkey = Portkey(
    api_key="PORTKEY_API_KEY",
    provider="PROVIDER_NAME", # This can be mistral-ai, openai, or anything else
    custom_host="http://MODEL_URL/v1/", # Your custom URL with version identifier
    Authorization="AUTH_KEY",
    x_api_key="API_KEY",
    forward_headers=["Authorization","xApiKey"] # Directly forward these headers
)

response = portkey.chat.completions.create(
  messages=[ {"role": "user", "content": "Hello!"} ]
)
```

```sh cURL
curl https://api.portkey.ai/v1/chat/completions \
  -H "Content-Type: application/json" \
  -H "x-portkey-api-key: $PORTKEY_API_KEY" \
  -H "x-portkey-custom-host: $YOUR_DEPLOYMENT_URL" \
  -H "x-portkey-provider: openai" \
  -H "x-api-key: $YOUR_AUTH_DETAILS" \
  -H "x-portkey-forward-headers: ['x-api-key']" \
  -d '{
    "model": "MODEL_NAME",
    "messages": [
      { "role": "user", "content": "Hello!" }
    ]
  }'

```

## Forward Sensitive Headers Securely

When integrating custom LLMs with Portkey, you may have sensitive information in your request headers that you don't want Portkey to track or log. Portkey provides a secure way to forward specific headers directly to your model's API without any processing.

Just specify an array of header names using the `forward_headers` property when initializing the Portkey client. Portkey will then forward these headers directly to your custom host URL without logging or tracking them.

Here's an example:


<Tabs>
    <Tab title="NodeJS">
        ```js
        import Portkey from 'portkey-ai'

        const portkey = new Portkey({
            apiKey: "PORTKEY_API_KEY",
            provider: "PROVIDER_NAME", // This can be mistral-ai, openai, or anything else
            customHost: "http://MODEL_URL/v1/", // Your custom URL with version identifier
            Authorization: "AUTH_KEY", // If you need to pass auth
            forwardHeaders: [ "Authorization" ]
        })
        ```

<Note>
With the JS SDK, you need to transform your headers to **Camel Case** and then include them while initializing the Portkey client.
Example: If you have a header of the format `X-My-Custom-Header`, it should be sent as `xMyCustomHeader` in the SDK
</Note>
    </Tab>
    <Tab title="Python">

```python
from portkey_ai import Portkey

portkey = Portkey(
    api_key="PORTKEY_API_KEY",
    provider="PROVIDER_NAME", # This can be mistral-ai, openai, or anything else
    custom_host="http://MODEL_URL/v1/", # Your custom URL with version identifier
    Authorization="AUTH_KEY", # If you need to pass auth
    forward_headers= [ "Authorization" ]
)
```

<Note>
With the Python SDK, you need to transform your headers to **Snake Case** and then include them while initializing the Portkey client.
Example: If you have a header of the format `X-My-Custom-Header`, it should be sent as `X_My_Custom_Header` in the SDK

</Note>
    </Tab>
  <Tab title="cURL">
    `x-portkey-forward-headers` accepts comma separated header names

    ```sh
    curl https://api.portkey.ai/v1/chat/completions \
      -H "Content-Type: application/json" \
      -H "x-portkey-api-key: $PORTKEY_API_KEY" \
      -H "x-portkey-provider: $PROVIDER_NAME" \
      -H "x-portkey-custom-host: https://MODEL_URL/v1" \
      -H "x-api-key: $API_KEY" \
      -H "x-secret-access-key: $ACCESS_KEY" \
      -H "x-key-id: $KEY_ID" \
      -H "x-portkey-forward-headers: x-api-key, x-secret-access-key, x-key-id" \
      -d '{
        "model": "llama2",
        "messages": [{ "role": "user", "content": "Say this is a test" }]
      }'
    ```
    </Tab>
  </Tabs>


#### Forward Headers in Portkey Config

You can also define `forward_headers` in your Config object and then pass the headers directly while making a request.

```json
{
    "strategy": {
        "mode": "loadbalance"
    },
    "targets": [
        {
            "provider": "openai",
            "api_key": ""
        },
        {
            "strategy": {
                "mode": "fallback"
            },
            "targets": [
                {
                    "provider": "azure-openai",
                    "custom_host": "http://MODEL_URL/v1",
                    "forward_headers": ["my-auth-header-1", "my-auth-header-2"]
                },
                {
                    "provider": "openai",
                    "api_key": "sk-***"
                }
            ]
        }
    ]
}
```


</CodeGroup>

## Troubleshooting

Common issues and solutions:

1. **Connection Issues**
   - Verify base URL configuration
   - Check authentication header format
   - Confirm network connectivity

2. **Performance Problems**
   - Review load balancing settings
   - Check server resources
   - Monitor request timeouts

3. **Authentication Errors**
   - Verify credential configuration
   - Check virtual key status
   - Confirm team access permissions

## FAQs

**Q: Can I use any private LLM with Portkey?**
A: Yes, as long as it implements a supported provider's API specification.

**Q: How do I handle multiple deployment endpoints?**
A: Configure multiple URLs in the load balancing settings for automatic distribution.

**Q: Are there any request volume limitations?**
A: No specific limitations beyond your private LLM's capabilities.

**Q: Can I use different models with the same private deployment?**
A: Yes, following the target provider's model specification format.

**Q: How are costs calculated for private LLMs?**
A: Currently based on target provider specifications, with custom pricing coming soon.

---

You'll find more information in the relevant sections:

1. [Add metadata to your requests](/product/observability/metadata)
2. [Add gateway configs to your requests](/product/ai-gateway/universal-api#ollama-in-configs)
3. [Tracing requests](/product/observability/traces)
4. [Setup a fallback from OpenAI to your local LLM](/product/ai-gateway/fallbacks)
