---
title: "Google Gemini"
---

Portkey provides a robust and secure gateway to facilitate the integration of various Large Language Models (LLMs) into your applications, including [Google Gemini APIs](https://cloud.google.com/vertex-ai/docs/generative-ai/model-reference/gemini).

With Portkey, you can take advantage of features like fast AI gateway access, observability, prompt management, and more, all while ensuring the secure management of your LLM API keys through a [virtual key](/product/ai-gateway/virtual-keys) system.
<Note>Provider Slug. `google`</Note>


## Portkey SDK Integration with Google Gemini Models

Portkey provides a consistent API to interact with models from various providers. To integrate Google Gemini with Portkey:

### 1\. Install the Portkey SDK

Add the Portkey SDK to your application to interact with Google Gemini's API through Portkey's gateway.
<Tabs>
    <Tab title="NodeJS">

```sh
npm install --save portkey-ai
```
    </Tab>
    <Tab title="Python">
```sh
pip install portkey-ai
```
    </Tab>
  </Tabs>


### 2\. Initialize Portkey with the Virtual Key

To use Gemini with Portkey, [get your API key from here](https://aistudio.google.com/app/apikey), then add it to Portkey to create the virtual key.
<Tabs>
    <Tab title="NodeJS SDK">
```js
import Portkey from 'portkey-ai'

const portkey = new Portkey({
    apiKey: "PORTKEY_API_KEY", // defaults to process.env["PORTKEY_API_KEY"]
    provider:"@PROVIDER" // Your Google Virtual Key
})
```
    </Tab>
    <Tab title="Python SDK">
        ```python
        from portkey_ai import Portkey

        portkey = Portkey(
            api_key="PORTKEY_API_KEY",  # Replace with your Portkey API key
            provider="@PROVIDER"   # Replace with your virtual key for Google
        )
        ```
    </Tab>

  </Tabs>



### **3\. Invoke Chat Completions with** Google Gemini

Use the Portkey instance to send requests to Google Gemini. You can also override the virtual key directly in the API call if needed.
<Tabs>
    <Tab title="NodeJS SDK">
        ```js
        const chatCompletion = await portkey.chat.completions.create({
            messages: [
                { role: 'system', content: 'You are not a helpful assistant' },
                { role: 'user', content: 'Say this is a test' }
            ],
            model: 'gemini-1.5-pro',
        });

        console.log(chatCompletion.choices);
        ```
    </Tab>
    <Tab title="Python SDK">

```python
completion = portkey.chat.completions.create(
    messages= [
        { "role": 'system', "content": 'You are not a helpful assistant' },
        { "role": 'user', "content": 'Say this is a test' }
    ],
    model= 'gemini-1.5-pro'
)

print(completion)
```
    </Tab>

</Tabs>


<Note>
Portkey supports the `system_instructions` parameter for Google Gemini 1.5 - allowing you to control the behavior and output of your Gemini-powered applications with ease.

Simply include your Gemini system prompt as part of the `{"role":"system"}` message within the `messages` array of your request body. Portkey Gateway will automatically transform your message to ensure seamless compatibility with the Google Gemini API.
</Note>

## Function Calling

Portkey supports function calling mode on Google's Gemini Models. Explore this <Icon icon="square-down" /> Cookbook for a deep dive and examples:

[Function Calling](/guides/getting-started/function-calling)




## Advanced Multimodal Capabilities with Gemini

Gemini models are inherently multimodal, capable of processing and understanding content from a wide array of file types. Portkey streamlines the integration of these powerful features by providing a unified, OpenAI-compatible API.

<Note>
  **The Portkey Advantage: A Unified Format for All Media**

  To simplify development, Portkey uses a consistent format for all multimodal requests. Whether you're sending an image, audio, video, or document, you will use an object with `type: 'image_url'` within the user message's `content` array.

  Portkey's AI Gateway intelligently interprets your request—based on the URL or data URI you provide—and translates it into the precise format required by the Google Gemini API. This means you only need to learn one structure for all your media processing needs.
</Note>

### Image Processing

Gemini can analyze images to describe their content, answer visual questions, or identify objects.

<Card href="https://ai.google.dev/gemini-api/docs/image-understanding" title="Gemini Image Understanding Docs">
</Card>

**Method 1: Sending an Image via Google Files URL**

Use the Google Files API to upload your image and get a URL. This is the recommended approach for larger files or when you need persistent storage.

<Info>
To upload files and get Google Files URLs, use the [Files API](https://ai.google.dev/gemini-api/docs/files). The URL format will be similar to: `https://generativelanguage.googleapis.com/v1beta/files/[FILE_ID]`
</Info>

<CodeGroup>
```javascript NodeJS
const chatCompletion = await portkey.chat.completions.create({
    model: 'gemini-1.5-pro',
    messages: [{
        role: 'user',
        content: [
            {
                type: 'image_url',
                image_url: {
                    url: 'https://generativelanguage.googleapis.com/v1beta/files/your-file-id'
                }
            },
            {
                type: 'text',
                text: 'Describe this image in detail.'
            }
        ]
    }],
});
console.log(chatCompletion.choices[0].message.content);
```
```python Python
completion = portkey.chat.completions.create(
    model='gemini-1.5-pro',
    messages=[{
        "role": "user",
        "content": [
            {
                "type": "image_url",
                "image_url": {
                    "url": "https://generativelanguage.googleapis.com/v1beta/files/your-file-id"
                }
            },
            {
                "type": "text",
                "text": "Describe this image in detail."
            }
        ]
    }],
)
print(completion.choices[0].message.content)
```
```sh cURL
curl --location 'https://api.portkey.ai/v1/chat/completions' \
--header 'x-portkey-provider: google' \
--header 'x-portkey-api-key: YOUR_PORTKEY_API_KEY' \
--header 'Authorization: YOUR_GEMINI_API_KEY' \
--header 'Content-Type: application/json' \
--data '{
    "model": "gemini-1.5-pro",
    "messages": [{
        "role": "user",
        "content": [
            {
                "type": "text",
                "text": "Describe this image in detail."
            },
            {
                "type": "image_url",
                "image_url": { "url": "https://generativelanguage.googleapis.com/v1beta/files/your-file-id" }
            }
        ]
    }]
}'
```
</CodeGroup>

**Method 2: Sending a Local Image as Base64 Data**

Use this method for local image files. The file is encoded into a Base64 string and sent as a data URI. This is ideal for smaller files when you don't want to use the Files API.

The data URI format is: `data:<MIME_TYPE>;base64,<YOUR_BASE64_DATA>`

<CodeGroup>
```javascript NodeJS
import fs from 'fs';

const imageBytes = fs.readFileSync('local-image.png');
const base64Image = imageBytes.toString('base64');
const imageUri = `data:image/png;base64,${base64Image}`;

const chatCompletion = await portkey.chat.completions.create({
    model: 'gemini-1.5-pro',
    messages: [{
        role: 'user',
        content: [
            { type: 'image_url', image_url: { url: imageUri }},
            { type: 'text', text: 'What is in this picture?' }
        ]
    }],
});
console.log(chatCompletion.choices[0].message.content);
```
```python Python
import base64

with open("local-image.png", "rb") as image_file:
    image_bytes = image_file.read()

base64_image = base64.b64encode(image_bytes).decode('utf-8')
image_uri = f"data:image/png;base64,{base64_image}"

completion = portkey.chat.completions.create(
    model='gemini-1.5-pro',
    messages=[{
        "role": "user",
        "content": [
            { "type": "image_url", "image_url": { "url": image_uri }},
            { "type": "text", "text": "What is in this picture?" }
        ]
    }],
)
print(completion.choices[0].message.content)
```
```sh cURL
# First, encode your image file to base64
# For example: base64 -i local-image.png -o image.b64
# Then use the encoded content in the request

curl --location 'https://api.portkey.ai/v1/chat/completions' \
--header 'x-portkey-provider: google' \
--header 'x-portkey-api-key: YOUR_PORTKEY_API_KEY' \
--header 'Authorization: YOUR_GEMINI_API_KEY' \
--header 'Content-Type: application/json' \
--data '{
    "model": "gemini-1.5-pro",
    "messages": [{
        "role": "user",
        "content": [
            {"type": "text", "text": "What is in this picture?"},
            {"type": "image_url", "image_url": {"url": "data:image/png;base64,YOUR_BASE64_IMAGE_DATA"}}
        ]
    }]
}'
```
</CodeGroup>

<Info>Supported Image MIME types: `image/png`, `image/jpeg`, `image/webp`, `image/heic`, `image/heif`</Info>

---

### Audio Processing

Gemini can transcribe speech, summarize audio content, or answer questions about sounds.

<Card href="https://ai.google.dev/gemini-api/docs/audio" title="Gemini Audio Understanding Docs">
</Card>

**Method 1: Sending Audio via Google Files URL**

Upload your audio file using the Files API to get a Google Files URL.

<CodeGroup>
```javascript NodeJS
const chatCompletion = await portkey.chat.completions.create({
    model: 'gemini-1.5-pro',
    messages: [{
        role: 'user',
        content: [
            {
                type: 'image_url',
                image_url: {
                    url: 'https://generativelanguage.googleapis.com/v1beta/files/your-audio-file-id'
                }
            },
            { type: 'text', text: 'Please transcribe the speech in this audio.' }
        ]
    }],
});
console.log(chatCompletion.choices[0].message.content);
```
```python Python
completion = portkey.chat.completions.create(
    model='gemini-1.5-pro',
    messages=[{
        "role": "user",
        "content": [
            {
                "type": "image_url",
                "image_url": {
                    "url": "https://generativelanguage.googleapis.com/v1beta/files/your-audio-file-id"
                }
            },
            { "type": "text", "text": "Please transcribe the speech in this audio." }
        ]
    }],
)
print(completion.choices[0].message.content)
```
```sh cURL
curl --location 'https://api.portkey.ai/v1/chat/completions' \
--header 'x-portkey-provider: google' \
--header 'x-portkey-api-key: YOUR_PORTKEY_API_KEY' \
--header 'Authorization: YOUR_GEMINI_API_KEY' \
--header 'Content-Type: application/json' \
--data '{
    "model": "gemini-1.5-pro",
    "messages": [{
        "role": "user",
        "content": [
            {"type": "text", "text": "Please transcribe the speech in this audio."},
            {"type": "image_url", "image_url": {"url": "https://generativelanguage.googleapis.com/v1beta/files/your-audio-file-id"}}
        ]
    }]
}'
```
</CodeGroup>

**Method 2: Sending Local Audio as Base64 Data**

This is the standard way to process local audio files directly through the API.

<CodeGroup>
```javascript NodeJS
import fs from 'fs';

const audioBytes = fs.readFileSync('audio-example.mp3');
const base64Audio = audioBytes.toString('base64');
const audioUri = `data:audio/mp3;base64,${base64Audio}`;

const chatCompletion = await portkey.chat.completions.create({
    model: 'gemini-1.5-pro',
    messages: [{
        role: 'user',
        content: [
            { type: 'image_url', image_url: { url: audioUri }},
            { type: 'text', text: 'Describe this audio' }
        ]
    }],
});
console.log(chatCompletion.choices[0].message.content);
```
```python Python
import base64

with open('audio-example.mp3', 'rb') as audio_file:
    audio_bytes = audio_file.read()

base64_audio = base64.b64encode(audio_bytes).decode('utf-8')
audio_uri = f"data:audio/mp3;base64,{base64_audio}"

completion = portkey.chat.completions.create(
    model='gemini-1.5-pro',
    messages=[{
        "role": "user",
        "content": [
            { "type": "image_url", "image_url": { "url": audio_uri }},
            { "type": "text", "text": "Describe this audio" }
        ]
    }],
)
print(completion.choices[0].message.content)
```
```sh cURL
# First, encode your audio file to base64
# For example: base64 -i audio-example.mp3 -o audio.b64
# Then use the encoded content in the request

curl --location 'https://api.portkey.ai/v1/chat/completions' \
--header 'x-portkey-provider: google' \
--header 'x-portkey-api-key: YOUR_PORTKEY_API_KEY' \
--header 'Authorization: YOUR_GEMINI_API_KEY' \
--header 'Content-Type: application/json' \
--data '{
    "model": "gemini-1.5-pro",
    "messages": [{
        "role": "user",
        "content": [
            {"type": "text", "text": "Describe this audio"},
            {"type": "image_url", "image_url": {"url": "data:audio/mp3;base64,YOUR_BASE64_AUDIO_DATA"}}
        ]
    }]
}'
```
</CodeGroup>

<Info>Supported Audio MIME types: `audio/wav`, `audio/mp3`, `audio/aiff`, `audio/aac`, `audio/ogg`, `audio/flac`</Info>

---

### Video Processing

Gemini can summarize videos, answer questions about specific events, and describe scenes.

<Card href="https://ai.google.dev/gemini-api/docs/video-understanding" title="Gemini Video Understanding Docs">
</Card>

**Method 1: Sending a Video via YouTube URL**

YouTube is the only supported public URL source for videos. Simply provide the YouTube video URL.

<CodeGroup>
```javascript NodeJS
const chatCompletion = await portkey.chat.completions.create({
    model: 'gemini-1.5-pro',
    messages: [{
        role: 'user',
        content: [
            {
                type: 'text',
                text: 'Describe this video in 3 sentences.'
            },
            {
                type: 'image_url',
                image_url: {
                    url: 'https://www.youtube.com/watch?v=9hE5-98ZeCg'
                }
            }
        ]
    }],
});
console.log(chatCompletion.choices[0].message.content);
```
```python Python
completion = portkey.chat.completions.create(
    model='gemini-1.5-pro',
    messages=[{
        "role": "user",
        "content": [
            {
                "type": "text",
                "text": "Describe this video in 3 sentences."
            },
            {
                "type": "image_url",
                "image_url": {
                    "url": "https://www.youtube.com/watch?v=9hE5-98ZeCg"
                }
            }
        ]
    }],
)
print(completion.choices[0].message.content)
```
```sh cURL
curl --location 'https://api.portkey.ai/v1/chat/completions' \
--header 'x-portkey-provider: google' \
--header 'x-portkey-api-key: YOUR_PORTKEY_API_KEY' \
--header 'Authorization: YOUR_GEMINI_API_KEY' \
--header 'Content-Type: application/json' \
--data '{
    "model": "gemini-1.5-pro",
    "messages": [{
        "role": "user",
        "content": [
            {"type": "text", "text": "Describe this video in 3 sentences."},
            {"type": "image_url", "image_url": {"url": "https://www.youtube.com/watch?v=9hE5-98ZeCg"}}
        ]
    }]
}'
```
</CodeGroup>

**Method 2: Sending Local Video as Base64 Data**

For smaller video files, you can encode them as base64. Note that this method has size limitations.

<CodeGroup>
```javascript NodeJS
import fs from 'fs';

const videoBytes = fs.readFileSync('video-example.mp4');
const base64Video = videoBytes.toString('base64');
const videoUri = `data:video/mp4;base64,${base64Video}`;

const chatCompletion = await portkey.chat.completions.create({
    model: 'gemini-1.5-pro',
    messages: [{
        role: 'user',
        content: [
            { type: 'image_url', image_url: { url: videoUri }},
            { type: 'text', text: 'Describe this video' }
        ]
    }],
});
console.log(chatCompletion.choices[0].message.content);
```
```python Python
import base64

with open('video-example.mp4', 'rb') as video_file:
    video_bytes = video_file.read()

base64_video = base64.b64encode(video_bytes).decode('utf-8')
video_uri = f"data:video/mp4;base64,{base64_video}"

completion = portkey.chat.completions.create(
    model='gemini-1.5-pro',
    messages=[{
        "role": "user",
        "content": [
            { "type": "image_url", "image_url": { "url": video_uri }},
            { "type": "text", "text": "Describe this video" }
        ]
    }],
)
print(completion.choices[0].message.content)
```
```sh cURL
# First, encode your video file to base64
# For example: base64 -i video-example.mp4 -o video.b64
# Then use the encoded content in the request

curl --location 'https://api.portkey.ai/v1/chat/completions' \
--header 'x-portkey-provider: google' \
--header 'x-portkey-api-key: YOUR_PORTKEY_API_KEY' \
--header 'Authorization: YOUR_GEMINI_API_KEY' \
--header 'Content-Type: application/json' \
--data '{
    "model": "gemini-1.5-pro",
    "messages": [{
        "role": "user",
        "content": [
            {"type": "text", "text": "Describe this video"},
            {"type": "image_url", "image_url": {"url": "data:video/mp4;base64,YOUR_BASE64_VIDEO_DATA"}}
        ]
    }]
}'
```
</CodeGroup>

**Method 3: Sending Video via Google Files URL**

For larger video files, upload them using the Files API to get a Google Files URL.

<CodeGroup>
```javascript NodeJS
const chatCompletion = await portkey.chat.completions.create({
    model: 'gemini-1.5-pro',
    messages: [{
        role: 'user',
        content: [
            {
                type: 'image_url',
                image_url: {
                    url: 'https://generativelanguage.googleapis.com/v1beta/files/your-video-file-id'
                }
            },
            { type: 'text', text: 'Please describe the main events in this video.' }
        ]
    }],
});
console.log(chatCompletion.choices[0].message.content);
```
```python Python
completion = portkey.chat.completions.create(
    model='gemini-1.5-pro',
    messages=[{
        "role": "user",
        "content": [
            {
                "type": "image_url",
                "image_url": {
                    "url": "https://generativelanguage.googleapis.com/v1beta/files/your-video-file-id"
                }
            },
            { "type": "text", "text": "Please describe the main events in this video." }
        ]
    }],
)
print(completion.choices[0].message.content)
```
```sh cURL
curl --location 'https://api.portkey.ai/v1/chat/completions' \
--header 'x-portkey-provider: google' \
--header 'x-portkey-api-key: YOUR_PORTKEY_API_KEY' \
--header 'Authorization: YOUR_GEMINI_API_KEY' \
--header 'Content-Type: application/json' \
--data '{
    "model": "gemini-1.5-pro",
    "messages": [{
        "role": "user",
        "content": [
            {"type": "text", "text": "Please describe the main events in this video."},
            {"type": "image_url", "image_url": {"url": "https://generativelanguage.googleapis.com/v1beta/files/your-video-file-id"}}
        ]
    }]
}'
```
</CodeGroup>

<Info>Supported Video MIME types: `video/mp4`, `video/mpeg`, `video/mov`, `video/avi`, `video/webm`, `video/wmv`</Info>

---

### Document Processing (PDF)

Gemini's vision capabilities excel at understanding the content of PDF documents, including text, tables, and images.

<Card href="https://ai.google.dev/gemini-api/docs/document-processing" title="Gemini Documents Understanding Docs">
</Card>

**Method 1: Sending a Document via Google Files URL**

Upload your PDF using the Files API to get a Google Files URL.

<CodeGroup>
```javascript NodeJS
const chatCompletion = await portkey.chat.completions.create({
    model: 'gemini-1.5-pro',
    messages: [{
        role: 'user',
        content: [
            {
                type: 'image_url',
                image_url: {
                    url: 'https://generativelanguage.googleapis.com/v1beta/files/your-pdf-file-id'
                }
            },
            { type: 'text', text: 'Summarize the key findings of this research paper.' }
        ]
    }],
});
console.log(chatCompletion.choices[0].message.content);
```
```python Python
completion = portkey.chat.completions.create(
    model='gemini-1.5-pro',
    messages=[{
        "role": "user",
        "content": [
            {
                "type": "image_url",
                "image_url": {
                    "url": "https://generativelanguage.googleapis.com/v1beta/files/your-pdf-file-id"
                }
            },
            { "type": "text", "text": "Summarize the key findings of this research paper." }
        ]
    }],
)
print(completion.choices[0].message.content)
```
```sh cURL
curl --location 'https://api.portkey.ai/v1/chat/completions' \
--header 'x-portkey-provider: google' \
--header 'x-portkey-api-key: YOUR_PORTKEY_API_KEY' \
--header 'Authorization: YOUR_GEMINI_API_KEY' \
--header 'Content-Type: application/json' \
--data '{
    "model": "gemini-1.5-pro",
    "messages": [{
        "role": "user",
        "content": [
            {"type": "text", "text": "Summarize the key findings of this research paper."},
            {"type": "image_url", "image_url": {"url": "https://generativelanguage.googleapis.com/v1beta/files/your-pdf-file-id"}}
        ]
    }]
}'
```
</CodeGroup>

**Method 2: Sending a Local Document as Base64 Data**

This is suitable for smaller, local PDF files.

<CodeGroup>
```javascript NodeJS
import fs from 'fs';

const pdfBytes = fs.readFileSync('whitepaper.pdf');
const base64Pdf = pdfBytes.toString('base64');
const pdfUri = `data:application/pdf;base64,${base64Pdf}`;

const chatCompletion = await portkey.chat.completions.create({
    model: 'gemini-1.5-pro',
    messages: [{
        role: 'user',
        content: [
            { type: 'image_url', image_url: { url: pdfUri }},
            { type: 'text', text: 'What is the main conclusion of this document?' }
        ]
    }],
});
console.log(chatCompletion.choices[0].message.content);
```
```python Python
import base64

with open("whitepaper.pdf", "rb") as pdf_file:
    pdf_bytes = pdf_file.read()

base64_pdf = base64.b64encode(pdf_bytes).decode('utf-8')
pdf_uri = f"data:application/pdf;base64,{base64_pdf}"

completion = portkey.chat.completions.create(
    model='gemini-1.5-pro',
    messages=[{
        "role": "user",
        "content": [
            { "type": "image_url", "image_url": { "url": pdf_uri }},
            { "type": "text", "text": "What is the main conclusion of this document?" }
        ]
    }],
)
print(completion.choices[0].message.content)
```
```sh cURL
# First, encode your PDF file to base64
# For example: base64 -i whitepaper.pdf -o pdf.b64
# Then use the encoded content in the request

curl --location 'https://api.portkey.ai/v1/chat/completions' \
--header 'x-portkey-provider: google' \
--header 'x-portkey-api-key: YOUR_PORTKEY_API_KEY' \
--header 'Authorization: YOUR_GEMINI_API_KEY' \
--header 'Content-Type: application/json' \
--data '{
    "model": "gemini-1.5-pro",
    "messages": [{
        "role": "user",
        "content": [
            {"type": "text", "text": "What is the main conclusion of this document?"},
            {"type": "image_url", "image_url": {"url": "data:application/pdf;base64,YOUR_BASE64_PDF_DATA"}}
        ]
    }]
}'
```
</CodeGroup>

<Note>While you can send other document types like `.txt` or `.html`, they will be treated as plain text. Gemini's native document vision capabilities are optimized for the `application/pdf` MIME type.</Note>

---

## Code Execution Tool

Gemini can use a built-in code interpreter tool to solve complex computational problems, perform calculations, and generate code. To enable this, simply include the `code_execution` tool in your request. The model will automatically decide when to invoke it.

<CodeGroup>
```javascript NodeJS
const response = await portkey.chat.completions.create({
    model: "gemini-1.5-pro",
    messages: [{
        "role": "user",
        "content": "Calculate the 20th Fibonacci number. Then find the nearest palindrome to it."
    }],
    tools: [{ "type": "code_execution" }]
});

console.log(response.choices[0].message.content);
```
```python Python
response = portkey.chat.completions.create(
    model="gemini-1.5-pro",
    messages=[{
        "role": "user",
        "content": "Calculate the 20th Fibonacci number. Then find the nearest palindrome to it."
    }],
    tools=[{ "type": "code_execution" }]
)

print(response.choices[0].message.content)
```
```sh cURL
curl --location 'https://api.portkey.ai/v1/chat/completions' \
--header 'x-portkey-provider: google' \
--header 'x-portkey-api-key: YOUR_PORTKEY_API_KEY' \
--header 'Authorization: YOUR_GEMINI_API_KEY' \
--header 'Content-Type: application/json' \
--data '{
    "model": "gemini-1.5-pro",
    "messages": [
        {
            "role": "user",
            "content": "Calculate the 20th Fibonacci number. Then find the nearest palindrome to it."
        }
    ],
    "tools": [{ "type": "code_execution" }]
}'
```
</CodeGroup>

<Note>
**Important:** For all file uploads (except YouTube videos), it's recommended to use the [Google Files API](https://ai.google.dev/gemini-api/docs/files) to upload your files first, then use the returned file URL in your requests. This approach provides better performance and reliability for larger files.
</Note>



## Grounding with Google Search

Vertex AI supports grounding with Google Search. This is a feature that allows you to ground your LLM responses with real-time search results.
Grounding is invoked by passing the `google_search` tool (for newer models like gemini-2.0-flash-001), and `google_search_retrieval` (for older models like gemini-1.5-flash) in the `tools` array.

```json
"tools": [
    {
        "type": "function",
        "function": {
            "name": "google_search" // or google_search_retrieval for older models
        }
    }]
```

<Warning>
If you mix regular tools with grounding tools, vertex might throw an error saying only one tool can be used at a time.
</Warning>

---

## Computer Use

Google Gemini supports computer use capabilities through the `gemini-2.5-computer-use-preview-10-2025` model. This allows the model to interact with web browsers and perform automated tasks.

<Note>
Computer use requires setting `x-portkey-strict-open-ai-compliance: false` in your request headers.
</Note>

### Single Turn Conversation

<CodeGroup>
```python Python
from portkey_ai import Portkey

portkey = Portkey(
    api_key="PORTKEY_API_KEY",
    provider="@PROVIDER",  # Your Google Virtual Key
    strict_open_ai_compliance=False
)

response = portkey.chat.completions.create(
    model="gemini-2.5-computer-use-preview-10-2025",
    stream=False,
    messages=[
        {
            "role": "system",
            "content": "Say Hi"
        },
        {
            "role": "user",
            "content": "Go to google.com and search for 'weather in New York'"
        }
    ],
    tools=[
        {
            "type": "function",
            "function": {
                "name": "computer_use",
                "parameters": {
                    "environment": "ENVIRONMENT_BROWSER"
                }
            }
        }
    ]
)

print(response)
```

```javascript NodeJS
import Portkey from 'portkey-ai';

const portkey = new Portkey({
    apiKey: "PORTKEY_API_KEY",
    provider: "@PROVIDER",  // Your Google Virtual Key
    strictOpenAiCompliance: false
});

const response = await portkey.chat.completions.create({
    model: "gemini-2.5-computer-use-preview-10-2025",
    stream: false,
    messages: [
        {
            role: "system",
            content: "Say Hi"
        },
        {
            role: "user",
            content: "Go to google.com and search for 'weather in New York'"
        }
    ],
    tools: [
        {
            type: "function",
            function: {
                name: "computer_use",
                parameters: {
                    environment: "ENVIRONMENT_BROWSER"
                }
            }
        }
    ]
});

console.log(response);
```

```sh cURL
curl --location 'https://api.portkey.ai/v1/chat/completions' \
--header 'x-portkey-provider: google' \
--header 'Content-Type: application/json' \
--header 'x-portkey-api-key: your-api-key' \
--header 'Authorization: YOUR_GEMINI_API_KEY' \
--header 'x-portkey-strict-open-ai-compliance: false' \
--data '{
    "model": "gemini-2.5-computer-use-preview-10-2025",
    "stream": false,
    "messages": [
        {
            "role": "system",
            "content": "Say Hi"
        },
        {
            "role": "user",
            "content": "Go to google.com and search for '\''weather in New York'\''"
        }
    ],
    "tools": [
        {
            "type": "function",
            "function": {
                "name": "computer_use",
                "parameters": {
                    "environment": "ENVIRONMENT_BROWSER"
                }
            }
        }
    ]
}'
```

```javascript OpenAI NodeJS
import OpenAI from 'openai';
import { PORTKEY_GATEWAY_URL, createHeaders } from 'portkey-ai';

const openai = new OpenAI({
    apiKey: 'YOUR_GEMINI_API_KEY',
    baseURL: PORTKEY_GATEWAY_URL,
    defaultHeaders: createHeaders({
        provider: 'google',
        apiKey: 'PORTKEY_API_KEY',
        strictOpenAiCompliance: false
    })
});

const response = await openai.chat.completions.create({
    model: 'gemini-2.5-computer-use-preview-10-2025',
    stream: false,
    messages: [
        {
            role: 'system',
            content: 'Say Hi'
        },
        {
            role: 'user',
            content: "Go to google.com and search for 'weather in New York'"
        }
    ],
    tools: [
        {
            type: 'function',
            function: {
                name: 'computer_use',
                parameters: {
                    environment: 'ENVIRONMENT_BROWSER'
                }
            }
        }
    ]
});

console.log(response);
```

```python OpenAI Python
from openai import OpenAI
from portkey_ai import PORTKEY_GATEWAY_URL, createHeaders

openai = OpenAI(
    api_key='YOUR_GEMINI_API_KEY',
    base_url=PORTKEY_GATEWAY_URL,
    default_headers=createHeaders(
        provider='google',
        api_key='PORTKEY_API_KEY',
        strict_open_ai_compliance=False
    )
)

response = openai.chat.completions.create(
    model='gemini-2.5-computer-use-preview-10-2025',
    stream=False,
    messages=[
        {
            'role': 'system',
            'content': 'Say Hi'
        },
        {
            'role': 'user',
            'content': "Go to google.com and search for 'weather in New York'"
        }
    ],
    tools=[
        {
            'type': 'function',
            'function': {
                'name': 'computer_use',
                'parameters': {
                    'environment': 'ENVIRONMENT_BROWSER'
                }
            }
        }
    ]
)

print(response)
```
</CodeGroup>

### Multi Turn Conversation

<CodeGroup>
```python Python
from portkey_ai import Portkey

portkey = Portkey(
    api_key="PORTKEY_API_KEY",
    provider="@PROVIDER",  # Your Google Virtual Key
    strict_open_ai_compliance=False
)

response = portkey.chat.completions.create(
    model="gemini-2.5-computer-use-preview-10-2025",
    stream=False,
    messages=[
        {
            "role": "system",
            "content": "Say Hi"
        },
        {
            "role": "user",
            "content": "Go to google.com and search for 'weather in New York'"
        },
        {
            "role": "assistant",
            "tool_calls": [
                {
                    "id": "portkey-50925c03-b8cc-4057-948b-13a9d9de19e0",
                    "type": "function",
                    "function": {
                        "name": "open_web_browser",
                        "arguments": "{}"
                    }
                }
            ]
        },
        {
            "role": "user",
            "content": "I've opened the browser"
        }
    ],
    tools=[
        {
            "type": "function",
            "function": {
                "name": "computerUse",
                "parameters": {
                    "environment": "ENVIRONMENT_BROWSER"
                }
            }
        }
    ]
)

print(response)
```

```javascript NodeJS
import Portkey from 'portkey-ai';

const portkey = new Portkey({
    apiKey: "PORTKEY_API_KEY",
    provider: "@PROVIDER",  // Your Google Virtual Key
    strictOpenAiCompliance: false
});

const response = await portkey.chat.completions.create({
    model: "gemini-2.5-computer-use-preview-10-2025",
    stream: false,
    messages: [
        {
            role: "system",
            content: "Say Hi"
        },
        {
            role: "user",
            content: "Go to google.com and search for 'weather in New York'"
        },
        {
            role: "assistant",
            tool_calls: [
                {
                    id: "portkey-50925c03-b8cc-4057-948b-13a9d9de19e0",
                    type: "function",
                    function: {
                        name: "open_web_browser",
                        arguments: "{}"
                    }
                }
            ]
        },
        {
            role: "user",
            content: "I've opened the browser"
        }
    ],
    tools: [
        {
            type: "function",
            function: {
                name: "computerUse",
                parameters: {
                    environment: "ENVIRONMENT_BROWSER"
                }
            }
        }
    ]
});

console.log(response);
```

```sh cURL
curl --location 'https://api.portkey.ai/v1/chat/completions' \
--header 'x-portkey-provider: google' \
--header 'Content-Type: application/json' \
--header 'x-portkey-api-key: your-api-key' \
--header 'Authorization: YOUR_GEMINI_API_KEY' \
--header 'x-portkey-strict-open-ai-compliance: false' \
--data '{
    "model": "gemini-2.5-computer-use-preview-10-2025",
    "stream": false,
    "messages": [
        {
            "role": "system",
            "content": "Say Hi"
        },
        {
            "role": "user",
            "content": "Go to google.com and search for '\''weather in New York'\''"
        },
        {
            "role": "assistant",
            "tool_calls": [
                {
                    "id": "portkey-50925c03-b8cc-4057-948b-13a9d9de19e0",
                    "type": "function",
                    "function": {
                        "name": "open_web_browser",
                        "arguments": "{}"
                    }
                }
            ]
        },
        {
            "role": "user",
            "content": "I'\''ve opened the browser"
        }
    ],
    "tools": [
        {
            "type": "function",
            "function": {
                "name": "computerUse",
                "parameters": {
                    "environment": "ENVIRONMENT_BROWSER"
                }
            }
        }
    ]
}'
```

```javascript OpenAI NodeJS
import OpenAI from 'openai';
import { PORTKEY_GATEWAY_URL, createHeaders } from 'portkey-ai';

const openai = new OpenAI({
    apiKey: 'YOUR_GEMINI_API_KEY',
    baseURL: PORTKEY_GATEWAY_URL,
    defaultHeaders: createHeaders({
        provider: 'google',
        apiKey: 'PORTKEY_API_KEY',
        strictOpenAiCompliance: false
    })
});

const response = await openai.chat.completions.create({
    model: 'gemini-2.5-computer-use-preview-10-2025',
    stream: false,
    messages: [
        {
            role: 'system',
            content: 'Say Hi'
        },
        {
            role: 'user',
            content: "Go to google.com and search for 'weather in New York'"
        },
        {
            role: 'assistant',
            tool_calls: [
                {
                    id: 'portkey-50925c03-b8cc-4057-948b-13a9d9de19e0',
                    type: 'function',
                    function: {
                        name: 'open_web_browser',
                        arguments: '{}'
                    }
                }
            ]
        },
        {
            role: 'user',
            content: "I've opened the browser"
        }
    ],
    tools: [
        {
            type: 'function',
            function: {
                name: 'computerUse',
                parameters: {
                    environment: 'ENVIRONMENT_BROWSER'
                }
            }
        }
    ]
});

console.log(response);
```

```python OpenAI Python
from openai import OpenAI
from portkey_ai import PORTKEY_GATEWAY_URL, createHeaders

openai = OpenAI(
    api_key='YOUR_GEMINI_API_KEY',
    base_url=PORTKEY_GATEWAY_URL,
    default_headers=createHeaders(
        provider='google',
        api_key='PORTKEY_API_KEY',
        strict_open_ai_compliance=False
    )
)

response = openai.chat.completions.create(
    model='gemini-2.5-computer-use-preview-10-2025',
    stream=False,
    messages=[
        {
            'role': 'system',
            'content': 'Say Hi'
        },
        {
            'role': 'user',
            'content': "Go to google.com and search for 'weather in New York'"
        },
        {
            'role': 'assistant',
            'tool_calls': [
                {
                    'id': 'portkey-50925c03-b8cc-4057-948b-13a9d9de19e0',
                    'type': 'function',
                    'function': {
                        'name': 'open_web_browser',
                        'arguments': '{}'
                    }
                }
            ]
        },
        {
            'role': 'user',
            'content': "I've opened the browser"
        }
    ],
    tools=[
        {
            'type': 'function',
            'function': {
                'name': 'computerUse',
                'parameters': {
                    'environment': 'ENVIRONMENT_BROWSER'
                }
            }
        }
    ]
)

print(response)
```
</CodeGroup>


## Extended Thinking (Reasoning Models) (Beta)

<Note>
The assistants thinking response is returned in the `response_chunk.choices[0].delta.content_blocks` array, not the `response.choices[0].message.content` string.
</Note>

Models like `gemini-2.5-flash-preview-04-17` `gemini-2.5-flash-preview-04-17` support [extended thinking](https://cloud.google.com/vertex-ai/generative-ai/docs/partner-models/use-claude#claude-3-7-sonnet).
This is similar to openai thinking, but you get the model's reasoning as it processes the request as well.

Note that you will have to set [`strict_open_ai_compliance=False`](/product/ai-gateway/strict-open-ai-compliance) in the headers to use this feature.

### Single turn conversation
<CodeGroup>
    ```py Python
    from portkey_ai import Portkey

    # Initialize the Portkey client
    portkey = Portkey(
        api_key="PORTKEY_API_KEY",  # Replace with your Portkey API key
        provider="@PROVIDER",
        strict_open_ai_compliance=False
    )

    # Create the request
    response = portkey.chat.completions.create(
      model="gemini-2.5-flash-preview-04-17",
      max_tokens=3000,
      thinking={
          "type": "enabled",
          "budget_tokens": 2030
      },
      stream=True,
      messages=[
          {
              "role": "user",
              "content": [
                  {
                      "type": "text",
                      "text": "when does the flight from new york to bengaluru land tomorrow, what time, what is its flight number, and what is its baggage belt?"
                  }
              ]
          }
      ]
    )
    print(response)
    # in case of streaming responses you'd have to parse the response_chunk.choices[0].delta.content_blocks array
    # response = portkey.chat.completions.create(
    #   ...same config as above but with stream: true
    # )
    # for chunk in response:
    #     if chunk.choices[0].delta:
    #         content_blocks = chunk.choices[0].delta.get("content_blocks")
    #         if content_blocks is not None:
    #             for content_block in content_blocks:
    #                 print(content_block)
    ```
    ```ts NodeJS
    import Portkey from 'portkey-ai';

    // Initialize the Portkey client
    const portkey = new Portkey({
      apiKey: "PORTKEY_API_KEY", // Replace with your Portkey API key
      provider:"@PROVIDER", // your vertex-ai virtual key
      strictOpenAiCompliance: false
    });

    // Generate a chat completion
    async function getChatCompletionFunctions() {
        const response = await portkey.chat.completions.create({
          model: "gemini-2.5-flash-preview-04-17",
          max_tokens: 3000,
          thinking: {
              type: "enabled",
              budget_tokens: 2030
          },
          stream: true,
          messages: [
              {
                  role: "user",
                  content: [
                      {
                          type: "text",
                          text: "when does the flight from new york to bengaluru land tomorrow, what time, what is its flight number, and what is its baggage belt?"
                      }
                  ]
              }
          ]
        });
        console.log(response);
      // in case of streaming responses you'd have to parse the response_chunk.choices[0].delta.content_blocks array
      // const response = await portkey.chat.completions.create({
      //   ...same config as above but with stream: true
      // });
      // for await (const chunk of response) {
      //   if (chunk.choices[0].delta?.content_blocks) {
      //     for (const contentBlock of chunk.choices[0].delta.content_blocks) {
      //       console.log(contentBlock);
      //     }
      //   }
      // }
      }
    // Call the function
    getChatCompletionFunctions();
    ```
    ```js OpenAI NodeJS
    import OpenAI from 'openai'; // We're using the v4 SDK
    import { PORTKEY_GATEWAY_URL, createHeaders } from 'portkey-ai'

    const openai = new OpenAI({
      apiKey: 'VERTEX_API_KEY', // defaults to process.env["OPENAI_API_KEY"],
      baseURL: PORTKEY_GATEWAY_URL,
      defaultHeaders: createHeaders({
        provider: "vertex-ai",
        apiKey: "PORTKEY_API_KEY", // defaults to process.env["PORTKEY_API_KEY"]
        strictOpenAiCompliance: false
      })
    });

    // Generate a chat completion with streaming
    async function getChatCompletionFunctions(){
      const response = await openai.chat.completions.create({
        model: "gemini-2.5-flash-preview-04-17",
        max_tokens: 3000,
        thinking: {
            type: "enabled",
            budget_tokens: 2030
        },
        stream: true,
        messages: [
            {
                role: "user",
                content: [
                    {
                        type: "text",
                        text: "when does the flight from new york to bengaluru land tomorrow, what time, what is its flight number, and what is its baggage belt?"
                    }
                ]
            }
        ],
      });

      console.log(response)
      // in case of streaming responses you'd have to parse the response_chunk.choices[0].delta.content_blocks array
      // const response = await openai.chat.completions.create({
      //   ...same config as above but with stream: true
      // });
      // for await (const chunk of response) {
      //   if (chunk.choices[0].delta?.content_blocks) {
      //     for (const contentBlock of chunk.choices[0].delta.content_blocks) {
      //       console.log(contentBlock);
      //     }
      //   }
      // }
    }
    await getChatCompletionFunctions();
    ```
    ```py OpenAI Python
    from openai import OpenAI
    from portkey_ai import PORTKEY_GATEWAY_URL, createHeaders

    openai = OpenAI(
        api_key='VERTEX_API_KEY',
        base_url=PORTKEY_GATEWAY_URL,
        default_headers=createHeaders(
            provider="vertex-ai",
            api_key="PORTKEY_API_KEY",
            strict_open_ai_compliance=False
        )
    )


    response = openai.chat.completions.create(
        model="gemini-2.5-flash-preview-04-17",
        max_tokens=3000,
        thinking={
            "type": "enabled",
            "budget_tokens": 2030
        },
        stream=True,
        messages=[
            {
                "role": "user",
                "content": [
                    {
                        "type": "text",
                        "text": "when does the flight from new york to bengaluru land tomorrow, what time, what is its flight number, and what is its baggage belt?"
                    }
                ]
            }
        ]
    )

    print(response)
    ```
    ```sh cURL
    curl "https://api.portkey.ai/v1/chat/completions" \
      -H "Content-Type: application/json" \
      -H "x-portkey-api-key: $PORTKEY_API_KEY" \
      -H "x-portkey-provider: vertex-ai" \
      -H "x-api-key: $VERTEX_API_KEY" \
      -H "x-portkey-strict-open-ai-compliance: false" \
      -d '{
        "model": "gemini-2.5-flash-preview-04-17",
        "max_tokens": 3000,
        "thinking": {
          "type": "enabled",
          "budget_tokens": 2030
        },
        "stream": true,
        "messages": [
          {
            "role": "user",
            "content": [
              {
                "type": "text",
                "text": "when does the flight from new york to bengaluru land tomorrow, what time, what is its flight number, and what is its baggage belt?"
              }
            ]
          }
        ]
      }'
    ```
</CodeGroup>

<Note>
    To disable thinking for gemini models like `gemini-2.5-flash-preview-04-17`, you are required to explicitly set `budget_tokens` to `0`.
    ```json
    "thinking": {
        "type": "enabled",
        "budget_tokens": 0
    }
    ```
</Note>

<Info>
Gemini grounding mode may not work via Portkey SDK. Contact support@portkey.ai for assistance.
</Info>

## Next Steps

The complete list of features supported in the SDK are available on the link below.
<Card title="SDK" href="/api-reference/portkey-sdk-client">
</Card>

You'll find more information in the relevant sections:

1. [Add metadata to your requests](/product/observability/metadata)
2. [Add gateway configs to your Gemini requests](/product/ai-gateway/configs)
3. [Tracing Google Gemini requests](/product/observability/traces)
4. [Setup a fallback from OpenAI to Gemini APIs](/product/ai-gateway/fallbacks)
