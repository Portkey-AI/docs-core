---
title: "Nscale (EU Sovereign)"
---

Portkey provides a robust and secure gateway to facilitate the integration of various Large Language Models (LLMs) into your applications, including the models hosted on [Nscale](https://docs.nscale.com/docs/inference/serverless-models/current).

<Note>
Provider Slug. `nscale`
</Note>

## Portkey SDK Integration with Nscale

Portkey provides a consistent API to interact with models from various providers. To integrate Nscale with Portkey:

### 1. Install the Portkey SDK
<Tabs>
  <Tab title="NodeJS">
    ```sh
    npm install --save portkey-ai
    ```
  </Tab>
  <Tab title="Python">
    ```sh
    pip install portkey-ai
    ```
  </Tab>
</Tabs>

### 2. Initialize Portkey with the Virtual Key
To use Nscale with Virtual Key, [get your API key from here](https://console.nscale.com). Then add it to Portkey to create the virtual key.
<Tabs>
  <Tab title="NodeJS SDK">
    ```js
    import Portkey from 'portkey-ai'

    const portkey = new Portkey({
      apiKey: "PORTKEY_API_KEY", // defaults to process.env["PORTKEY_API_KEY"]
      provider:"@PROVIDER" // Your Nscale Virtual Key
    })
    ```
  </Tab>
  <Tab title="Python SDK">
    ```python
    from portkey_ai import Portkey

    portkey = Portkey(
      api_key="PORTKEY_API_KEY",  # Replace with your Portkey API key
      provider="@NSCALE_PROVIDER"
    )
    ```
  </Tab>
</Tabs>

### 3. Invoke Chat Completions
<Tabs>
  <Tab title="NodeJS SDK">
    ```js
    const chatCompletion = await portkey.chat.completions.create({
      messages: [{ role: 'user', content: 'Say this is a test' }],
      model: 'meta-llama/Llama-4-Scout-17B-16E-Instruct',
    });

    console.log(chatCompletion.choices);
    ```
  </Tab>
  <Tab title="Python SDK">
    ```python
    completion = portkey.chat.completions.create(
      messages= [{ "role": 'user', "content": 'Say this is a test' }],
      model= 'meta-llama/Llama-4-Scout-17B-16E-Instruct'
    )

    print(completion)
    ```
  </Tab>
</Tabs>

### 4. Invoke Image Generation
<Tabs>
  <Tab title="NodeJS SDK">
    ```js
    const response = await portkey.images.generations.create({
      prompt: "A beautiful sunset over mountains",
      model: "stabilityai/stable-diffusion-xl-base-1.0",
      n: 1,
      size: "1024x1024"
    });

    console.log(response.data[0].url);
    ```
  </Tab>
  <Tab title="Python SDK">
    ```python
    response = portkey.images.generate(
      prompt="A beautiful sunset over mountains",
      model="stabilityai/stable-diffusion-xl-base-1.0",
      n=1,
      size="1024x1024"
    )

    print(response.data[0].url)
    ```
  </Tab>
</Tabs>

---

## Supported Models

<CardGroup cols={1}>
  <Card title="View Available Models" icon="list" href="https://docs.nscale.com/docs/inference/serverless-models/current">
    Explore the complete list of available models on Nscale's documentation, including chat models, image generation models, and their pricing details.
  </Card>
</CardGroup>

---

## Next Steps

The complete list of features supported in the SDK are available on the link below.
<Card title="SDK" href="/api-reference/sdk">
</Card>

You'll find more information in the relevant sections:

1. [Add metadata to your requests](/product/observability/metadata)
2. [Add gateway configs to your Nscale requests](/product/ai-gateway/configs)
3. [Tracing Nscale requests](/product/observability/traces)
4. [Setup a fallback from OpenAI to Nscale](/product/ai-gateway/fallbacks) 