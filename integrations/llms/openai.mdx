---
title: "OpenAI"
description: "Integrate OpenAI's GPT models with Portkey's AI Gateway"
---

Portkey provides a robust and secure gateway to integrate [OpenAI's APIs](https://platform.openai.com/docs/api-reference/introduction) into your applications, including GPT-4o, o1, DALL·E, Whisper, and more.

With Portkey, take advantage of features like fast AI gateway access, observability, prompt management, and more, while securely managing API keys through [Model Catalog](/product/model-catalog).

<CardGroup cols={3}>
  <Card title="All Models" icon="check-circle" color="#10b981">
    Full support for GPT-4o, o1, GPT-4, GPT-3.5, and all OpenAI models
  </Card>
  <Card title="All Endpoints" icon="check-circle" color="#10b981">
    Chat, completions, embeddings, audio, images, and more fully supported
  </Card>
  <Card title="Multi-SDK Support" icon="check-circle" color="#10b981">
    Use with OpenAI SDK, Portkey SDK, or popular frameworks like LangChain
  </Card>
</CardGroup>

## Quick Start

Get OpenAI working in 3 steps:

<CodeGroup>
```python Python icon="python"
from portkey_ai import Portkey

# 1. Install: pip install portkey-ai
# 2. Add @openai provider in model catalog
# 3. Use it:

portkey = Portkey(api_key="PORTKEY_API_KEY")

response = portkey.chat.completions.create(
    model="@openai/gpt-4o",
    messages=[{"role": "user", "content": "Say this is a test"}]
)

print(response.choices[0].message.content)
```

```js Javascript icon="square-js"
import Portkey from 'portkey-ai'

// 1. Install: npm install portkey-ai
// 2. Add @openai provider in model catalog
// 3. Use it:

const portkey = new Portkey({
    apiKey: "PORTKEY_API_KEY"
})

const response = await portkey.chat.completions.create({
    model: "@openai/gpt-4o",
    messages: [{ role: "user", content: "Say this is a test" }]
})

console.log(response.choices[0].message.content)
```

```python OpenAI Py icon="openai"
from openai import OpenAI
from portkey_ai import PORTKEY_GATEWAY_URL

# 1. Install: pip install openai portkey-ai
# 2. Add @openai provider in model catalog
# 3. Use it:

client = OpenAI(
    api_key="PORTKEY_API_KEY",  # Portkey API key
    base_url=PORTKEY_GATEWAY_URL
)

response = client.chat.completions.create(
    model="@openai/gpt-4o",
    messages=[{"role": "user", "content": "Say this is a test"}]
)

print(response.choices[0].message.content)
```

```js OpenAI JS icon="openai"
import OpenAI from "openai"
import { PORTKEY_GATEWAY_URL } from "portkey-ai"

// 1. Install: npm install openai portkey-ai
// 2. Add @openai provider in model catalog
// 3. Use it:

const client = new OpenAI({
    apiKey: "PORTKEY_API_KEY",  // Portkey API key
    baseURL: PORTKEY_GATEWAY_URL
})

const response = await client.chat.completions.create({
    model: "@openai/gpt-4o",
    messages: [{ role: "user", content: "Say this is a test" }]
})

console.log(response.choices[0].message.content)
```

```sh cURL icon="square-terminal"
# 1. Add @openai provider in model catalog
# 2. Use it:

curl https://api.portkey.ai/v1/chat/completions \
  -H "Content-Type: application/json" \
  -H "x-portkey-api-key: $PORTKEY_API_KEY" \
  -H "x-portkey-provider: @openai" \
  -d '{
    "model": "gpt-4o",
    "messages": [
      { "role": "user", "content": "Say this is a test" }
    ]
  }'
```
</CodeGroup>

<Note>
**Tip:** You can also set `provider="@openai"` in `Portkey()` and use just `model="gpt-4o"` in the request.

**Legacy support:** The `virtual_key` parameter still works for backwards compatibility.
</Note>

## Add Provider in Model Catalog

1. Go to [**Model Catalog → Add Provider**](https://app.portkey.ai/model-catalog/providers)
2. Select **OpenAI**
3. Choose existing credentials or create new by entering your [OpenAI API key](https://platform.openai.com/api-keys)
4. (Optional) Add your OpenAI **Organization ID** and **Project ID** for better cost tracking
5. Name your provider (e.g., `openai-prod`)

<Card title="Complete Setup Guide →" href="/product/model-catalog">
  See all setup options, code examples, and detailed instructions
</Card>

## Basic Usage

### Streaming

Stream responses for real-time output in your applications:

<CodeGroup>
```python Python icon="python"
response = portkey.chat.completions.create(
    model="@openai/gpt-4o",
    messages=[{"role": "user", "content": "Tell me a story"}],
    stream=True
)

for chunk in response:
    if chunk.choices[0].delta.content:
        print(chunk.choices[0].delta.content, end="", flush=True)
```

```js Javascript icon="square-js"
const response = await portkey.chat.completions.create({
    model: "@openai/gpt-4o",
    messages: [{ role: "user", content: "Tell me a story" }],
    stream: true
})

for await (const chunk of response) {
    if (chunk.choices[0]?.delta?.content) {
        process.stdout.write(chunk.choices[0].delta.content)
    }
}
```
</CodeGroup>

## Advanced Features

### Responses API

OpenAI's Responses API combines the best of both Chat Completions and Assistants APIs. Portkey fully supports this API with both the Portkey SDK and OpenAI SDK.

<CodeGroup>
```python Python icon="python"
from portkey_ai import Portkey

portkey = Portkey(api_key="PORTKEY_API_KEY")

response = portkey.responses.create(
    model="@openai/gpt-4.1",
    input="Tell me a three sentence bedtime story about a unicorn."
)

print(response)
```

```js Javascript icon="square-js"
import Portkey from 'portkey-ai'

const portkey = new Portkey({
    apiKey: "PORTKEY_API_KEY"
})

const response = await portkey.responses.create({
    model: "@openai/gpt-4.1",
    input: "Tell me a three sentence bedtime story about a unicorn."
})

console.log(response)
```

```python OpenAI Py icon="openai"
from openai import OpenAI
from portkey_ai import PORTKEY_GATEWAY_URL

client = OpenAI(
    api_key="PORTKEY_API_KEY",
    base_url=PORTKEY_GATEWAY_URL
)

response = client.responses.create(
    model="@openai/gpt-4.1",
    input="Tell me a three sentence bedtime story about a unicorn."
)

print(response)
```

```js OpenAI JS icon="openai"
import OpenAI from 'openai'
import { PORTKEY_GATEWAY_URL } from 'portkey-ai'

const openai = new OpenAI({
    apiKey: "PORTKEY_API_KEY",
    baseURL: PORTKEY_GATEWAY_URL
})

const response = await openai.responses.create({
    model: "@openai/gpt-4.1",
    input: "Tell me a three sentence bedtime story about a unicorn."
})

console.log(response)
```
</CodeGroup>

<Note>
The Responses API provides a more flexible foundation for building agentic applications with built-in tools that execute automatically.
</Note>

<Card title="Remote MCP support on Responses API" href="/product/ai-gateway/remote-mcp">
    Portkey supports Remote MCP support by OpenAI on its Responses API. Learn More
</Card>

#### Streaming with Responses API

<CodeGroup>
```python Python icon="python"
response = portkey.responses.create(
    model="@openai/gpt-4.1",
    instructions="You are a helpful assistant.",
    input="Hello!",
    stream=True
)

for event in response:
    print(event)
```

```js Javascript icon="square-js"
const response = await portkey.responses.create({
    model: "@openai/gpt-4.1",
    instructions: "You are a helpful assistant.",
    input: "Hello!",
    stream: true
})

for await (const event of response) {
    console.log(event)
}
```

```python OpenAI Py icon="openai"
response = client.responses.create(
    model="gpt-4.1",
    instructions="You are a helpful assistant.",
    input="Hello!",
    stream=True
)

for event in response:
    print(event)
```

```js OpenAI JS icon="openai"
const response = await openai.responses.create({
    model: "gpt-4.1",
    instructions: "You are a helpful assistant.",
    input: "Hello!",
    stream: true
})

for await (const event of response) {
    console.log(event)
}
```
</CodeGroup>

### Realtime API

Portkey supports OpenAI's Realtime API with a seamless integration. This allows you to use Portkey's logging, cost tracking, and guardrail features while using the Realtime API.

<Card title="Realtime API" href="/product/ai-gateway/realtime-api"/>



### Using Vision Models

Portkey's multimodal Gateway fully supports OpenAI vision models as well. See this guide for more info:

<Info>
[Vision](/product/ai-gateway/multimodal-capabilities/vision)
</Info>

#### Vision with the Responses API

The Responses API also processes images alongside text:

<CodeGroup>
```python Python icon="python"
response = portkey.responses.create(
    model="@openai/gpt-4.1",
    input=[
        {
            "role": "user",
            "content": [
                { "type": "input_text", "text": "What is in this image?" },
                {
                    "type": "input_image",
                    "image_url": "https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg"
                }
            ]
        }
    ]
)

print(response)
```

```js Javascript icon="square-js"
const response = await portkey.responses.create({
    model: "@openai/gpt-4.1",
    input: [
        {
            role: "user",
            content: [
                { type: "input_text", text: "What is in this image?" },
                {
                    type: "input_image",
                    image_url: "https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg"
                }
            ]
        }
    ]
})

console.log(response)
```

```python OpenAI Py icon="openai"
response = client.responses.create(
    model="gpt-4.1",
    input=[
        {
            "role": "user",
            "content": [
                { "type": "input_text", "text": "What is in this image?" },
                {
                    "type": "input_image",
                    "image_url": "https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg"
                }
            ]
        }
    ]
)

print(response)
```

```js OpenAI JS icon="openai"
const response = await openai.responses.create({
    model: "gpt-4.1",
    input: [
        {
            role: "user",
            content: [
                { type: "input_text", text: "What is in this image?" },
                {
                    type: "input_image",
                    image_url: "https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg"
                }
            ]
        }
    ]
})

console.log(response)
```
</CodeGroup>



### Function Calling

Function calls within your OpenAI or Portkey SDK operations remain standard. These logs will appear in Portkey, highlighting the utilized functions and their outputs.

Additionally, you can define functions within your prompts and invoke the `portkey.prompts.completions.create` method as above.


#### Function Calling with the Responses API

The Responses API also supports function calling with the same powerful capabilities:

<CodeGroup>
```python Python icon="python"
tools = [
    {
        "type": "function",
        "name": "get_current_weather",
        "description": "Get the current weather in a given location",
        "parameters": {
            "type": "object",
            "properties": {
                "location": {
                    "type": "string",
                    "description": "The city and state, e.g. San Francisco, CA"
                },
                "unit": {"type": "string", "enum": ["celsius", "fahrenheit"]}
            },
            "required": ["location", "unit"]
        }
    }
]

response = portkey.responses.create(
    model="@openai/gpt-4.1",
    tools=tools,
    input="What is the weather like in Boston today?",
    tool_choice="auto"
)

print(response)
```

```js Javascript icon="square-js"
const tools = [
    {
        type: "function",
        name: "get_current_weather",
        description: "Get the current weather in a given location",
        parameters: {
            type: "object",
            properties: {
                location: {
                    type: "string",
                    description: "The city and state, e.g. San Francisco, CA"
                },
                unit: { type: "string", enum: ["celsius", "fahrenheit"] }
            },
            required: ["location", "unit"]
        }
    }
]

const response = await portkey.responses.create({
    model: "@openai/gpt-4.1",
    tools: tools,
    input: "What is the weather like in Boston today?",
    tool_choice: "auto"
})

console.log(response)
```

```python OpenAI Py icon="openai"
tools = [
    {
        "type": "function",
        "name": "get_current_weather",
        "description": "Get the current weather in a given location",
        "parameters": {
            "type": "object",
            "properties": {
                "location": {
                    "type": "string",
                    "description": "The city and state, e.g. San Francisco, CA"
                },
                "unit": {"type": "string", "enum": ["celsius", "fahrenheit"]}
            },
            "required": ["location", "unit"]
        }
    }
]

response = client.responses.create(
    model="gpt-4.1",
    tools=tools,
    input="What is the weather like in Boston today?",
    tool_choice="auto"
)

print(response)
```

```js OpenAI JS icon="openai"
const tools = [
    {
        type: "function",
        name: "get_current_weather",
        description: "Get the current weather in a given location",
        parameters: {
            type: "object",
            properties: {
                location: {
                    type: "string",
                    description: "The city and state, e.g. San Francisco, CA"
                },
                unit: { type: "string", enum: ["celsius", "fahrenheit"] }
            },
            required: ["location", "unit"]
        }
    }
]

const response = await openai.responses.create({
    model: "gpt-4.1",
    tools: tools,
    input: "What is the weather like in Boston today?",
    tool_choice: "auto"
})

console.log(response)
```
</CodeGroup>



### Fine-Tuning

Please refer to our fine-tuning guides to take advantage of Portkey's advanced [continuous fine-tuning](/product/autonomous-fine-tuning) capabilities.

### Image Generation

Portkey supports multiple modalities for OpenAI. Make image generation requests through Portkey's AI Gateway the same way as making completion calls.

<CodeGroup>
```js Javascript icon="square-js"
// Define the OpenAI client as shown above

const image = await openai.images.generate({
  model:"dall-e-3",
  prompt:"Lucy in the sky with diamonds",
  size:"1024x1024"
})
```

```python Python icon="python"
# Define the OpenAI client as shown above

image = openai.images.generate(
  model="dall-e-3",
  prompt="Lucy in the sky with diamonds",
  size="1024x1024"
)
```
</CodeGroup>

Portkey's fast AI gateway captures the information about the request on your Portkey Dashboard. On your logs screen, you'd be able to see this request with the request and response.
<Frame caption="Log view for an image generation request on OpenAI">
<img src="/images/llms/openai-logs.png" alt="querying-vision-language-models" />
</Frame>

More information on image generation is available in the [API Reference](/provider-endpoints/images/create-image#create-image).

### Video Generation with Sora

Portkey supports OpenAI's Sora video generation models through the AI Gateway. Generate videos using the Portkey Python SDK:

```python
from portkey_ai import Portkey

client = Portkey(
    api_key="PORTKEY_API_KEY"
)

video = client.videos.create(
    model="@openai/sora-2",
    prompt="A video of a cool cat on a motorcycle in the night",
)

print("Video generation started:", video)
```

<Note>
Pricing for video generation requests will be visible on your Portkey dashboard, allowing you to track costs alongside your other API usage.
</Note>

### Audio - Transcription, Translation, and Text-to-Speech

Portkey's multimodal Gateway also supports the `audio` methods on OpenAI API. Check out the below guides for more info:

Check out the below guides for more info:
<Info>
[Text-to-Speech](/product/ai-gateway/multimodal-capabilities/text-to-speech)
</Info>
<Info>
[Speech-to-Text](/product/ai-gateway/multimodal-capabilities/speech-to-text)
</Info>
---

## Integrated Tools with Responses API

### Web Search Tool

Web search delivers accurate and clearly-cited answers from the web, using the same tool as search in ChatGPT:

<CodeGroup>
```python Python icon="python"
response = portkey.responses.create(
    model="@openai/gpt-4.1",
    tools=[{
        "type": "web_search_preview",
        "search_context_size": "medium", # Options: "high", "medium" (default), or "low"
        "user_location": {  # Optional - for localized results
            "type": "approximate",
            "country": "US",
            "city": "San Francisco",
            "region": "California"
        }
    }],
    input="What was a positive news story from today?"
)

print(response)
```

```js Javascript icon="square-js"
const response = await portkey.responses.create({
    model: "@openai/gpt-4.1",
    tools: [{
        type: "web_search_preview",
        search_context_size: "medium", // Options: "high", "medium" (default), or "low"
        user_location: {  // Optional - for localized results
            type: "approximate",
            country: "US",
            city: "San Francisco",
            region: "California"
        }
    }],
    input: "What was a positive news story from today?"
})

console.log(response)
```
</CodeGroup>

<Note>
  **Options for `search_context_size`:**
  - `high`: Most comprehensive context, higher cost, slower response
  - `medium`: Balanced context, cost, and latency (default)
  - `low`: Minimal context, lowest cost, fastest response

  Responses include citations for URLs found in search results, with clickable references.
</Note>

### File Search Tool

File search enables quick retrieval from your knowledge base across multiple file types:

<CodeGroup>
```python Python icon="python"
response = portkey.responses.create(
    model="@openai/gpt-4.1",
    tools=[{
        "type": "file_search",
        "vector_store_ids": ["vs_1234567890"],
        "max_num_results": 20,
        "filters": {  # Optional - filter by metadata
            "type": "eq",
            "key": "document_type",
            "value": "report"
        }
    }],
    input="What are the attributes of an ancient brown dragon?"
)

print(response)
```

```js Javascript icon="square-js"
const response = await portkey.responses.create({
    model: "@openai/gpt-4.1",
    tools: [{
        type: "file_search",
        vector_store_ids: ["vs_1234567890"],
        max_num_results: 20,
        filters: {  // Optional - filter by metadata
            type: "eq",
            key: "document_type",
            value: "report"
        }
    }],
    input: "What are the attributes of an ancient brown dragon?"
})

console.log(response)
```
</CodeGroup>

<Note>
  This tool requires you to first create a vector store and upload files to it. Supports various file formats including PDFs, DOCXs, TXT, and more. Results include file citations in the response.
</Note>

### Enhanced Reasoning

Control the depth of model reasoning for more comprehensive analysis:

<CodeGroup>
```python Python icon="python"
response = portkey.responses.create(
    model="@openai/o3-mini",
    input="How much wood would a woodchuck chuck?",
    reasoning={
        "effort": "high"  # Options: "high", "medium", or "low"
    }
)

print(response)
```

```js Javascript icon="square-js"
const response = await portkey.responses.create({
    model: "@openai/o3-mini",
    input: "How much wood would a woodchuck chuck?",
    reasoning: {
        effort: "high"  // Options: "high", "medium", or "low"
    }
})

console.log(response)
```
</CodeGroup>

### Computer Use Assistant

Portkey also supports the Computer Use Assistant (CUA) tool, which helps agents control computers or virtual machines through screenshots and actions. This feature is available for select developers as a research preview on premium tiers.


<Card href="https://platform.openai.com/docs/guides/tools-computer-use?lang=python">
    Learn More about Computer use tool here
</Card>



## Managing OpenAI Projects & Organizations in Portkey

When integrating OpenAI with Portkey, specify your OpenAI organization and project IDs along with your API key. This is particularly useful if you belong to multiple organizations or are accessing projects through a legacy user API key.

Specifying the organization and project IDs helps you maintain better control over your access rules, usage, and costs.

Add your Org & Project details using:

1. Adding in Model Catalog (Recommended)
2. Defining a Gateway Config
3. Passing Details in a Request

Let's explore each method in more detail.

### Using Model Catalog

When adding OpenAI from the Model Catalog, Portkey automatically displays optional fields for the organization ID and project ID alongside the API key field.

[Get your OpenAI API key from here](https://platform.openai.com/api-keys), then add it to Portkey along with your org/project details.
<Frame>
![LOGO](/images/llms/virtual.png)
</Frame>
<Info>
[Model Catalog](/product/model-catalog)
</Info>
Portkey takes budget management a step further than OpenAI. While OpenAI allows setting budget limits per project, Portkey enables you to set budget limits for each provider you create. For more information on budget limits, refer to this documentation:

<Info>
[Budget Limits](/product/ai-gateway/virtual-keys/budget-limits)
</Info>

### Using the Gateway Config

You can also specify the organization and project details in the gateway config, either at the root level or within a specific target.

```json
{
	"provider": "@openai",
	"openai_organization": "org-xxxxxx",
	"openai_project": "proj_xxxxxxxx"
}
```

### While Making a Request

You can also pass your organization and project details directly when making a request using curl, the OpenAI SDK, or the Portkey SDK.

<CodeGroup>
```python OpenAI Py icon="openai"
from openai import OpenAI
from portkey_ai import PORTKEY_GATEWAY_URL

client = OpenAI(
    api_key="PORTKEY_API_KEY",
    organization="org-xxxxxxxxxx",
    project="proj_xxxxxxxxx",
    base_url=PORTKEY_GATEWAY_URL
)

chat_complete = client.chat.completions.create(
    model="@openai/gpt-4o",
    messages=[{"role": "user", "content": "Say this is a test"}],
)

print(chat_complete.choices[0].message.content)
```

```js OpenAI JS icon="openai"
import OpenAI from "openai"
import { PORTKEY_GATEWAY_URL } from "portkey-ai"

const openai = new OpenAI({
  apiKey: "PORTKEY_API_KEY",
  organization: "org-xxxxxx",
  project: "proj_xxxxxxx",
  baseURL: PORTKEY_GATEWAY_URL
})

async function main() {
  const chatCompletion = await openai.chat.completions.create({
    messages: [{ role: "user", content: "Say this is a test" }],
    model: "@openai/gpt-4o",
  })

  console.log(chatCompletion.choices)
}

main()
```

```sh cURL icon="square-terminal"
curl https://api.portkey.ai/v1/chat/completions \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer $OPENAI_API_KEY" \
  -H "x-portkey-openai-organization: org-xxxxxxx" \
  -H "x-portkey-openai-project: proj_xxxxxxx" \
  -H "x-portkey-api-key: $PORTKEY_API_KEY" \
  -H "x-portkey-provider: @openai" \
  -d '{
    "model": "gpt-4o",
    "messages": [{"role": "user","content": "Hello!"}]
  }'
```

```python Python icon="python"
from portkey_ai import Portkey

portkey = Portkey(
    api_key="PORTKEY_API_KEY",
    Authorization="Bearer OPENAI_API_KEY",
    openai_organization="org-xxxxxxxxx",
    openai_project="proj_xxxxxxxxx",
)

chat_complete = portkey.chat.completions.create(
    model="@openai/gpt-4o",
    messages=[{"role": "user", "content": "Say this is a test"}],
)

print(chat_complete.choices[0].message.content)
```

```js Javascript icon="square-js"
import Portkey from "portkey-ai"

const portkey = new Portkey({
  apiKey: "PORTKEY_API_KEY",
  Authorization: "Bearer OPENAI_API_KEY",
  openaiOrganization: "org-xxxxxxxxxxx",
  openaiProject: "proj_xxxxxxxxxxxxx",
})

async function main() {
  const chatCompletion = await portkey.chat.completions.create({
    messages: [{ role: "user", content: "Say this is a test" }],
    model: "@openai/gpt-4o",
  })

  console.log(chatCompletion.choices)
}

main()
```
</CodeGroup>

---

## Limitations

<Warning>
Portkey does not currently support:
- Streaming for audio endpoints
</Warning>

### Vision Model Limitations
- **Medical images**: Vision models are not suitable for interpreting specialized medical images like CT scans and shouldn't be used for medical advice.
- **Non-English**: The models may not perform optimally when handling images with text of non-Latin alphabets, such as Japanese or Korean.
- **Small text**: Enlarge text within the image to improve readability, but avoid cropping important details.
- **Rotation**: The models may misinterpret rotated / upside-down text or images.
- **Visual elements**: The models may struggle to understand graphs or text where colors or styles like solid, dashed, or dotted lines vary.
- **Spatial reasoning**: The models struggle with tasks requiring precise spatial localization, such as identifying chess positions.
- **Accuracy**: The models may generate incorrect descriptions or captions in certain scenarios.
- **Image shape**: The models struggle with panoramic and fisheye images.
- **Metadata and resizing**: The models do not process original file names or metadata, and images are resized before analysis, affecting their original dimensions.
- **Counting**: May give approximate counts for objects in images.
- **CAPTCHAS**: For safety reasons, CAPTCHA submissions are blocked by OpenAI.

### Image Generation Limitations
- **DALL·E 3 Restrictions:**
  - Only supports image generation (no editing or variations)
  - Limited to one image per request
  - Fixed size options: 1024x1024, 1024x1792, or 1792x1024 pixels
  - Automatic prompt enhancement cannot be disabled
- **Image Requirements:**
  - Must be PNG format
  - Maximum file size: 4MB
  - Must be square dimensions
  - For edits/variations: input images must meet same requirements
- **Content Restrictions:**
  - All prompts and images are filtered based on OpenAI's content policy
  - Violating content will return an error
  - Edited areas must be described in full context, not just the edited portion
- **Technical Limitations:**
  - Image URLs expire after 1 hour
  - Image editing (inpainting) and variations only available in DALL·E 2
  - Response format limited to URL or Base64 data

### Speech-to-Text Limitations
- **File Restrictions:**
  - Maximum file size: 25 MB
  - Supported formats: mp3, mp4, mpeg, mpga, m4a, wav, webm
  - No streaming support
- **Language Limitations:**
  - Translation output available only in English
  - Variable accuracy for non-listed languages
  - Limited control over generated audio compared to other language models
- **Technical Constraints:**
  - Prompt limited to first 244 tokens
  - Restricted processing for longer audio files
  - No real-time transcription support

### Text-to-Speech Limitations
- **Voice Restrictions:**
  - Limited to 6 pre-built voices (alloy, echo, fable, onyx, nova, shimmer)
  - Voices optimized primarily for English
  - No custom voice creation support
  - No direct control over emotional range or tone
- **Audio Quality Trade-offs:**
  - tts-1: Lower latency but potentially more static
  - tts-1-hd: Higher quality but increased latency
  - Quality differences may vary by listening device
- **Usage Requirements:**
  - Must disclose AI-generated nature to end users
  - Cannot create custom voice clones
  - Performance varies for non-English languages

---

## Frequently Asked Questions

### General FAQs
<AccordionGroup>
<Accordion title="How to get the OpenAI API key?">
You can sign up to OpenAI [here](https://platform.openai.com/docs/overview) and grab your scoped API key [here](https://platform.openai.com/api-keys).
</Accordion>
<Accordion title="Is it free to use the OpenAI API?">
The OpenAI API can be used by signing up to the OpenAI platform. You can find the pricing info [here](https://openai.com/api/pricing/)
</Accordion>
<Accordion title="I am getting rate limited on OpenAI API">
You can find your current rate limits imposed by OpenAI [here](https://platform.openai.com/settings/organization/limits). For more tips, check out [this guide](/guides/getting-started/tackling-rate-limiting).
</Accordion>
</AccordionGroup>

### Vision FAQs
<AccordionGroup>
<Accordion title="Can I fine-tune OpenAI models on vision requests?">
Vision fine-tuning is available for [some OpenAI models](https://platform.openai.com/docs/guides/fine-tuning#vision).
</Accordion>
<Accordion title="Can I use gpt-4o or other chat models to generate images?">
No, you can use dall-e-3 to generate images and gpt-4o and other chat models to understand images.
</Accordion>
<Accordion title="What type of files can I upload for vision requests?">
OpenAI currently supports PNG (.png), JPEG (.jpeg and .jpg), WEBP (.webp), and non-animated GIF (.gif).
</Accordion>
<Accordion title="For vision requests, is there a limit to the size of the image I can upload?">
OpenAI currently restricts image uploads to 20MB per image.
</Accordion>
<Accordion title="How do rate limits work for vision requests?">
OpenAI processes images at the token level, so each image that's processed counts towards your tokens per minute (TPM) limit. See how OpenAI [calculates costs here](https://platform.openai.com/docs/guides/vision#calculating-costs) for details on the formula used to determine token count per image.
</Accordion>
<Accordion title="Can models understand image metadata?">
No, the models do not receive image metadata.
</Accordion>
</AccordionGroup>

### Embedding FAQs
<AccordionGroup>
<Accordion title="How can I tell how many tokens a string has before I embed it?">
[This cookbook by OpenAI](https://cookbook.openai.com/examples/how_to_count_tokens_with_tiktoken) illustrates how to leverage their Tiktoken library to count tokens for various embedding requests.
</Accordion>
<Accordion title="How can I retrieve K nearest embedding vectors quickly?">
Using a specialized vector database helps here. [Check out this cookbook by OpenAI](https://cookbook.openai.com/examples/vector_databases/readme) for a deep dive.
</Accordion>
<Accordion title="Do V3 embedding models know about recent events?">
The cutoff date for V3 embedding models (`text-embedding-3-large` & `text-embedding-3-small`) is **September 2021** - so they do not know about the most recent events.
</Accordion>
</AccordionGroup>

### Prompt Caching FAQs
<AccordionGroup>
<Accordion title="How is data privacy maintained for caches?">
OpenAI Prompt caches are not shared between organizations. Only members of the same organization can access caches of identical prompts.
</Accordion>
<Accordion title="Does Prompt Caching affect output token generation or the final response of the API?">
Prompt Caching does not influence the generation of output tokens or the final response provided by the API. Regardless of whether caching is used, the output generated will be identical. This is because only the prompt itself is cached, while the actual response is computed anew each time based on the cached prompt.
</Accordion>
<Accordion title="Is there a way to manually clear the cache?">
Manual cache clearing is not currently available. Prompts that have not been encountered recently are automatically cleared from the cache. Typical cache evictions occur after 5-10 minutes of inactivity, though sometimes lasting up to a maximum of one hour during off-peak periods.
</Accordion>
<Accordion title="Will I be expected to pay extra for writing to Prompt Caching?">
No. Caching happens automatically, with no explicit action needed or extra cost paid to use the caching feature.
</Accordion>
<Accordion title="Do cached prompts contribute to TPM rate limits?">
Yes, as caching does not affect rate limits.
</Accordion>
<Accordion title="Is discounting for Prompt Caching available on Scale Tier and the Batch API?">
Discounting for Prompt Caching is not available on the Batch API but is available on Scale Tier. With Scale Tier, any tokens that are spilled over to the shared API will also be eligible for caching.
</Accordion>
<Accordion title="Does Prompt Caching work on Zero Data Retention requests?">
Yes, Prompt Caching is compliant with existing Zero Data Retention policies.
</Accordion>
</AccordionGroup>

### Image Generation FAQs
<AccordionGroup>
<Accordion title="What's the difference between DALL·E 2 and DALL·E 3?">
DALL·E 3 offers higher quality images and enhanced capabilities, but only supports image generation. DALL·E 2 supports all three capabilities: generation, editing, and variations.
</Accordion>
<Accordion title="How long do the generated image URLs last?">
Generated image URLs expire after one hour. Download or process the images before expiration.
</Accordion>
<Accordion title="What are the size requirements for uploading images?">
Images must be square PNG files under 4MB. For editing features, both the image and mask must have identical dimensions.
</Accordion>
<Accordion title="Can I disable DALL·E 3's automatic prompt enhancement?">
While you can't completely disable it, you can add "I NEED to test how the tool works with extremely simple prompts. DO NOT add any detail, just use it AS-IS:" to your prompt.
</Accordion>
<Accordion title="How many images can I generate per request?">
DALL·E 3 supports 1 image per request (use parallel requests for more), while DALL·E 2 supports up to 10 images per request.
</Accordion>
<Accordion title="What image formats are supported?">
The API requires PNG format for all image uploads and manipulations. Generated images can be returned as either a URL or Base64 data.
</Accordion>
<Accordion title="How does image editing (inpainting) work?">
Available only in DALL·E 2, inpainting requires both an original image and a mask. The transparent areas of the mask indicate where the image should be edited, and your prompt should describe the complete new image, not just the edited area.
</Accordion>
</AccordionGroup>

### Speech-to-Text FAQs
<AccordionGroup>
<Accordion title="What audio file formats are supported?">
The API supports mp3, mp4, mpeg, mpga, m4a, wav, and webm formats, with a maximum file size of 25 MB.
</Accordion>
<Accordion title="Can I translate audio to languages other than English?">
No, currently the translation API only supports output in English, regardless of the input language.
</Accordion>
<Accordion title="How do I handle audio files longer than 25 MB?">
You'll need to either compress the audio file or split it into smaller chunks. Tools like PyDub can help split audio files while avoiding mid-sentence breaks.
</Accordion>
<Accordion title="Does the API support all languages equally well?">
While the model was trained on 98 languages, only languages with less than 50% word error rate are officially supported. Other languages may work but with lower accuracy.
</Accordion>
<Accordion title="Can I get timestamps in the transcription?">
Yes, using the `timestamp_granularities` parameter, you can get timestamps at the segment level, word level, or both.
</Accordion>
<Accordion title="How can I improve transcription accuracy for specific terms?">
You can use the prompt parameter to provide context or correct spellings of specific terms, or use post-processing with GPT-4 for more extensive corrections.
</Accordion>
<Accordion title="What's the difference between transcription and translation?">
Transcription provides output in the original language, while translation always converts the audio to English text.
</Accordion>
</AccordionGroup>

### Text-to-Speech FAQs
<AccordionGroup>
<Accordion title="What are the differences between TTS-1 and TTS-1-HD models?">
TTS-1 offers lower latency for real-time applications but may include more static. TTS-1-HD provides higher quality audio but with increased generation time.
</Accordion>
<Accordion title="Which audio formats are supported?">
The API supports multiple formats: MP3 (default), Opus (for streaming), AAC (for mobile), FLAC (lossless), WAV (uncompressed), and PCM (raw 24kHz samples).
</Accordion>
<Accordion title="Can I create or clone custom voices?">
No, the API only supports the six built-in voices (alloy, echo, fable, onyx, nova, and shimmer). Custom voice creation is not available.
</Accordion>
<Accordion title="How well does it support non-English languages?">
While the voices are optimized for English, the API supports multiple languages with varying effectiveness. Performance quality may vary by language.
</Accordion>
<Accordion title="Can I control the emotional tone or style of the speech?">
There's no direct mechanism to control emotional output. While capitalization and grammar might influence the output, results are inconsistent.
</Accordion>
<Accordion title="Is real-time streaming supported?">
Yes, the API supports real-time audio streaming using chunk transfer encoding, allowing audio playback before complete file generation.
</Accordion>
<Accordion title="Do I need to disclose that the audio is AI-generated?">
Yes, OpenAI's usage policies require clear disclosure to end users that they are hearing AI-generated voices, not human ones.
</Accordion>
</AccordionGroup>

---

## Next Steps

<CardGroup cols={2}>
  <Card title="Add Metadata" icon="tags" href="/product/observability/metadata">
    Add metadata to your OpenAI requests
  </Card>
  <Card title="Gateway Configs" icon="gear" href="/product/ai-gateway/configs">
    Add gateway configs to your OpenAI requests
  </Card>
  <Card title="Tracing" icon="chart-line" href="/product/observability/traces">
    Trace your OpenAI requests
  </Card>
  <Card title="Fallbacks" icon="arrow-rotate-left" href="/product/ai-gateway/fallbacks">
    Setup fallback from OpenAI to other providers
  </Card>
</CardGroup>

For complete SDK documentation:

<Card title="SDK Reference" icon="code" href="/api-reference/sdk/list">
  Complete Portkey SDK documentation
</Card>
