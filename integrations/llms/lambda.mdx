---
title: "Lambda Labs"
description: Use Lambda's GPU-powered inference for Llama and open-source models through Portkey.
---

## Quick Start

Get started with Lambda Labs in under 2 minutes:

<CodeGroup>

```python Python (Portkey SDK)
from portkey_ai import Portkey

portkey = Portkey(api_key="PORTKEY_API_KEY")

response = portkey.chat.completions.create(
    model="@lambda/llama3.1-8b-instruct",
    messages=[{"role": "user", "content": "Hello!"}]
)

print(response.choices[0].message.content)
```

```javascript JavaScript (Portkey SDK)
import Portkey from 'portkey-ai';

const portkey = new Portkey({
    apiKey: 'PORTKEY_API_KEY'
});

async function main() {
    const response = await portkey.chat.completions.create({
        model: "@lambda/llama3.1-8b-instruct",
        messages: [{ role: "user", content: "Hello!" }]
});

    console.log(response.choices[0].message.content);
}

main();
```

```python Python (OpenAI SDK)
from openai import OpenAI
from portkey_ai import createHeaders, PORTKEY_GATEWAY_URL

client = OpenAI(
    api_key="x",
    base_url=PORTKEY_GATEWAY_URL,
    default_headers=createHeaders(
        api_key="PORTKEY_API_KEY",
        provider="@lambda"
    )
)

response = client.chat.completions.create(
    model="llama3.1-8b-instruct",
    messages=[{"role": "user", "content": "Hello!"}]
)

print(response.choices[0].message.content)
```

```javascript JavaScript (OpenAI SDK)
import OpenAI from 'openai';
import { PORTKEY_GATEWAY_URL, createHeaders } from 'portkey-ai'

const client = new OpenAI({
    apiKey: 'x',
    baseURL: PORTKEY_GATEWAY_URL,
    defaultHeaders: createHeaders({
        apiKey: "PORTKEY_API_KEY",
        provider: "@lambda"
    })
});

async function main() {
    const response = await client.chat.completions.create({
        model: 'llama3.1-8b-instruct',
        messages: [{ role: 'user', content: 'Hello!' }]
});

    console.log(response.choices[0].message.content);
}

main();
```

```bash cURL
curl https://api.portkey.ai/v1/chat/completions \
     -H "Content-Type: application/json" \
    -H "x-portkey-api-key: $PORTKEY_API_KEY" \
    -H "x-portkey-provider: @lambda" \
     -d '{
       "model": "llama3.1-8b-instruct",
        "messages": [{"role": "user", "content": "Hello!"}]
     }'
```

</CodeGroup>

## Add Provider in Model Catalog

Before making requests, add Lambda Labs to your Model Catalog:

1. Go to [**Model Catalog â†’ Add Provider**](https://app.portkey.ai/model-catalog/providers)
2. Select **Lambda Labs**
3. Enter your [Lambda API key](https://cloud.lambdalabs.com/api-keys)
4. Name your provider (e.g., `lambda`)

<Card title="Complete Setup Guide" icon="book" href="/product/model-catalog">
  See all setup options and detailed configuration instructions
  </Card>

---

## Lambda Capabilities

### Streaming

Stream responses for real-time output:

<CodeGroup>

```python Python
from portkey_ai import Portkey

portkey = Portkey(api_key="PORTKEY_API_KEY", provider="@lambda")

stream = portkey.chat.completions.create(
    model="llama3.1-8b-instruct",
    messages=[{"role": "user", "content": "Tell me a story"}],
    stream=True
)

for chunk in stream:
    print(chunk.choices[0].delta.content or "", end="", flush=True)
```

```javascript Node.js
import Portkey from 'portkey-ai';

const portkey = new Portkey({
    apiKey: 'PORTKEY_API_KEY',
    provider: '@lambda'
});

const stream = await portkey.chat.completions.create({
    model: 'llama3.1-8b-instruct',
    messages: [{ role: 'user', content: 'Tell me a story' }],
    stream: true
});

for await (const chunk of stream) {
    process.stdout.write(chunk.choices[0]?.delta?.content || '');
}
```

</CodeGroup>

### Function Calling

Use Lambda's function calling capabilities:

<CodeGroup>

```python Python
from portkey_ai import Portkey

portkey = Portkey(api_key="PORTKEY_API_KEY", provider="@lambda")

tools = [{
    "type": "function",
    "function": {
        "name": "getWeather",
        "description": "Get the current weather",
        "parameters": {
            "type": "object",
            "properties": {
                "location": {"type": "string", "description": "City and state"},
                "unit": {"type": "string", "enum": ["celsius", "fahrenheit"]}
            },
            "required": ["location"]
        }
    }
}]

response = portkey.chat.completions.create(
    model="llama3.1-8b-instruct",
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "What's the weather in Delhi?"}
    ],
    tools=tools,
    tool_choice="auto"
)

print(response.choices[0].message)
```

```javascript Node.js
import Portkey from 'portkey-ai';

const portkey = new Portkey({
    apiKey: 'PORTKEY_API_KEY',
    provider: '@lambda'
});

const tools = [{
    type: "function",
    function: {
        name: "getWeather",
        description: "Get the current weather",
        parameters: {
            type: "object",
            properties: {
                location: { type: "string", description: "City and state" },
                unit: { type: "string", enum: ["celsius", "fahrenheit"] }
            },
            required: ["location"]
        }
    }
}];

const response = await portkey.chat.completions.create({
    model: "llama3.1-8b-instruct",
    messages: [
        { role: "system", content: "You are a helpful assistant." },
        { role: "user", content: "What's the weather in Delhi?" }
    ],
    tools,
    tool_choice: "auto"
});

console.log(response.choices[0].message);
```

</CodeGroup>

---

## Supported Models

Lambda Labs provides GPU-powered inference for open-source models:

<Accordion title="View All Lambda Models">

- deepseek-coder-v2-lite-instruct
- dracarys2-72b-instruct
- hermes3-405b
- hermes3-405b-fp8-128k
- hermes3-70b
- hermes3-8b
- lfm-40b
- llama3.1-405b-instruct-fp8
- llama3.1-70b-instruct-fp8
- llama3.1-8b-instruct
- llama3.2-3b-instruct
- llama3.1-nemotron-70b-instruct

</Accordion>

## Supported Endpoints and Parameters

| Endpoint | Supported Parameters |
|----------|---------------------|
| `/chat/completions` | messages, max_tokens, temperature, top_p, stream, presence_penalty, frequency_penalty, tools, tool_choice |
| `/completions` | model, prompt, max_tokens, temperature, top_p, n, stream, logprobs, echo, stop, presence_penalty, frequency_penalty, best_of, logit_bias, user, seed, suffix |

Check [Lambda's documentation](https://docs.lambdalabs.com/) for more details.

---

## Next Steps

<CardGroup cols={2}>
  <Card title="Gateway Configs" icon="sliders" href="/product/ai-gateway">
    Add fallbacks, load balancing, and more
  </Card>
  <Card title="Observability" icon="chart-line" href="/product/observability">
    Monitor and trace your Lambda requests
  </Card>
  <Card title="Prompt Library" icon="book" href="/product/prompt-engineering-studio">
    Manage and version your prompts
  </Card>
  <Card title="Metadata" icon="tag" href="/product/observability/metadata">
    Add custom metadata to requests
  </Card>
</CardGroup>

For complete SDK documentation:

<Card title="SDK Reference" icon="code" href="/api-reference/sdk/list">
  Complete Portkey SDK documentation
</Card>
