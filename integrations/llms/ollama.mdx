---
title: "Ollama"
description: "Integrate Ollama-hosted models with Portkey for local LLM deployment with full observability."
---

Portkey provides a robust gateway to facilitate the integration of your **locally hosted models through Ollama**.

<AccordionGroup>
<Accordion title="Local Setup (npm or Docker)">

First, install the Gateway locally:

<CodeGroup>
```bash npx
npx @portkey-ai/gateway
```

```bash Docker
docker run -d -p 8787:8787 portkeyai/gateway:latest
```
</CodeGroup>

Then, connect to your local Ollama instance:

```python Python
from portkey_ai import Portkey

portkey = Portkey(
    base_url="http://localhost:8787",  # Your local Gateway
    provider="ollama",
    custom_host="http://localhost:11434"  # Your Ollama instance
)

response = portkey.chat.completions.create(
    model="llama3",
    messages=[{"role": "user", "content": "Hello!"}]
)
```

</Accordion>
</AccordionGroup>

## Integration Steps

<Steps>
<Step title="Expose your Ollama API">
Expose your Ollama API using a tunneling service like [ngrok](https://ngrok.com/) or make it publicly accessible. Skip this if you're self-hosting the Gateway.

For using Ollama with ngrok, here's a [useful guide](https://github.com/ollama/ollama/blob/main/docs/faq.md#how-can-i-use-ollama-with-ngrok):

```sh
ngrok http 11434 --host-header="localhost:11434"
```
</Step>

<Step title="Add to Model Catalog">

1. Go to [**Model Catalog â†’ Add Provider**](https://app.portkey.ai/model-catalog/providers)
2. Enable **"Local/Privately hosted provider"** toggle
3. Select **Ollama** as the provider type
4. Enter your Ollama URL in **Custom Host**: `https://your-ollama.ngrok-free.app`
5. Name your provider (e.g., `my-ollama`)

<Card title="Complete Setup Guide" icon="book" href="/product/model-catalog">
  See all setup options
</Card>
</Step>

<Step title="Use in Your Application">

<CodeGroup>

```python Python
from portkey_ai import Portkey

portkey = Portkey(
    api_key="PORTKEY_API_KEY",
    provider="@my-ollama"
)

response = portkey.chat.completions.create(
    model="llama3",
    messages=[{"role": "user", "content": "Hello!"}]
)

print(response.choices[0].message.content)
```

```javascript Node.js
import Portkey from 'portkey-ai';

const portkey = new Portkey({
    apiKey: 'PORTKEY_API_KEY',
    provider: '@my-ollama'
});

const response = await portkey.chat.completions.create({
    model: 'llama3',
    messages: [{ role: 'user', content: 'Hello!' }]
});

console.log(response.choices[0].message.content);
```

</CodeGroup>

**Or use custom host directly:**

<CodeGroup>

```python Python
from portkey_ai import Portkey

portkey = Portkey(
    api_key="PORTKEY_API_KEY",
    provider="ollama",
    custom_host="https://your-ollama.ngrok-free.app"
)
```

```javascript Node.js
import Portkey from 'portkey-ai';

const portkey = new Portkey({
    apiKey: 'PORTKEY_API_KEY',
    provider: 'ollama',
    customHost: 'https://your-ollama.ngrok-free.app'
});
```

</CodeGroup>

</Step>
</Steps>

<Note>
**Important:** For Ollama integration, you only need to pass the base URL to `customHost` **without** the version identifier (such as `/v1`) - Portkey handles the rest!
</Note>

---

## Supported Models

Ollama supports a wide range of models including:

- Llama 3, Llama 3.1, Llama 3.2
- Mistral, Mixtral
- Gemma, Gemma 2
- Phi-3
- Qwen 2
- And many more!

Check [Ollama's model library](https://ollama.com/library) for the complete list.

---

## Next Steps

<CardGroup cols={2}>
  <Card title="Gateway Configs" icon="sliders" href="/product/ai-gateway">
    Add retries, timeouts, and fallbacks
  </Card>
  <Card title="Observability" icon="chart-line" href="/product/observability">
    Monitor your Ollama requests
  </Card>
  <Card title="Custom Host Guide" icon="server" href="/product/ai-gateway/universal-api#integrating-local-or-private-models">
    Learn more about custom host setup
  </Card>
  <Card title="BYOLLM Guide" icon="book" href="/integrations/llms/byollm">
    Complete guide for private LLMs
  </Card>
</CardGroup>

For complete SDK documentation:

<Card title="SDK Reference" icon="code" href="/api-reference/sdk/list">
  Complete Portkey SDK documentation
</Card>
