---
title: "Anthropic"
---

Portkey provides a robust and secure gateway to facilitate the integration of various Large Language Models (LLMs) into your applications, including [Anthropic's Claude APIs](https://docs.anthropic.com/claude/reference/getting-started-with-the-api).

With Portkey, you can take advantage of features like fast AI gateway access, observability, prompt management, and more, all while ensuring the secure management of your LLM API keys through a [virtual key](/product/ai-gateway/virtual-keys) system.
<Note>Provider Slug. `anthropic`</Note>


## Portkey SDK Integration with Anthropic

Portkey provides a consistent API to interact with models from various providers. To integrate Anthropic with Portkey:

### 1\. Install the Portkey SDK

Add the Portkey SDK to your application to interact with Anthropic's API through Portkey's gateway.

<Tabs>
    <Tab title="NodeJS">
        ```sh
        npm install --save portkey-ai
        ```
    </Tab>
    <Tab title="Python">
        ```sh
        pip install portkey-ai
        ```
    </Tab>
  </Tabs>




### 2\. Initialize Portkey with the Virtual Key

To use Anthropic with Portkey, [get your Anthropic API key from here](https://console.anthropic.com/settings/keys), then add it to Portkey to create your Anthropic virtual key.

<Tabs>
    <Tab title="NodeJS SDK">

```js
import Portkey from 'portkey-ai'

const portkey = new Portkey({
    apiKey: "PORTKEY_API_KEY", // defaults to process.env["PORTKEY_API_KEY"]
    virtualKey: "VIRTUAL_KEY" // Your Anthropic Virtual Key
})
```

    </Tab>
    <Tab title="Python SDK">

```python
from portkey_ai import Portkey

portkey = Portkey(
    api_key="PORTKEY_API_KEY",  # Replace with your Portkey API key
    virtual_key="VIRTUAL_KEY"   # Replace with your virtual key for Anthropic
)
```
    </Tab>
  <Tab title="OpenAI Python SDK">

```python
from openai import OpenAI

from portkey_ai import PORTKEY_GATEWAY_URL, createHeaders

client = OpenAI(
    api_key="ANTHROPIC_API_KEY",
    base_url=PORTKEY_GATEWAY_URL,
    default_headers=createHeaders(
        api_key="PORTKEY_API_KEY",
        provider="anthropic"
    )
)
```

    </Tab>
    <Tab title="OpenAI Node SDK">

```js
import OpenAI from "openai";

import { PORTKEY_GATEWAY_URL, createHeaders } from "portkey-ai";

const client = new OpenAI({
  apiKey: "ANTHROPIC_API_KEY",
  baseURL: PORTKEY_GATEWAY_URL,
  defaultHeaders: createHeaders({
    provider: "anthropic",
    apiKey: "PORTKEY_API_KEY",
  }),
});
```
    </Tab>
  </Tabs>



### 3\. Invoke Chat Completions with Anthropic

Use the Portkey instance to send requests to Anthropic. You can also override the virtual key directly in the API call if needed.


<Tabs>
    <Tab title="NodeJS SDK">

```js
const chatCompletion = await portkey.chat.completions.create({
    messages: [{ role: 'user', content: 'Say this is a test' }],
    model: 'claude-3-opus-20240229',
    max_tokens: 250 // Required field for Anthropic
});

console.log(chatCompletion.choices[0].message.content);
```

    </Tab>
    <Tab title="Python SDK">

```python
chat_completion = portkey.chat.completions.create(
    messages= [{ "role": 'user', "content": 'Say this is a test' }],
    model= 'claude-3-opus-20240229',
    max_tokens=250 # Required field for Anthropic
)

print(chat_completion.choices[0].message.content)
```

    </Tab>
  <Tab title="OpenAI Python SDK">

```python
chat_completion = client.chat.completions.create(
    messages = [{ "role": 'user', "content": 'Say this is a test' }],
    model = 'claude-3-opus-20240229',
    max_tokens = 250
)

print(chat_completion.choices[0].message.content)
```

    </Tab>
    <Tab title="OpenAI Node SDK">

```js
async function main() {
    const chatCompletion = await client.chat.completions.create({
        model: "claude-3-opus-20240229",
        max_tokens: 1024,
        messages: [{ role: "user", content: "Hello, Claude" }],
    });

    console.log(chatCompletion.choices[0].message.content);

}

main();
```
    </Tab>
  </Tabs>




## How to Use Anthropic System Prompt

With Portkey, we make Anthropic models interoperable with the OpenAI schema and SDK methods. So, instead of passing the `system` prompt separately, you can pass it as part of the `messages` body, similar to OpenAI:

<Tabs>
    <Tab title="NodeJS">

```js
const chatCompletion = await portkey.chat.completions.create({
    messages: [
        { role: 'system', content: 'Your system prompt' },
        { role: 'user', content: 'Say this is a test' }
    ],
    model: 'claude-3-opus-20240229',
    max_tokens: 250
});

console.log(chatCompletion.choices);
```

    </Tab>
    <Tab title="Python">

```python
completion = portkey.chat.completions.create(
    messages= [
        { "role": 'system', "content": 'Your system prompt' },
        { "role": 'user', "content": 'Say this is a test' }
    ],
    model= 'claude-3-opus-20240229',
    max_tokens=250 # Required field for Anthropic
)

print(completion.choices)
```

    </Tab>

  </Tabs>



## Vision Chat Completion Usage


Portkey's multimodal Gateway fully supports Anthropic's vision models `claude-3-sonnet`, `claude-3-haiku`,  `claude-3-opus`, and the latest `claude-3.5-sonnet`.
Portkey follows the OpenAI schema, which means you can send your image data to Anthropic in the same format as OpenAI.


<Note>
   - Anthropic ONLY accepts `base64` -encoded images. Unlike OpenAI, it **does not** support  `image URLs`.
- With Portkey, you can use the same format to send base64-encoded images to both Anthropic and OpenAI models.


</Note>

Here's an example using Anthropic `claude-3.5-sonnet` model

<Tabs>
  <Tab title="Python">
    ```python
    import base64
    import httpx
    from portkey_ai import Portkey

    # Fetch and encode the image
    image_url = "https://upload.wikimedia.org/wikipedia/commons/a/a7/Camponotus_flavomarginatus_ant.jpg"
    image_data = base64.b64encode(httpx.get(image_url).content).decode("utf-8")

    # Initialize the Portkey client
    portkey = Portkey(
        api_key="PORTKEY_API_KEY",  # Replace with your Portkey API key
        virtual_key="VIRTUAL_KEY"   # Add your provider's virtual key
    )

    # Create the request
    response = portkey.chat.completions.create(
        model="claude-3-5-sonnet-20240620",
        messages=[
            {
                "role": "system",
                "content": "You are a helpful assistant, who describes imagse"
            },
            {
                "role": "user",
                "content": [
                    {
                        "type": "image_url",
                        "image_url": {
                            "url": f"data:image/jpeg;base64,{image_data}"
                        }
                    }
                ]
            }
        ],
        max_tokens=1400,
    )
    print(response)
    ```
  </Tab>
  <Tab title="NodeJS">
    ```javascript
    import Portkey from 'portkey-ai';

    // Initialize the Portkey client
    const portkey = new Portkey({
      apiKey: "PORTKEY_API_KEY", // Replace with your Portkey API key
      virtualKey: "VIRTUAL_KEY" // Add your anthropic's virtual key
    });

    // Generate a chat completion
    async function getChatCompletionFunctions() {
        const response = await portkey.chat.completions.create({
          model: "claude-3-5-sonnet-20240620",
          messages: [
            {
              role: "system",
              content: "You are a helpful assistant who describes images."
            },
            {
              role: "user",
              content: [
                { type: "text", text: "What's in this image?" },
                {
                  type: "image_url",
                  image_url: {
                    url: "data:image/jpeg;base64,BASE64_IMAGE_DATA"
                  }
                }
              ]
            }
          ],
          max_tokens: 300
        });
        console.log(response);
      }
    // Call the function
    getChatCompletionFunctions();
    ```
  </Tab>
  <Tab title="OpenAI NodeJS">
    ```javascript
    import OpenAI from 'openai'; // We're using the v4 SDK
    import { PORTKEY_GATEWAY_URL, createHeaders } from 'portkey-ai'

    const openai = new OpenAI({
      apiKey: 'ANTHROPIC_API_KEY', // defaults to process.env["OPENAI_API_KEY"],
      baseURL: PORTKEY_GATEWAY_URL,
      defaultHeaders: createHeaders({
        provider: "openai",
        apiKey: "PORTKEY_API_KEY" // defaults to process.env["PORTKEY_API_KEY"]
      })
    });

    // Generate a chat completion with streaming
    async function getChatCompletionFunctions(){
      const response = await openai.chat.completions.create({
        model: "claude-3-5-sonnet-20240620",
        messages: [
          {
            role: "user",
            content: [
              { type: "text", text: "What's in this image?" },
              {
                type: "image_url",
                image_url:
                  "https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg",
              },
            ],
          },
        ],
      });

      console.log(response)

    }
    await getChatCompletionFunctions();
    ```
  </Tab>
  <Tab title="OpenAI Python">
    ```python
    from openai import OpenAI
    from portkey_ai import PORTKEY_GATEWAY_URL, createHeaders

    openai = OpenAI(
        api_key='Anthropic_API_KEY',
        base_url=PORTKEY_GATEWAY_URL,
        default_headers=createHeaders(
            provider="anthropic",
            api_key="PORTKEY_API_KEY"
        )
    )


    response = openai.chat.completions.create(
       model="claude-3-5-sonnet-20240620",
        messages=[
            {
                "role": "system",
                "content": "You are a helpful assistant, who describes imagse"
            },
            {
                "role": "user",
                "content": [
                    {
                        "type": "image_url",
                        "image_url": {
                            "url": f"data:image/jpeg;base64,{base_64_encoded_image}"
                        }
                    }
                ]
            }
        ],
        max_tokens=1400,
    )

    print(response)
    ```
  </Tab>
  <Tab title="cURL">
    ```sh
    curl "https://api.portkey.ai/v1/chat/completions" \
      -H "Content-Type: application/json" \
      -H "x-portkey-api-key: $PORTKEY_API_KEY" \
      -H "x-portkey-provider: anthropic" \
      -H "x-api-key: $ANTHROPIC_API_KEY" \
      -d '{
        "model": "claude-3-5-sonnet-20240620",
        "messages": [
          {
            "role": "system",
            "content": "You are a helpful assistant who describes images."
          },
          {
            "role": "user",
            "content": [
              {
                "type": "text",
                "text": "What's in this image?"
              },
              {
                "type": "image_url",
                "image_url": {
                  "url": "data:image/jpeg;base64,BASE64_IMAGE_DATA"
                }
              }
            ]
          }
        ],
        "max_tokens": 300
      }'
    ```
  </Tab>
</Tabs>

<Note>
  To prompt with pdfs, simply update the "url" field inside the "image_url" object to this pattern: `data:application/pdf;base64,BASE64_PDF_DATA`
</Note>

## Thinking mode [BETA]

Portkey supports Anthropic's thinking mode, both single turn and multi-turn conversations.

you can send the thinking mode parameters in the request body, the way you would normally for anthropic

```json
{
    "model": "claude-3-7-sonnet-latest",
    "max_tokens": 3000,
    "thinking": {
        "type": "enabled",
        "budget_tokens": 2030
    },
    "stream": false,
    "messages": [
        {
            "role": "user",
            "content": [
                {
                    "type": "text",
                    "text": "when does the flight from baroda to bangalore land tomorrow, what time, what is its flight number, and what is its baggage belt?"
                }
            ]
        },
        {
            "role": "assistant",
            "content": [
                {
                    "type": "thinking",
                    "thinking": "This question asks for specific flight information:\n1. Arrival time for a flight from Baroda (Vadodara) to Bangalore tomorrow\n2. Flight number \n3. Baggage belt number\n\nThe problem is that I don't have access to real-time flight information, airport systems, or tomorrow's specific flight schedules. This is dynamic information that requires:\n- Access to current airline schedules\n- Access to airport systems for baggage belt assignments\n- Knowledge of the specific date \"tomorrow\" refers to\n\nI need to explain that I can't provide this specific information and suggest alternatives for the person to get this information.",
                    "signature": "EuYBCkQYAiJAPS3kSezRtnL0OHmJ7vvqBwMFiG5NHO6tjAjIilTNj9dbqHEKz86lBFc2zAztHcqiwfyGKHLfsh0LwBKoqe7vDBIMAukH7O6zLPWucVERGgzMAztWdzT+uAtvBQ4iMAhq+AwA2IuVB0ovsj03PI9c3WEqXBC+SvhIyw+xEzUcqhjKkZogzRJrO4OKyaOBOCpQ7gS3oh8VNr6kBkDneXTuGnxyeaJNdHkzWwCm3cBKjxD0Hu5wK3YQwFc4fG9Zultu7KD1KOjzc3PAdJ4MwH5MKBZ21THX96DcfuzZCktGOpc="
                },
                {
                    "type": "text",
                    "text": "I don't have access to real-time flight information or schedules. To get accurate details about:\n\n- The arrival time of a flight from Baroda (Vadodara) to Bangalore tomorrow\n- The flight number\n- The baggage belt assignment\n\nYou would need to:\n\n1. Check the airline's official website or app\n2. Call the airline's customer service\n3. Use flight tracking websites/apps like FlightRadar24 or Skyscanner\n4. Contact the airports directly\n5. Check your booking confirmation if you've already purchased a ticket\n\nBaggage belt assignments are typically displayed on airport information screens upon arrival, as they can change based on airport operations."
                }
            ]
        },
        {
            "role": "user",
            "content": "thanks that's good to know, can you book me a ticket on hogwarts express?"
        }
    ]
}
```

since there is no feasible mapping between anthropic thinking mode responses and openai response signature, you would need to use the
`x-portkey-include-raw-response: true` header alongwith your request and parse the thinking mode response from the raw response.

Here's a [guide](/product/ai-gateway/strict-open-ai-compliance#raw-response) on how to pass the `x-portkey-include-raw-response: true` header

#### [API Reference](#vision-chat-completion-usage)

On completion, the request will get logged in Portkey where any image inputs or outputs can be viewed. Portkey will automatically render the base64 images to help you debug any issues quickly.

**For more info, check out this guide:**

<Card title="Vision" href="/product/ai-gateway/multimodal-capabilities/vision">
  Learn more about Vision capabilities
</Card>

## Prompt Caching

Portkey also works with Anthropic's new prompt caching feature and helps you save time & money for all your Anthropic requests. Refer to this guide to learn how to enable it:

<Card title="Prompt Caching" href="/integrations/llms/anthropic/prompt-caching">
</Card>



## Managing Anthropic Prompts

You can manage all prompts to Anthropic in the [Prompt Library](/product/prompt-library). All the current models of Anthropic are supported and you can easily start testing different prompts.

Once you're ready with your prompt, you can use the `portkey.prompts.completions.create` interface to use the prompt in your application.

## Next Steps

The complete list of features supported in the SDK are available on the link below.

<Card title="SDK" href="/api-reference/portkey-sdk-client">
</Card>


You'll find more information in the relevant sections:

1. [Add metadata to your requests](/product/observability/metadata)
2. [Add gateway configs to your Anthropic requests](/product/ai-gateway/configs)
3. [Tracing Anthropic requests](/product/observability/traces)
4. [Setup a fallback from OpenAI to Anthropic's Claude APIs](/product/ai-gateway/fallbacks)
