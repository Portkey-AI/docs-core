---
title: "Azure OpenAI"
description: "Azure OpenAI is a great alternative to accessing the best models including GPT-4 and more in your private environments. Portkey provides complete support for Azure OpenAI."
---

With Portkey, you can take advantage of features like fast AI gateway access, observability, prompt management, and more, all while ensuring the secure management of your LLM API keys through a [integration](/product/modelc-catalog) system.
<Note>
Provider Slug. `azure-openai`
</Note>
## Portkey SDK Integration with Azure OpenAI

Portkey provides a consistent API to interact with models from various providers. To integrate Azure OpenAI with Portkey:









# Creating Your Azure OpenAI Integration

<Note>
This integration is for all OpenAI models deployed on either Azure OpenAI or Azure AI Foundry.
</Note>

Integrate Azure OpenAI models with Portkey to centrally manage your AI models and deployments. This guide walks you through setting up the integration using API key authentication.

## Prerequisites

Before creating your integration, you'll need:
- An active [Azure account](https://ai.azure.com)
- Access to your Azure portal
- A model deployment on Azure (e.g., GPT-4, GPT-4o-mini)

## Step 1: Start Creating Your Integration

Navigate to the Integrations page in your Portkey dashboard and select **Azure OpenAI** as your provider.

<Frame>
  <img src="/images/product/model-catalog/integrations-page.png" alt="Creating Azure OpenAI Integration" />
</Frame>


## Step 2: Configure Integration Details

Fill in the basic information for your integration:

- **Name**: A descriptive name for this integration (e.g., "Azure OpenAI Production")
- **Short Description**: Optional context about this integration's purpose
- **Slug**: A unique identifier used in API calls (e.g., "azure-openai-prod")


## Step 3: Set Up Authentication

Portkey supports three authentication methods for Azure OpenAI. For most use cases, we recommend using the **Default (API Key)** method.


<Frame>
  <img src="/images/llms/azure/azure-openai-2.png" alt="Complete Integration Form" />
</Frame>



### Gather Your Azure Credentials

From your Azure portal, you'll need to collect:

<Frame>
  <img src="/images/llms/azure/azure-1.2.png" alt="Azure Portal Overview" />
</Frame>

### Enter Credentials in Portkey

1. Navigate to your model deployment in Azure
2. Click on the deployment to view details
3. Copy the **API Key** from the authentication section

<Note>
We recommend importing your Azure details (resource name, deployment details, API version) directly from your Target URI. Simply copy the target URL and import it.

<Frame>
  <img src="/images/llms/azure/azure-openai-1.png" alt="Import from Target URI" />
</Frame>

</Note>

4. **Azure Resource Name**: Get Your resource Name from Azure


<Accordion title="Find Your Azure Resource Name">

Your  Azure resource Name is different from your Project Name. Here's how you can find it:

    <Frame>
      <img src="/images/llms/azure/azure-openai-6.png" alt="Azure Resource Name" />
    </Frame>

    <Tabs>
    <Tab title="Azure AI Foundry">
    <Frame>
      <img src="/images/llms/azure/azure-openai-4.png" alt="Azure AI Foundry Configuration" />
    </Frame>
    </Tab>
    <Tab title="Azure OpenAI">
    <Frame>
      <img src="/images/llms/azure/azure-openai-5.png" alt="Azure OpenAI Configuration" />
    </Frame>
    </Tab>
    </Tabs>

</Accordion>

4. Note the **API Version** and enter it in the given field
5. **Alias Name**: A Portkey-specific field for accessing the model - name it as you prefer
6. **Foundation Model**: Select a foundation model from the list that matches your deployment. This helps Portkey track costs and metrics. If your model isn't listed, choose a similar model type to begin with.


## Adding Multiple Models to Your Azure OpenAI Integration

You can deploy multiple models through a single Azure OpenAI integration by adding multiple deployments under the same integration.

<Frame>
  <img src="/images/llms/azure/azure-openai-1.png" alt="Add Multiple Models" />
</Frame>

Follow the same steps as above for each additional model deployment.



### 1\. Install the Portkey SDK

Add the Portkey SDK to your application to interact with Azure OpenAI's API through Portkey's gateway.

<Tabs>
  <Tab title="NodeJS">

```sh
npm install --save portkey-ai
```
  </Tab>
  <Tab title="Python">

```sh
pip install portkey-ai
```
  </Tab>
</Tabs>




### 2\. Initialize Portkey with the Azure

Set up Portkey with your Azure Integration as part of the initialization configuration. You can create a [provider](/product/model-catalog) for Azure in the Portkey UI.

<Tabs>
  <Tab title="NodeJS SDK">
    ```js
    import Portkey from 'portkey-ai'

    const portkey = new Portkey({
        apiKey: "PORTKEY_API_KEY", // defaults to process.env["PORTKEY_API_KEY"]
        provider:"@AZURE_PROVIDER" // Your Azure Provider Slug
    })
    ```
  </Tab>
  <Tab title="Python SDK">
```python
from portkey_ai import Portkey

portkey = Portkey(
    api_key="PORTKEY_API_KEY",  # Replace with your Portkey API key
    provider="@AZURE_PROVIDER"   # Replace with your Provider slug for Azure
)
```
  </Tab>
</Tabs>

### **3\. Invoke Chat Completions with Azure OpenAI**

Use the Portkey instance to send requests to your Azure deployments. You can also override the provider slug directly in the API call if needed.

<Tabs>
  <Tab title="NodeJS SDK">
    ```js
    const chatCompletion = await portkey.chat.completions.create({
        messages: [{ role: 'user', content: 'Say this is a test' }],
        model: 'gpt4', // This would be your deployment or model name
    });

    console.log(chatCompletion.choices);
    ```

  </Tab>
  <Tab title="Python SDK">
```python
completion = portkey.chat.completions.create(
    messages= [{ "role": 'user', "content": 'Say this is a test' }],
    model= 'custom_model_name'
)

print(completion.choices)
```
  </Tab>

</Tabs>



## Managing Azure OpenAI Prompts

You can manage all prompts to Azure OpenAI in the [Prompt Library](/product/prompt-library). All the current models of OpenAI are supported and you can easily start testing different prompts.

Once you're ready with your prompt, you can use the `portkey.prompts.completions.create` interface to use the prompt in your application.

## Image Generation

Portkey supports multiple modalities for Azure OpenAI and you can make image generation requests through Portkey's AI Gateway the same way as making completion calls.

<Tabs>
  <Tab title="Portkey NodeJS">
```js
import Portkey from 'portkey-ai'

const portkey = new Portkey({
    apiKey: "PORTKEY_API_KEY",
    provider:"@DALL-E_PROVIDER" // Referencing a Dall-E Azure deployment with Provider Slug
})

const image = await portkey.images.generate({
  prompt:"Lucy in the sky with diamonds",
  size:"1024x1024"
})
```
  </Tab>
  <Tab title="Portkey Python">
```python
from portkey_ai import Portkey

portkey = Portkey(
    api_key="PORTKEY_API_KEY",
    provider="@DALL-E_PROVIDER"   # Referencing a Dall-E Azure deployment with Provider Slug
)

image = portkey.images.generate(
  prompt="Lucy in the sky with diamonds",
  size="1024x1024"
)
```
  </Tab>
</Tabs>


Portkey's fast AI gateway captures the information about the request on your Portkey Dashboard. On your logs screen, you'd be able to see this request with the request and response.

<Frame>
  <img src="/images/llms/api.png" alt="api" />
  </Frame>

Log view for an image generation request on Azure OpenAI

More information on image generation is available in the [API Reference](https://portkey.ai/docs/api-reference/completions-1#create-image).

---

## Making Requests Without Model Catalog

Here's how you can pass your Azure OpenAI details & secrets directly without using the Model Catalog feature.

### Key Mapping

In a typical Azure OpenAI request,



```sh
curl https://{YOUR_RESOURCE_NAME}.openai.azure.com/openai/deployments/{YOUR_DEPLOYMENT_NAME}/chat/completions?api-version={API_VERSION} \
  -H "Content-Type: application/json" \
  -H "api-key: {YOUR_API_KEY}" \
  -d '{
    "model": "gpt-4o",
    "messages": [
      {
        "role": "system",
        "content": "You are a helpful assistant"
      },
      {
        "role": "user",
        "content": "what is a portkey?"
      }
    ]
}'
```

| Parameter             | Node SDK                             | Python SDK                            | REST Headers                  |
| --------------------- | ------------------------------------ | ------------------------------------- | ----------------------------- |
| AZURE RESOURCE NAME   | azureResourceName                    | azure_resource_name                 | x-portkey-azure-resource-name |
| AZURE DEPLOYMENT NAME | azureDeploymentId                    | azure_deployment_id                 | x-portkey-azure-deployment-id |
| API VERSION           | azureApiVersion                      | azure_api_version                   | x-portkey-azure-api-version   |
| AZURE API KEY         | Authorization: "Bearer + {API_KEY}" | Authorization = "Bearer + {API_KEY}" | Authorization                 |
| AZURE MODEL NAME      | azureModelName                       | azure_model_name                    | x-portkey-azure-model-name    |

### Example

<Tabs>
  <Tab title="Node">
    ```js
    import Portkey from 'portkey-ai'

    const portkey = new Portkey({
        apiKey: "PORTKEY_API_KEY",
        provider: "azure-openai",
        azureResourceName: "AZURE_RESOURCE_NAME",
        azureDeploymentId: "AZURE_DEPLOYMENT_NAME",
        azureApiVersion: "AZURE_API_VERSION",
        azureModelName: "AZURE_MODEL_NAME"
        Authorization: "Bearer API_KEY"
    })
    ```
  </Tab>
  <Tab title="Python">

```python
from portkey_ai import Portkey

portkey = Portkey(
    api_key = "PORTKEY_API_KEY",
    provider = "azure-openai",
    azure_resource_name = "AZURE_RESOURCE_NAME",
    azure_deployment_id = "AZURE_DEPLOYMENT_NAME",
    azure_api_version = "AZURE_API_VERSION",
    azure_model_name = "AZURE_MODEL_NAME",
    Authorization = "Bearer API_KEY"
)
```
  </Tab>
<Tab title="cURL">
  ```sh
  curl https://api.portkey.ai/v1/chat/completions \
    -H "Content-Type: application/json" \
    -H "Authorization: Bearer $AZURE_OPENAI_API_KEY" \
    -H "x-portkey-api-key: $PORTKEY_API_KEY" \
    -H "x-portkey-provider: azure-openai" \
    -H "x-portkey-azure-resource-name: $AZURE_RESOURCE_NAME" \
    -H "x-portkey-azure-deployment-id: $AZURE_DEPLOYMENY_ID" \
    -H "x-portkey-azure-model-name: $AZURE_MODEL_NAME" \
    -H "x-portkey-azure-api-version: $AZURE_API_VERSION" \
    -d '{
      "model": "gpt-4o",
      "messages": [{"role": "user","content": "Hello!"}]
    }'
  ```
  </Tab>
</Tabs>


### How to Pass JWT (JSON Web Tokens)

If you have configured fine-grained access for Azure OpenAI and need to use `JSON web token (JWT)` in the `Authorization` header instead of the regular `API Key`, you can use the `forwardHeaders` parameter to do this.

<Tabs>
  <Tab title="Node">
    ```js
    import Portkey from 'portkey-ai'

    const portkey = new Portkey({
        apiKey: "PORTKEY_API_KEY",
        provider: "azure-openai",
        azureResourceName: "AZURE_RESOURCE_NAME",
        azureDeploymendId: "AZURE_DEPLOYMENT_NAME",
        azureApiVersion: "AZURE_API_VERSION",
        azureModelName: "AZURE_MODEL_NAME",
        Authorization: "Bearer JWT_KEY", // Pass your JWT here
        forwardHeaders: [ "Authorization" ]
    })
    ```
  </Tab>
  <Tab title="Python">

```js
import Portkey from 'portkey-ai'

const portkey = new Portkey({
    api_key = "PORTKEY_API_KEY",
    provider = "azure-openai",
    azure_resource_name = "AZURE_RESOURCE_NAME",
    azure_deploymend_id = "AZURE_DEPLOYMENT_NAME",
    azure_api_version = "AZURE_API_VERSION",
    azure_model_name = "AZURE_MODEL_NAME",
    Authorization = "Bearer API_KEY", # Pass your JWT here
    forward_headers= [ "Authorization" ]
)
```
  </Tab>
</Tabs>

For further questions on custom Azure deployments or fine-grained access tokens, reach out to us on support@portkey.ai

## Next Steps

The complete list of features supported in the SDK are available on the link below.

<Card title="SDK" href="/api-reference/portkey-sdk-client">
</Card>


You'll find more information in the relevant sections:

1. [Add metadata to your requests](/product/observability/metadata)
2. [Add gateway configs to your Azure OpenAI requests](/product/ai-gateway/configs)
3. [Tracing Azure OpenAI requests](/product/observability/traces)
4. [Setup a fallback from OpenAI to Azure OpenAI APIs](/product/ai-gateway/fallbacks)
