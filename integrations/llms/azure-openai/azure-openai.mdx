---
title: "Azure OpenAI"
description: "Azure OpenAI is a great alternative to accessing the best models including GPT-4 and more in your private environments. Portkey provides complete support for Azure OpenAI."
---

With Portkey, you can take advantage of features like fast AI gateway access, observability, prompt management, and more, all while ensuring the secure management of your LLM API keys through a [virtual key](/product/ai-gateway/virtual-keys) system.
<Note>
Provider Slug. `azure-openai`
</Note>
## Portkey SDK Integration with Azure OpenAI

Portkey provides a consistent API to interact with models from various providers. To integrate Azure OpenAI with Portkey:

### First, add your Azure details to Portkey's Virtual Keys
<Frame>
<img src="/images/llms/azureopenai.png" alt="azure" />
</Frame>


**Here's a step-by-step guide:**

1. Request access to Azure OpenAI [here](https://aka.ms/oai/access).
2. Create a resource in the Azure portal [here](https://portal.azure.com/?microsoft%5Fazure%5Fmarketplace%5FItemHideKey=microsoft%5Fopenai%5Ftip#create/Microsoft.CognitiveServicesOpenAI). (This will be your **Resource Name**)
3. Deploy a model in Azure OpenAI Studio [here](https://oai.azure.com/). (This will be your **Deployment Name)**
4. Select your `Foundation Model` from the dropdowon on the modal.
5. Now, on Azure OpenAI studio, go to any playground (chat or completions), click on a UI element called "View code". Note down the API version & API key from here. (This will be your **Azure API Version** & **Azure API Key**)

When you input these details, the foundation model will be auto populated. More details in [this guide](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/create-resource?pivots=web-portal).

<Note>
If you do not want to add your Azure details to Portkey vault, you can also directly pass them while instantiating the Portkey client. [More on that here.](/integrations/llms/azure-openai#making-requests-without-virtual-keys)
</Note>
**Now, let's make a request using this virtual key!**

### 1\. Install the Portkey SDK

Add the Portkey SDK to your application to interact with Azure OpenAI's API through Portkey's gateway.

<Tabs>
  <Tab title="NodeJS">

```sh
npm install --save portkey-ai
```
  </Tab>
  <Tab title="Python">

```sh
pip install portkey-ai
```
  </Tab>
</Tabs>




### 2\. Initialize Portkey with the Virtual Key

Set up Portkey with your virtual key as part of the initialization configuration. You can create a [virtual key](/product/ai-gateway/virtual-keys) for Azure in the Portkey UI.

<Tabs>
  <Tab title="NodeJS SDK">
    ```js
    import Portkey from 'portkey-ai'

    const portkey = new Portkey({
        apiKey: "PORTKEY_API_KEY", // defaults to process.env["PORTKEY_API_KEY"]
        virtualKey: "AZURE_VIRTUAL_KEY" // Your Azure Virtual Key
    })
    ```
  </Tab>
  <Tab title="Python SDK">
```python
from portkey_ai import Portkey

portkey = Portkey(
    api_key="PORTKEY_API_KEY",  # Replace with your Portkey API key
    virtual_key="AZURE_VIRTUAL_KEY"   # Replace with your virtual key for Azure
)
```
  </Tab>
</Tabs>

### **3\. Invoke Chat Completions with Azure OpenAI**

Use the Portkey instance to send requests to your Azure deployments. You can also override the virtual key directly in the API call if needed.

<Tabs>
  <Tab title="NodeJS SDK">
    ```js
    const chatCompletion = await portkey.chat.completions.create({
        messages: [{ role: 'user', content: 'Say this is a test' }],
        model: 'gpt4', // This would be your deployment or model name
    });

    console.log(chatCompletion.choices);
    ```

  </Tab>
  <Tab title="Python SDK">
```python
completion = portkey.chat.completions.create(
    messages= [{ "role": 'user', "content": 'Say this is a test' }],
    model= 'custom_model_name'
)

print(completion.choices)
```
  </Tab>

</Tabs>



## Managing Azure OpenAI Prompts

You can manage all prompts to Azure OpenAI in the [Prompt Library](/product/prompt-library). All the current models of OpenAI are supported and you can easily start testing different prompts.

Once you're ready with your prompt, you can use the `portkey.prompts.completions.create` interface to use the prompt in your application.

## Image Generation

Portkey supports multiple modalities for Azure OpenAI and you can make image generation requests through Portkey's AI Gateway the same way as making completion calls.

<Tabs>
  <Tab title="Portkey NodeJS">
```js
import Portkey from 'portkey-ai'

const portkey = new Portkey({
    apiKey: "PORTKEY_API_KEY",
    virtualKey: "DALL-E_VIRTUAL_KEY" // Referencing a Dall-E Azure deployment with Virtual Key
})

const image = await portkey.images.generate({
  prompt:"Lucy in the sky with diamonds",
  size:"1024x1024"
})
```
  </Tab>
  <Tab title="Portkey Python">
```python
from portkey_ai import Portkey

portkey = Portkey(
    api_key="PORTKEY_API_KEY",
    virtual_key="DALL-E_VIRTUAL_KEY"   # Referencing a Dall-E Azure deployment with Virtual Key
)

image = portkey.images.generate(
  prompt="Lucy in the sky with diamonds",
  size="1024x1024"
)
```
  </Tab>
</Tabs>


Portkey's fast AI gateway captures the information about the request on your Portkey Dashboard. On your logs screen, you'd be able to see this request with the request and response.

<Frame>
  <img src="/images/llms/api.png" alt="api" />
  </Frame>

Log view for an image generation request on Azure OpenAI

More information on image generation is available in the [API Reference](https://portkey.ai/docs/api-reference/completions-1#create-image).

---

## Making Requests Without Virtual Keys

Here's how you can pass your Azure OpenAI details & secrets directly without using the Virutal Keys feature.

### Key Mapping

In a typical Azure OpenAI request,



```sh
curl https://{YOUR_RESOURCE_NAME}.openai.azure.com/openai/deployments/{YOUR_DEPLOYMENT_NAME}/chat/completions?api-version={API_VERSION} \
  -H "Content-Type: application/json" \
  -H "api-key: {YOUR_API_KEY}" \
  -d '{
    "model": "gpt-4o",
    "messages": [
      {
        "role": "system",
        "content": "You are a helpful assistant"
      },
      {
        "role": "user",
        "content": "what is a portkey?"
      }
    ]
}'
```

| Parameter             | Node SDK                             | Python SDK                            | REST Headers                  |
| --------------------- | ------------------------------------ | ------------------------------------- | ----------------------------- |
| AZURE RESOURCE NAME   | azureResourceName                    | azure_resource_name                 | x-portkey-azure-resource-name |
| AZURE DEPLOYMENT NAME | azureDeploymentId                    | azure_deployment_id                 | x-portkey-azure-deployment-id |
| API VERSION           | azureApiVersion                      | azure_api_version                   | x-portkey-azure-api-version   |
| AZURE API KEY         | Authorization: "Bearer + {API_KEY}" | Authorization = "Bearer + {API_KEY}" | Authorization                 |
| AZURE MODEL NAME      | azureModelName                       | azure_model_name                    | x-portkey-azure-model-name    |

### Example

<Tabs>
  <Tab title="Node">
    ```js
    import Portkey from 'portkey-ai'

    const portkey = new Portkey({
        apiKey: "PORTKEY_API_KEY",
        provider: "azure-openai",
        azureResourceName: "AZURE_RESOURCE_NAME",
        azureDeploymentId: "AZURE_DEPLOYMENT_NAME",
        azureApiVersion: "AZURE_API_VERSION",
        azureModelName: "AZURE_MODEL_NAME"
        Authorization: "Bearer API_KEY"
    })
    ```
  </Tab>
  <Tab title="Python">

```python
from portkey_ai import Portkey

portkey = Portkey(
    api_key = "PORTKEY_API_KEY",
    provider = "azure-openai",
    azure_resource_name = "AZURE_RESOURCE_NAME",
    azure_deployment_id = "AZURE_DEPLOYMENT_NAME",
    azure_api_version = "AZURE_API_VERSION",
    azure_model_name = "AZURE_MODEL_NAME",
    Authorization = "Bearer API_KEY"
)
```
  </Tab>
<Tab title="cURL">
  ```sh
  curl https://api.portkey.ai/v1/chat/completions \
    -H "Content-Type: application/json" \
    -H "Authorization: Bearer $AZURE_OPENAI_API_KEY" \
    -H "x-portkey-api-key: $PORTKEY_API_KEY" \
    -H "x-portkey-provider: azure-openai" \
    -H "x-portkey-azure-resource-name: $AZURE_RESOURCE_NAME" \
    -H "x-portkey-azure-deployment-id: $AZURE_DEPLOYMENY_ID" \
    -H "x-portkey-azure-model-name: $AZURE_MODEL_NAME" \
    -H "x-portkey-azure-api-version: $AZURE_API_VERSION" \
    -d '{
      "model": "gpt-4o",
      "messages": [{"role": "user","content": "Hello!"}]
    }'
  ```
  </Tab>
</Tabs>


### How to Pass JWT (JSON Web Tokens)

If you have configured fine-grained access for Azure OpenAI and need to use `JSON web token (JWT)` in the `Authorization` header instead of the regular `API Key`, you can use the `forwardHeaders` parameter to do this.

<Tabs>
  <Tab title="Node">
    ```js
    import Portkey from 'portkey-ai'

    const portkey = new Portkey({
        apiKey: "PORTKEY_API_KEY",
        provider: "azure-openai",
        azureResourceName: "AZURE_RESOURCE_NAME",
        azureDeploymendId: "AZURE_DEPLOYMENT_NAME",
        azureApiVersion: "AZURE_API_VERSION",
        azureModelName: "AZURE_MODEL_NAME",
        Authorization: "Bearer JWT_KEY", // Pass your JWT here
        forwardHeaders: [ "Authorization" ]
    })
    ```
  </Tab>
  <Tab title="Python">

```js
import Portkey from 'portkey-ai'

const portkey = new Portkey({
    api_key = "PORTKEY_API_KEY",
    provider = "azure-openai",
    azure_resource_name = "AZURE_RESOURCE_NAME",
    azure_deploymend_id = "AZURE_DEPLOYMENT_NAME",
    azure_api_version = "AZURE_API_VERSION",
    azure_model_name = "AZURE_MODEL_NAME",
    Authorization = "Bearer API_KEY", # Pass your JWT here
    forward_headers= [ "Authorization" ]
)
```
  </Tab>
</Tabs>

For further questions on custom Azure deployments or fine-grained access tokens, reach out to us on support@portkey.ai

## Next Steps

The complete list of features supported in the SDK are available on the link below.

<Card title="SDK" href="/api-reference/portkey-sdk-client">
</Card>


You'll find more information in the relevant sections:

1. [Add metadata to your requests](/product/observability/metadata)
2. [Add gateway configs to your Azure OpenAI requests](/product/ai-gateway/configs)
3. [Tracing Azure OpenAI requests](/product/observability/traces)
4. [Setup a fallback from OpenAI to Azure OpenAI APIs](/product/ai-gateway/fallbacks)
