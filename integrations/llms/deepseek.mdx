---
title: "DeepSeek"
description: "Integrate DeepSeek models with Portkey's AI Gateway"
---

Portkey provides a robust and secure gateway to integrate various Large Language Models (LLMs) into applications, including DeepSeek's models.

With Portkey, take advantage of features like fast AI gateway access, observability, prompt management, and more, while securely managing API keys through [Model Catalog](/product/model-catalog).

## Quick Start

Get DeepSeek working in 3 steps:

<CodeGroup>
```python Python icon="python"
from portkey_ai import Portkey

# 1. Install: pip install portkey-ai
# 2. Add @deepseek provider in model catalog
# 3. Use it:

portkey = Portkey(api_key="PORTKEY_API_KEY")

response = portkey.chat.completions.create(
    model="@deepseek/deepseek-chat",
    messages=[{"role": "user", "content": "Say this is a test"}]
)

print(response.choices[0].message.content)
```

```js Javascript icon="square-js"
    import Portkey from 'portkey-ai'

// 1. Install: npm install portkey-ai
// 2. Add @deepseek provider in model catalog
// 3. Use it:

    const portkey = new Portkey({
    apiKey: "PORTKEY_API_KEY"
})

const response = await portkey.chat.completions.create({
    model: "@deepseek/deepseek-chat",
    messages: [{ role: "user", content: "Say this is a test" }]
})

console.log(response.choices[0].message.content)
```

```python OpenAI Py icon="python"
from openai import OpenAI
from portkey_ai import PORTKEY_GATEWAY_URL

# 1. Install: pip install openai portkey-ai
# 2. Add @deepseek provider in model catalog
# 3. Use it:

client = OpenAI(
    api_key="PORTKEY_API_KEY",  # Portkey API key
    base_url=PORTKEY_GATEWAY_URL
)

response = client.chat.completions.create(
    model="@deepseek/deepseek-chat",
    messages=[{"role": "user", "content": "Say this is a test"}]
)

print(response.choices[0].message.content)
```

```js OpenAI JS icon="square-js"
import OpenAI from "openai"
import { PORTKEY_GATEWAY_URL } from "portkey-ai"

// 1. Install: npm install openai portkey-ai
// 2. Add @deepseek provider in model catalog
// 3. Use it:

const client = new OpenAI({
    apiKey: "PORTKEY_API_KEY",  // Portkey API key
    baseURL: PORTKEY_GATEWAY_URL
})

const response = await client.chat.completions.create({
    model: "@deepseek/deepseek-chat",
    messages: [{ role: "user", content: "Say this is a test" }]
})

console.log(response.choices[0].message.content)
```

```sh cURL icon="square-terminal"
# 1. Add @deepseek provider in model catalog
# 2. Use it:

curl https://api.portkey.ai/v1/chat/completions \
  -H "Content-Type: application/json" \
  -H "x-portkey-api-key: $PORTKEY_API_KEY" \
  -d '{
    "model": "@deepseek/deepseek-chat",
    "messages": [
      { "role": "user", "content": "Say this is a test" }
    ]
  }'
```
</CodeGroup>

<Note>
**Tip:** You can also set `provider="@deepseek"` in `Portkey()` and use just `model="deepseek-chat"` in the request.
</Note>

## Add Provider in Model Catalog

1. Go to [**Model Catalog → Add Provider**](https://app.portkey.ai/model-catalog/providers)
2. Select **DeepSeek**
3. Choose existing credentials or create new by entering your [DeepSeek API key](https://platform.deepseek.com/api_keys)
4. Name your provider (e.g., `deepseek-prod`)

<Card title="Complete Setup Guide →" href="/product/model-catalog">
  See all setup options, code examples, and detailed instructions
</Card>

## Advanced Features

### Multi-round Conversations

DeepSeek supports multi-turn conversations where context is maintained across messages:

<CodeGroup>
```python Python
from portkey_ai import Portkey

    client = Portkey(
    api_key="PORTKEY_API_KEY",
    provider="@deepseek"
    )

    # Round 1
    messages = [{"role": "user", "content": "What's the highest mountain in the world?"}]
    response = client.chat.completions.create(
        model="deepseek-chat",
        messages=messages
    )

    messages.append(response.choices[0].message)
    print(f"Messages Round 1: {messages}")

    # Round 2
    messages.append({"role": "user", "content": "What is the second?"})
    response = client.chat.completions.create(
        model="deepseek-chat",
        messages=messages
    )

    messages.append(response.choices[0].message)
    print(f"Messages Round 2: {messages}")
    ```

```js Javascript
import Portkey from 'portkey-ai'

    const client = new Portkey({
    apiKey: "PORTKEY_API_KEY",
    provider: "@deepseek"
})

// Round 1
let messages = [{ role: "user", content: "What's the highest mountain in the world?" }]

let response = await client.chat.completions.create({
    model: "deepseek-chat",
    messages: messages
})

messages.push(response.choices[0].message)
console.log(`Messages Round 1: ${JSON.stringify(messages, null, 2)}`)

// Round 2
messages.push({ role: "user", content: "What is the second?" })
response = await client.chat.completions.create({
        model: "deepseek-chat",
    messages: messages
})

messages.push(response.choices[0].message)
console.log(`Messages Round 2: ${JSON.stringify(messages, null, 2)}`)
    ```
</CodeGroup>

### JSON Output

Force structured JSON responses from DeepSeek models:

<CodeGroup>
```python Python
    import json
from portkey_ai import Portkey

    client = Portkey(
    api_key="PORTKEY_API_KEY",
    provider="@deepseek"
    )

    system_prompt = """
    The user will provide some exam text. Please parse the "question" and "answer" and output them in JSON format.

    EXAMPLE INPUT:
    Which is the highest mountain in the world? Mount Everest.

    EXAMPLE JSON OUTPUT:
    {
        "question": "Which is the highest mountain in the world?",
        "answer": "Mount Everest"
    }
    """

    user_prompt = "Which is the longest river in the world? The Nile River."

messages = [
    {"role": "system", "content": system_prompt},
    {"role": "user", "content": user_prompt}
]

    response = client.chat.completions.create(
        model="deepseek-chat",
        messages=messages,
    response_format={"type": "json_object"}
    )

    print(json.loads(response.choices[0].message.content))
    ```

```js Javascript
import Portkey from 'portkey-ai'

const client = new Portkey({
    apiKey: "PORTKEY_API_KEY",
    provider: "@deepseek"
})

const systemPrompt = `
The user will provide some exam text. Please parse the "question" and "answer" and output them in JSON format.

EXAMPLE INPUT:
Which is the highest mountain in the world? Mount Everest.

EXAMPLE JSON OUTPUT:
{
    "question": "Which is the highest mountain in the world?",
    "answer": "Mount Everest"
}
`

const userPrompt = "Which is the longest river in the world? The Nile River."

const messages = [
    { role: "system", content: systemPrompt },
    { role: "user", content: userPrompt }
]

const response = await client.chat.completions.create({
    model: "deepseek-chat",
    messages: messages,
    responseFormat: { type: "json_object" }
})

console.log(JSON.parse(response.choices[0].message.content))
```
</CodeGroup>

## Managing DeepSeek Prompts

Manage all prompt templates to DeepSeek in the [Prompt Library](/product/prompt-library). All current DeepSeek models are supported, and you can easily test different prompts.

Use the `portkey.prompts.completions.create` interface to use the prompt in an application.

## Supported Endpoints

- Chat Completions
- Streaming Chat Completions

## Next Steps

<CardGroup cols={2}>
  <Card title="Add Metadata" icon="tags" href="/product/observability/metadata">
    Add metadata to your DeepSeek requests
  </Card>
  <Card title="Gateway Configs" icon="gear" href="/product/ai-gateway/configs">
    Add gateway configs to your DeepSeek requests
  </Card>
  <Card title="Tracing" icon="chart-line" href="/product/observability/traces">
    Trace your DeepSeek requests
  </Card>
  <Card title="Fallbacks" icon="arrow-rotate-left" href="/product/ai-gateway/fallbacks">
    Setup fallback from OpenAI to DeepSeek
  </Card>
</CardGroup>

For complete SDK documentation:

<Card title="SDK Reference" icon="code" href="/api-reference/sdk/list">
  Complete Portkey SDK documentation
</Card>
