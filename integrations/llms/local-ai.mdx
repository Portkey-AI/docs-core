---
title: "LocalAI"
description: "Integrate LocalAI-hosted models with Portkey for local LLM deployment with observability."
---

Portkey provides a robust gateway to facilitate the integration of your **locally hosted models through** [**LocalAI**](https://localai.io/).

## Integration Steps

<Steps>
<Step title="Expose your LocalAI Server">
Ensure your LocalAI API is externally accessible. If running on `http://localhost`, use a tool like `ngrok` to create a public URL.

```sh
ngrok http 8080
```
</Step>

<Step title="Add to Model Catalog">

1. Go to [**Model Catalog â†’ Add Provider**](https://app.portkey.ai/model-catalog/providers)
2. Enable **"Local/Privately hosted provider"** toggle
3. Select **OpenAI** as the provider type (LocalAI follows OpenAI API schema)
4. Enter your LocalAI URL with `/v1` in **Custom Host**: `https://your-localai.ngrok-free.app/v1`
5. Name your provider (e.g., `my-localai`)

<Card title="Complete Setup Guide" icon="book" href="/product/model-catalog">
  See all setup options
</Card>
</Step>

<Step title="Use in Your Application">

<CodeGroup>

```python Python
from portkey_ai import Portkey

portkey = Portkey(
    api_key="PORTKEY_API_KEY",
    provider="@my-localai"
)

response = portkey.chat.completions.create(
    model="ggml-koala-7b-model-q4_0-r2.bin",
    messages=[{"role": "user", "content": "Hello!"}]
)

print(response.choices[0].message.content)
```

```javascript Node.js
import Portkey from 'portkey-ai';

const portkey = new Portkey({
    apiKey: 'PORTKEY_API_KEY',
    provider: '@my-localai'
});

const response = await portkey.chat.completions.create({
    model: 'ggml-koala-7b-model-q4_0-r2.bin',
    messages: [{ role: 'user', content: 'Hello!' }]
});

console.log(response.choices[0].message.content);
```

</CodeGroup>

**Or use custom host directly:**

<CodeGroup>

```python Python
from portkey_ai import Portkey

portkey = Portkey(
    api_key="PORTKEY_API_KEY",
    provider="openai",
    custom_host="https://your-localai.ngrok-free.app/v1"
)
```

```javascript Node.js
import Portkey from 'portkey-ai';

const portkey = new Portkey({
    apiKey: 'PORTKEY_API_KEY',
    provider: 'openai',
    customHost: 'https://your-localai.ngrok-free.app/v1'
});
```

</CodeGroup>

</Step>
</Steps>

<Note>
**Important:** 
- Don't forget to include the version identifier (`/v1`) in the Custom Host URL
- Portkey supports all endpoints that adhere to the OpenAI specification
</Note>

---

## Next Steps

<CardGroup cols={2}>
  <Card title="Gateway Configs" icon="sliders" href="/product/ai-gateway">
    Add retries, timeouts, and fallbacks
  </Card>
  <Card title="Observability" icon="chart-line" href="/product/observability">
    Monitor your LocalAI requests
  </Card>
  <Card title="Custom Host Guide" icon="server" href="/product/ai-gateway/universal-api#integrating-local-or-private-models">
    Learn more about custom host setup
  </Card>
  <Card title="BYOLLM Guide" icon="book" href="/integrations/llms/byollm">
    Complete guide for private LLMs
  </Card>
</CardGroup>

For complete SDK documentation:

<Card title="SDK Reference" icon="code" href="/api-reference/sdk/list">
  Complete Portkey SDK documentation
</Card>
