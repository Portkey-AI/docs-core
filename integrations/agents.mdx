---
title: "Overview"
description: "Portkey helps bring your agents to production"
---
<CardGroup cols={4}>
  <Card title="OpenAI Agents (Python)" href="/integrations/agents/openai-agents">
    <Frame>
    <img src="/images/openai-agents.png" alt="OpenAI Agents" />
    </Frame>
  </Card>

  <Card title="OpenAI Agents (Type Script)" href="/integrations/agents/openai-agents-ts">
    <Frame>
    <img src="/images/openai-agents.png" alt="OpenAI Agents" />
    </Frame>
  </Card>

  <Card title="Pydantic AI" href="/integrations/agents/pydantic-ai">
    <Frame>
    <img src="/images/autogen/pydantic.png" alt="Pydantic AI Agents" />
    </Frame>
  </Card>

  <Card title="Autogen" href="/integrations/agents/autogen">
    <Frame>
    <img src="/images/autogen/autogen.jpg" alt="Autogen" />
    </Frame>
  </Card>

  <Card title="CrewAI" href="/integrations/agents/crewai">
  <Frame>
    <img src="/images/autogen/crew-ai.avif" alt="CrewAI" />
  </Frame>
  </Card>

  <Card title="Agno AI" href="/integrations/agents/agno-ai">
    <Frame>
    <img src="/images/agno-ai.svg" alt="Agno-AI" />
    </Frame>
  </Card>

  <Card title="Llama Index" href="/integrations/agents/llama-agents">
    <Frame>
    <img src="/images/autogen/llama-a.jpg" alt="Llama Index" />
    </Frame>
  </Card>

  <Card title="LangChain" href="/integrations/agents/langchain-agents">
    <Frame>
    <img src="/images/autogen/langchain-a.avif" alt="LangChain" />
    </Frame>
  </Card>
  <Card title="LangGraph" href="/integrations/agents/langgraph">
    <Frame>
    <img src="/images/autogen/Langgraph.jpeg" alt="Langgraph" />
    </Frame>
  </Card>
  <Card title="Langroid" href="/integrations/agents/langroid">
  <Frame>
    <img src="/images/autogen/langgroid.png" alt="Langroid" />
  </Frame>
  </Card>

 <Card title="OpenAI Swarm" href="/integrations/agents/openai-swarm">
    <Frame>
    <img src="/images/autogen/swarm.png" alt="Swarm" />
    </Frame>
  </Card>
  <Card title="Control Flow" href="/integrations/agents/control-flow">
    <Frame>
    <img src="/images/autogen/control-flow.avif" alt="Control Flow" />
    </Frame>
  </Card>

  <Card title="Strands Agents" href="/integrations/agents/strands">
    <Frame>
    <img src="/images/strands.svg" alt="Strands" width="40" />
    </Frame>
  </Card>

  <Card title="Bring Your Agent" href="/integrations/agents/bring-your-own-agents">
    <Frame>
    <img src="/images/autogen/bya.avif" alt="Bring Your Agent" />
    </Frame>
  </Card>
</CardGroup>



## Integrate Portkey with your agents with just 2 lines of code

```py Langchain
from langchain_openai import ChatOpenAI
from portkey_ai import createHeaders, PORTKEY_GATEWAY_URL

llm = ChatOpenAI(
    api_key="OpenAI_API_Key",
    base_url=PORTKEY_GATEWAY_URL,
    default_headers=createHeaders(
        provider="openai", #choose your provider
        api_key="PORTKEY_API_KEY"
    )
)
```

### Get Started with Portkey x Agent Cookbooks

* [Autogen](https://dub.sh/Autogen-docs)
* [CrewAI](https://git.new/crewAI-docs)
* [Phidata](https://dub.sh/Phidata-docs)
* [Llama Index ](https://git.new/llama-agents)
* [Control Flow](https://dub.sh/Control-Flow-docs)

***

## Key Production Features

By routing your agent's requests through Portkey, you make your agents production-grade with the following features.

### 1\. [Interoperability](/product/ai-gateway/universal-api)

Easily switch between LLM providers. Call various LLMs such as Anthropic, Gemini, Mistral, Azure OpenAI, Google Vertex AI, AWS Bedrock and much more by simply changing the `provider ` and `API key` in the LLM object.

### 2\. [Caching](/product/ai-gateway/cache-simple-and-semantic)

Improve performance and reduce costs on your Agent's LLM calls by storing past responses in the Portkey cache. Choose between Simple and Semantic cache modes in your Portkey's gateway config.


```json
{
 "cache": {
    "mode": "semantic" // Choose between "simple" or "semantic"
 }
}
```

### 3\. [Reliability](/product/ai-gateway)

Set up **fallbacks** between different LLMs or providers, **load balance** your requests across multiple instances or API keys, set **automatic retries**, and **request timeouts.** Ensure your agents' resilience with advanced reliability features.

```json
{
  "retry": {
    "attempts": 5
  },
  "strategy": {
    "mode": "loadbalance" // Choose between "loadbalance" or "fallback"
  },
  "targets": [
    {
      "provider": "openai",
      "api_key": "OpenAI_API_Key"
    },
    {
      "provider": "anthropic",
      "api_key": "Anthropic_API_Key"
    }
  ]
}
```

### 4\. [Observability](/product/observability)

Portkey automatically logs key details about your agent runs, including cost, tokens used, response time, etc. For agent-specific observability, add Trace IDs to the request headers for each agent. This enables filtering analytics by Trace IDs, ensuring deeper monitoring and analysis.

### 5\. [Logs](/product/observability/logs)

Access a dedicated section to view records of action executions, including parameters, outcomes, and errors. Filter logs of your agent run based on multiple parameters such as trace ID, model, tokens used, metadata, etc.

<Frame>
<img src="/images/integrations/analytics-new-gif.gif" alt="azure" />
</Frame>

### 6\. [Prompt Management](/product/prompt-library)

Use Portkey as a centralized hub to store, version, and experiment with your agent's prompts across multiple LLMs. Easily modify your prompts and run A/B tests without worrying about the breaking prod.

### 7\. [Continuous Improvement](/product/observability/feedback)

Improve your Agent runs by capturing qualitative & quantitative user feedback on your requests, and then using that feedback to make your prompts AND LLMs themselves better.

### 8\. [Security & Compliance](/product/enterprise-offering/security-portkey)

Set budget limits on provider API keys and implement fine-grained user roles and permissions for both the app and the Portkey APIs.
