---
title: "CrewAI"
description: "Add observability, reliability, and governance to CrewAI agents."
---

Portkey enhances CrewAI with production features:

- Complete observability of agent steps, tool use, and interactions
- Built-in reliability with fallbacks, retries, and load balancing
- Cost tracking and optimization
- Access to 1600+ LLMs through a single integration
- Guardrails for safe, compliant agent behavior

<Card title="CrewAI Documentation" icon="arrow-up-right-from-square" href="https://docs.crewai.com/">
  Learn more about CrewAI's core concepts
</Card>

## Quick Start

<Steps>
<Step title="Install packages">
```bash
pip install -U crewai portkey-ai
```
</Step>

<Step title="Add provider in Model Catalog">
Go to [Model Catalog → Add Provider](https://app.portkey.ai/model-catalog). Select your provider (OpenAI, Anthropic, etc.), enter API keys, and name it (e.g., `openai-prod`).

Your provider slug is `@openai-prod`.
</Step>

<Step title="Get Portkey API Key">
Create an API key at [app.portkey.ai/api-keys](https://app.portkey.ai/api-keys).

**Pro tip:** Attach a default [config](https://app.portkey.ai/configs) for fallbacks, caching, and guardrails—applies automatically.
</Step>

<Step title="Configure CrewAI">
```python
from crewai import Agent, LLM
from portkey_ai import createHeaders, PORTKEY_GATEWAY_URL

llm = LLM(
    model="gpt-4o",
    base_url=PORTKEY_GATEWAY_URL,
    api_key="dummy",  # Required by CrewAI, not used
    extra_headers=createHeaders(
        api_key="YOUR_PORTKEY_API_KEY",
        provider="@openai-prod"
    )
)

researcher = Agent(
    config=self.agents_config['lead_market_analyst'],
    llm=llm
)
```
</Step>
</Steps>

## Production Features

### Observability

All agent interactions are automatically logged:

<Frame>
  <img src="/images/product/product-11-1.webp"/>
</Frame>

Add trace IDs to group related requests:

```python
llm = LLM(
    model="gpt-4o",
    base_url=PORTKEY_GATEWAY_URL,
    api_key="dummy",
    extra_headers=createHeaders(
        api_key="YOUR_PORTKEY_API_KEY",
        provider="@openai-prod",
        trace_id="crew-session-123"
    )
)
```

Add metadata for filtering and analytics:

```python
extra_headers=createHeaders(
    api_key="YOUR_PORTKEY_API_KEY",
    provider="@openai-prod",
    metadata={
        "crew_type": "research",
        "_user": "user_123",
        "environment": "production"
    }
)
```

<Frame>
  <img src="/images/metadata.png" alt="Analytics with metadata filters" />
</Frame>

### Reliability

Enable fallbacks, retries, and load balancing via [Configs](https://app.portkey.ai/configs). Attach to your API key or pass inline:

```python
llm = LLM(
    model="gpt-4o",
    base_url=PORTKEY_GATEWAY_URL,
    api_key="dummy",
    extra_headers=createHeaders(
        api_key="YOUR_PORTKEY_API_KEY",
        config={
            "strategy": { "mode": "fallback" },
            "targets": [
                { "override_params": { "model": "@openai-prod/gpt-4o" } },
                { "override_params": { "model": "@anthropic-prod/claude-sonnet-4" } }
            ]
        }
    )
)
```

<CardGroup cols="2">
  <Card title="Automatic Retries" icon="rotate" href="../../product/ai-gateway/automatic-retries">
    Handles temporary failures automatically
  </Card>
  <Card title="Request Timeouts" icon="clock" href="../../product/ai-gateway/request-timeouts">
    Prevent agents from hanging
  </Card>
  <Card title="Conditional Routing" icon="route" href="../../product/ai-gateway/conditional-routing">
    Route requests based on custom logic
  </Card>
  <Card title="Load Balancing" icon="scale-balanced" href="../../product/ai-gateway/load-balancing">
    Distribute across multiple keys
  </Card>
</CardGroup>

### Guardrails

Add input/output validation:

```python
extra_headers=createHeaders(
    api_key="YOUR_PORTKEY_API_KEY",
    provider="@openai-prod",
    config={
        "input_guardrails": ["guardrail-id-xxx"],
        "output_guardrails": ["guardrail-id-yyy"]
    }
)
```

Guardrails can:
- Detect and redact PII
- Filter harmful content
- Validate response formats
- Apply custom business rules

<Card title="Guardrails Guide" icon="shield-check" href="/product/guardrails">
  PII detection, content filtering, and custom rules
</Card>

### Caching

Reduce costs with response caching:

<CodeGroup>

```python Simple Cache
extra_headers=createHeaders(
    api_key="YOUR_PORTKEY_API_KEY",
    provider="@openai-prod",
    config={ "cache": { "mode": "simple" } }
)
```

```python Semantic Cache
extra_headers=createHeaders(
    api_key="YOUR_PORTKEY_API_KEY",
    provider="@openai-prod",
    config={ "cache": { "mode": "semantic" } }
)
```

</CodeGroup>

### Switching Providers

Change the provider to switch models:

```python
# OpenAI
createHeaders(api_key="YOUR_PORTKEY_API_KEY", provider="@openai-prod")

# Anthropic
createHeaders(api_key="YOUR_PORTKEY_API_KEY", provider="@anthropic-prod")

# Google
createHeaders(api_key="YOUR_PORTKEY_API_KEY", provider="@google-prod")
```

Update `model` in the LLM to match the provider's model format.

### Prompt Templates

Use Portkey's prompt management for versioned, templated prompts:

```python
from portkey_ai import Portkey

portkey = Portkey(api_key="YOUR_PORTKEY_API_KEY")

prompt_data = portkey.prompts.render(
    prompt_id="YOUR_PROMPT_ID",
    variables={"agent_role": "Senior Research Scientist"}
)

researcher = Agent(
    role="Senior Research Scientist",
    backstory=prompt_data.data.messages[0]["content"],
    llm=llm
)
```

<Card title="Prompt Engineering Studio" icon="wand-magic-sparkles" href="/product/prompt-library">
  Prompt versioning and collaboration
</Card>

## Enterprise Governance

Set up centralized control for CrewAI across your organization.

<Steps>
<Step title="Add Provider with Budget">
Go to [Model Catalog](https://app.portkey.ai/model-catalog) → Add Provider. Set budget limits and rate limits per provider.
</Step>

<Step title="Create Config">
Go to [Configs](https://app.portkey.ai/configs):

```json
{
  "override_params": { "model": "@openai-prod/gpt-4o" }
}
```

Add fallbacks, guardrails, or routing as needed.
</Step>

<Step title="Create Team API Keys">
Go to [API Keys](https://app.portkey.ai/api-keys). Create keys per team, attach configs, and set permissions.

<Frame>
<img src="/images/integrations/api-key.png" width="500"/>
</Frame>
</Step>

<Step title="Distribute to Teams">
Teams use their Portkey API key—no raw provider keys needed:

```python
llm = LLM(
    model="gpt-4o",
    base_url=PORTKEY_GATEWAY_URL,
    api_key="TEAM_PORTKEY_API_KEY"  # Config attached to key
)
```
</Step>
</Steps>

Benefits:
- Rotate provider keys without code changes
- Per-team budgets and rate limits
- Centralized usage analytics
- Instant access revocation

<Card title="Enterprise Features" icon="building" href="/product/enterprise-offering">
  Governance, security, and compliance
</Card>

## FAQ

<AccordionGroup>
  <Accordion title="Can I use Portkey with existing CrewAI apps?">
    Yes. Update your LLM configuration—agent and crew code stays unchanged.
  </Accordion>

  <Accordion title="Does Portkey work with all CrewAI features?">
    Yes. Agents, tools, human-in-the-loop, sequential/hierarchical processes all work.
  </Accordion>

  <Accordion title="Can I track multiple agents in a crew?">
    Use a consistent `trace_id` across agents to see the full workflow in one trace.
  </Accordion>

  <Accordion title="Can I use my own API keys?">
    Yes. Portkey stores your provider keys securely. Rotate keys without code changes.
  </Accordion>
</AccordionGroup>

## Resources

<CardGroup cols="2">
  <Card title="CrewAI Docs" icon="book" href="https://docs.crewai.com/">
    Official documentation
  </Card>
  <Card title="Book a Demo" icon="calendar" href="https://calendly.com/portkey-ai">
    Get implementation guidance
  </Card>
</CardGroup>
