---
title: "Strands Agents"
description: "Add observability, reliability, and multi-provider support to Strands Agents."
---

Portkey enhances AWS's Strands Agents with production features:

- Complete observability of agent steps, tool use, and LLM interactions
- Built-in reliability with fallbacks, retries, and load balancing
- Access to 1600+ LLMs through the same OpenAI-compatible interface
- Zero code changes to existing agent logic

<Card title="Strands Agents Documentation" href="https://strandsagents.com/">
  Learn more about Strands Agents
</Card>

## Quick Start

<Steps>
<Step title="Install packages">
```bash
pip install -U strands-agents strands-agents-tools openai portkey-ai
```
</Step>

<Step title="Add provider in Model Catalog">
Go to [Model Catalog → Add Provider](https://app.portkey.ai/model-catalog). Select your provider (OpenAI, Anthropic, etc.), enter API keys, and name it (e.g., `openai-prod`).

Your provider slug is `@openai-prod`.
</Step>

<Step title="Get Portkey API Key">
Create an API key at [app.portkey.ai/api-keys](https://app.portkey.ai/api-keys).

**Pro tip:** Attach a default [config](https://app.portkey.ai/configs) for fallbacks and routing—applies automatically.
</Step>

<Step title="Configure Strands">
```python
from strands import Agent
from strands.models.openai import OpenAIModel
from strands_tools import calculator
from portkey_ai import PORTKEY_GATEWAY_URL, createHeaders

model = OpenAIModel(
    client_args={
        "api_key": "YOUR_PORTKEY_API_KEY",
        "base_url": PORTKEY_GATEWAY_URL,
        "default_headers": createHeaders(provider="@openai-prod")
    },
    model_id="gpt-4o",
    params={"temperature": 0.7}
)

agent = Agent(model=model, tools=[calculator])
response = agent("What is 2+2?")
```
</Step>
</Steps>

## How It Works

The integration uses Strands' `client_args` parameter to route requests through Portkey:

```python
# Strands passes client_args to OpenAI client
client_args = client_args or {}
self.client = openai.OpenAI(**client_args)  # Portkey config gets passed here
```

All Portkey features work without changes to agent logic, tool usage, or response handling.

## Production Features

### Observability

All agent interactions are automatically logged:

<Frame>
  <img src="/images/product/product-11-1.webp"/>
</Frame>

Add trace IDs to group related requests:

```python
model = OpenAIModel(
    client_args={
        "api_key": "YOUR_PORTKEY_API_KEY",
        "base_url": PORTKEY_GATEWAY_URL,
        "default_headers": createHeaders(
            provider="@openai-prod",
            trace_id="agent-session-123"
        )
    },
    model_id="gpt-4o",
    params={"temperature": 0.7}
)
```

Add metadata for filtering and analytics:

```python
"default_headers": createHeaders(
    provider="@openai-prod",
    trace_id="support-bot",
    metadata={
        "agent_type": "customer_support",
        "user_tier": "premium",
        "session_id": "sess_789"
    }
)
```

### Reliability

Enable fallbacks, retries, and load balancing via [Configs](https://app.portkey.ai/configs). Attach to your API key or pass inline:

<CodeGroup>

```python Fallbacks
model = OpenAIModel(
    client_args={
        "api_key": "YOUR_PORTKEY_API_KEY",
        "base_url": PORTKEY_GATEWAY_URL,
        "default_headers": createHeaders(
            config={
                "strategy": {
                    "mode": "fallback",
                    "on_status_codes": [429, 503, 502]
                },
                "targets": [
                    { "override_params": { "model": "@openai-prod/gpt-4o" } },
                    { "override_params": { "model": "@anthropic-prod/claude-sonnet-4" } }
                ]
            }
        )
    },
    model_id="gpt-4o",
    params={"temperature": 0.7}
)
```

```python Load Balancing
model = OpenAIModel(
    client_args={
        "api_key": "YOUR_PORTKEY_API_KEY",
        "base_url": PORTKEY_GATEWAY_URL,
        "default_headers": createHeaders(
            config={
                "strategy": { "mode": "loadbalance" },
                "targets": [
                    { "override_params": { "model": "@openai-prod/gpt-4o" }, "weight": 70 },
                    { "override_params": { "model": "@openai-backup/gpt-4o" }, "weight": 30 }
                ]
            }
        )
    },
    model_id="gpt-4o",
    params={"temperature": 0.7}
)
```

</CodeGroup>

<Card title="Conditional Routing" icon="route" href="/product/ai-gateway/conditional-routing">
  Route based on metadata, input, or custom logic
</Card>

### Switching Providers

Change the provider to use different models:

<CodeGroup>

```python Anthropic
model = OpenAIModel(
    client_args={
        "api_key": "YOUR_PORTKEY_API_KEY",
        "base_url": PORTKEY_GATEWAY_URL,
        "default_headers": createHeaders(provider="@anthropic-prod")
    },
    model_id="claude-3-7-sonnet-latest",
    params={"max_tokens": 1000}
)
```

```python Multiple Providers
# Reasoning tasks
reasoning_model = OpenAIModel(
    client_args={
        "api_key": "YOUR_PORTKEY_API_KEY",
        "base_url": PORTKEY_GATEWAY_URL,
        "default_headers": createHeaders(provider="@openai-prod")
    },
    model_id="gpt-4o",
    params={"temperature": 0.1}
)

# Creative tasks
creative_model = OpenAIModel(
    client_args={
        "api_key": "YOUR_PORTKEY_API_KEY",
        "base_url": PORTKEY_GATEWAY_URL,
        "default_headers": createHeaders(provider="@google-prod")
    },
    model_id="gemini-2.0-flash-exp",
    params={"temperature": 0.8}
)

reasoning_agent = Agent(model=reasoning_model, tools=[calculator])
creative_agent = Agent(model=creative_model, tools=[])
```

</CodeGroup>

<Card title="Supported Providers" icon="server" href="/integrations/llms">
  1600+ models from OpenAI, Anthropic, Google, Mistral, and more
</Card>

### Guardrails

Add input/output validation for safe agent behavior:

```python
"default_headers": createHeaders(
    provider="@openai-prod",
    config={
        "input_guardrails": ["guardrail-id-xxx"],
        "output_guardrails": ["guardrail-id-yyy"]
    }
)
```

<Card title="Guardrails Guide" icon="shield-check" href="/product/guardrails">
  PII detection, content filtering, and custom rules
</Card>

## Enterprise Governance

Set up centralized control for Strands Agents across your organization.

<Steps>
<Step title="Add Provider with Budget">
Go to [Model Catalog](https://app.portkey.ai/model-catalog) → Add Provider. Set budget limits and rate limits.
</Step>

<Step title="Create Config">
Go to [Configs](https://app.portkey.ai/configs):

```json
{
  "override_params": { "model": "@openai-prod/gpt-4o" }
}
```
</Step>

<Step title="Create Team API Keys">
Go to [API Keys](https://app.portkey.ai/api-keys). Create keys per team, attach configs.
</Step>

<Step title="Distribute to Teams">
Teams use their Portkey API key—no raw provider keys needed:

```python
model = OpenAIModel(
    client_args={
        "api_key": "TEAM_PORTKEY_API_KEY",  # Config attached to key
        "base_url": PORTKEY_GATEWAY_URL
    },
    model_id="gpt-4o",
    params={"temperature": 0.7}
)
```
</Step>
</Steps>

Benefits:
- Rotate provider keys without code changes
- Per-team budgets and rate limits
- Centralized usage analytics
- Instant access revocation

## Troubleshooting

<AccordionGroup>
  <Accordion title="Import errors">
Ensure all packages are installed:
```bash
pip install -U strands-agents strands-agents-tools openai portkey-ai
```
  </Accordion>

  <Accordion title="Authentication errors">
Test your Portkey API key:
```python
from portkey_ai import Portkey

portkey = Portkey(api_key="YOUR_PORTKEY_API_KEY")
response = portkey.chat.completions.create(
    messages=[{"role": "user", "content": "test"}],
    model="gpt-4o"
)
```
  </Accordion>

  <Accordion title="Rate limiting despite fallbacks">
Ensure fallbacks include rate limit codes:
```python
"strategy": {
    "mode": "fallback",
    "on_status_codes": [429, 503, 502, 504]
}
```
  </Accordion>

  <Accordion title="Missing traces/logs">
Verify `base_url` is set:
```python
print(model.client.base_url)  # Should be https://api.portkey.ai/v1
```
  </Accordion>
</AccordionGroup>

## FAQ

<AccordionGroup>
  <Accordion title="Can I use Portkey with existing Strands apps?">
    Yes. Update your model initialization—agent code stays unchanged.
  </Accordion>

  <Accordion title="Does Portkey work with all Strands features?">
    Yes. Tool use, multi-agent systems, and all features work.
  </Accordion>

  <Accordion title="Can I track multiple agents in a workflow?">
    Use a consistent `trace_id` across agents to see the full workflow in one trace.
  </Accordion>

  <Accordion title="Can I use my own API keys?">
    Yes. Portkey stores your provider keys securely. Rotate keys without code changes.
  </Accordion>
</AccordionGroup>

## Resources

<CardGroup cols="2">
  <Card title="Strands Agents Docs" icon="book" href="https://strandsagents.com/">
    Official documentation
  </Card>
  <Card title="Book a Demo" icon="calendar" href="https://calendly.com/portkey-ai">
    Get implementation guidance
  </Card>
</CardGroup>
