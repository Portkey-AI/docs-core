---
title: "Langchain Agents"
description: "Add observability, reliability, and multi-provider support to Langchain agents."
---

Portkey enhances Langchain agents with production features:

- Complete observability of agent steps and LLM interactions
- Built-in reliability with fallbacks, retries, and load balancing
- Access to 1600+ LLMs through a single integration
- Cost tracking and optimization

## Quick Start

```python
from langchain_openai import ChatOpenAI
from portkey_ai import createHeaders, PORTKEY_GATEWAY_URL

llm = ChatOpenAI(
    api_key="YOUR_PORTKEY_API_KEY",
    base_url=PORTKEY_GATEWAY_URL,
    default_headers=createHeaders(provider="@openai-prod")
)
```

All agent interactions are now logged in your [Portkey dashboard](https://app.portkey.ai/logs).

[![Open in Colab](/images/guides/colab-badge.svg)](https://colab.research.google.com/drive/1ab_XnSf-HR1KndEGBgXDW6RvONKoHdzL?usp=sharing)

## Setup

<Steps>
<Step title="Install packages">
```bash
pip install -qU langchain langchain-openai portkey-ai
```
</Step>

<Step title="Add provider in Model Catalog">
Go to [Model Catalog â†’ Add Provider](https://app.portkey.ai/model-catalog). Select your provider, enter API keys, and name it (e.g., `openai-prod`).

Your provider slug is `@openai-prod`.
</Step>

<Step title="Get Portkey API Key">
Create an API key at [app.portkey.ai/api-keys](https://app.portkey.ai/api-keys).

**Pro tip:** Attach a default [config](https://app.portkey.ai/configs) for fallbacks and caching.
</Step>

<Step title="Configure Langchain LLM">
```python
from langchain_openai import ChatOpenAI
from portkey_ai import createHeaders, PORTKEY_GATEWAY_URL

llm = ChatOpenAI(
    api_key="YOUR_PORTKEY_API_KEY",
    base_url=PORTKEY_GATEWAY_URL,
    default_headers=createHeaders(provider="@openai-prod")
)
```
</Step>
</Steps>

## Production Features

### Observability

Add trace IDs and metadata for filtering:

```python
default_headers=createHeaders(
    provider="@openai-prod",
    trace_id="research-agent-1",
    metadata={"agent_type": "research", "_user": "user_123"}
)
```

### Tracing with Callback Handler

Use the Portkey callback handler for detailed traces:

```python
from portkey_ai.langchain import LangchainCallbackHandler

portkey_handler = LangchainCallbackHandler(
    api_key="YOUR_PORTKEY_API_KEY",
    metadata={"session_id": "session_1", "agent_id": "research_agent_1"}
)

llm = ChatOpenAI(
    api_key="YOUR_OPENAI_API_KEY",
    callbacks=[portkey_handler]
)
```

<Frame>
  <img src="/images/autogen/autogen-4.gif"/>
</Frame>

### Reliability

Enable fallbacks via [Configs](https://app.portkey.ai/configs):

```json
{
  "strategy": { "mode": "fallback" },
  "targets": [
    { "override_params": { "model": "@openai-prod/gpt-4o" } },
    { "override_params": { "model": "@anthropic-prod/claude-sonnet-4" } }
  ]
}
```

### Guardrails

Add input/output validation:

```json
{
  "input_guardrails": ["guardrail-id-xxx"],
  "output_guardrails": ["guardrail-id-yyy"]
}
```

<Card title="Guardrails Guide" icon="shield-check" href="/product/guardrails">
  PII detection, content filtering, and custom rules
</Card>

### Caching

Reduce costs with response caching:

```json
{
  "cache": { "mode": "semantic" }
}
```

### Switching Providers

Change the provider to switch models:

```python
# OpenAI
createHeaders(provider="@openai-prod")

# Anthropic
createHeaders(provider="@anthropic-prod")

# Azure OpenAI
createHeaders(provider="@azure-prod")

# AWS Bedrock
createHeaders(provider="@bedrock-prod")
```

<Card title="Supported Providers" icon="server" href="/integrations/llms">
  See all 1600+ supported models
</Card>

## Next Steps

<CardGroup cols={2}>
  <Card title="Model Catalog" icon="database" href="/product/model-catalog">
    Set up providers and budgets
  </Card>
  <Card title="Configs" icon="gear" href="/product/ai-gateway/configs">
    Fallbacks, caching, and routing
  </Card>
</CardGroup>
