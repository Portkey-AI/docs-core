---
title: "OpenAI Agents SDK (Python)"
description: "Add observability, reliability, and multi-provider support to OpenAI Agents."
---

Portkey enhances OpenAI Agents SDK with production features—no changes to your agent logic:

- Complete observability of agent steps, tool use, and handoffs
- Built-in reliability with fallbacks, retries, and load balancing
- Access to 1600+ LLMs through the same interface
- Cost tracking and optimization
- Guardrails for safe agent behavior

<Card title="OpenAI Agents SDK Documentation" icon="arrow-up-right-from-square" href="https://openai.github.io/openai-agents-python/">
  Learn more about OpenAI Agents SDK
</Card>

## Quick Start

The integration requires one change: set a Portkey-configured client as the default.

```python
from agents import Agent, Runner, set_default_openai_client, set_default_openai_api
from openai import AsyncOpenAI
from portkey_ai import PORTKEY_GATEWAY_URL, createHeaders

# Configure Portkey as the default client
client = AsyncOpenAI(
    base_url=PORTKEY_GATEWAY_URL,
    api_key="YOUR_PORTKEY_API_KEY",
    default_headers=createHeaders(provider="@openai-prod")
)
set_default_openai_client(client, use_for_tracing=False)
set_default_openai_api("chat_completions")

# Your agent code stays exactly the same
agent = Agent(
    name="Math Tutor",
    instructions="You provide help with math problems. Explain your reasoning at each step.",
    model="gpt-4o"
)

result = Runner.run_sync(agent, "What is the derivative of x^2?")
print(result.final_output)
```

All agent interactions are now logged in your [Portkey dashboard](https://app.portkey.ai/logs).

## Setup

<Steps>
<Step title="Install packages">
```bash
pip install -U openai-agents portkey-ai
```
</Step>

<Step title="Add provider in Model Catalog">
Go to [Model Catalog → Add Provider](https://app.portkey.ai/model-catalog). Select your provider (OpenAI, Anthropic, etc.), enter API keys, and name it (e.g., `openai-prod`).

Your provider slug is `@openai-prod`.
</Step>

<Step title="Get Portkey API Key">
Create an API key at [app.portkey.ai/api-keys](https://app.portkey.ai/api-keys).

**Pro tip:** Attach a default [config](https://app.portkey.ai/configs) for fallbacks, caching, and guardrails—applies automatically without code changes.
</Step>

<Step title="Set the default client">
```python
from agents import set_default_openai_client, set_default_openai_api
from openai import AsyncOpenAI
from portkey_ai import PORTKEY_GATEWAY_URL, createHeaders

client = AsyncOpenAI(
    base_url=PORTKEY_GATEWAY_URL,
    api_key="YOUR_PORTKEY_API_KEY",
    default_headers=createHeaders(provider="@openai-prod")
)
set_default_openai_client(client, use_for_tracing=False)
set_default_openai_api("chat_completions")
```

All agents now route through Portkey.
</Step>
</Steps>

## Integration Approaches

Three ways to integrate, depending on your needs:

<Tabs>
<Tab title="Global Client (Recommended)">
Set once, applies to all agents:

```python
from agents import set_default_openai_client, set_default_openai_api
from openai import AsyncOpenAI
from portkey_ai import PORTKEY_GATEWAY_URL, createHeaders

client = AsyncOpenAI(
    base_url=PORTKEY_GATEWAY_URL,
    api_key="YOUR_PORTKEY_API_KEY",
    default_headers=createHeaders(provider="@openai-prod")
)
set_default_openai_client(client, use_for_tracing=False)
set_default_openai_api("chat_completions")

# All agents use Portkey automatically
agent = Agent(name="Assistant", instructions="You are helpful.", model="gpt-4o")
```

**Best for:** Full application migration with minimal code changes.
</Tab>

<Tab title="Per-Run Config">
Use a ModelProvider to control which runs use Portkey:

```python
from agents import Agent, Runner, RunConfig, ModelProvider, OpenAIChatCompletionsModel
from openai import AsyncOpenAI
from portkey_ai import PORTKEY_GATEWAY_URL, createHeaders

client = AsyncOpenAI(
    base_url=PORTKEY_GATEWAY_URL,
    api_key="YOUR_PORTKEY_API_KEY",
    default_headers=createHeaders(provider="@openai-prod")
)

class PortkeyProvider(ModelProvider):
    def get_model(self, model_name: str | None):
        return OpenAIChatCompletionsModel(
            model=model_name or "gpt-4o",
            openai_client=client
        )

# Only this run uses Portkey
result = await Runner.run(
    agent,
    "Hello",
    run_config=RunConfig(model_provider=PortkeyProvider())
)
```

**Best for:** A/B testing, staged rollouts, toggling providers at runtime.
</Tab>

<Tab title="Per-Agent Model">
Attach a specific model to each agent:

```python
from agents import Agent, Runner, OpenAIChatCompletionsModel
from openai import AsyncOpenAI
from portkey_ai import PORTKEY_GATEWAY_URL, createHeaders

client = AsyncOpenAI(
    base_url=PORTKEY_GATEWAY_URL,
    api_key="YOUR_PORTKEY_API_KEY",
    default_headers=createHeaders(provider="@anthropic-prod")
)

agent = Agent(
    name="Writer",
    instructions="You write haikus.",
    model=OpenAIChatCompletionsModel(
        model="claude-sonnet-4-20250514",
        openai_client=client
    )
)
```

**Best for:** Mixed environments where different agents need different providers.
</Tab>
</Tabs>

## Production Features

### Observability

All agent interactions are automatically logged:

<Frame>
  <img src="/images/product/product-11-1.webp"/>
</Frame>

Add trace IDs to group related requests:

```python
client = AsyncOpenAI(
    base_url=PORTKEY_GATEWAY_URL,
    api_key="YOUR_PORTKEY_API_KEY",
    default_headers=createHeaders(
        provider="@openai-prod",
        trace_id="agent-session-123"
    )
)
```

Add metadata for filtering and analytics:

```python
default_headers=createHeaders(
    provider="@openai-prod",
    trace_id="homework-tutor",
    metadata={
        "agent_type": "tutor",
        "_user": "user_123",
        "environment": "production"
    }
)
```

<Frame>
  <img src="/images/metadata.png" alt="Analytics with metadata filters" />
</Frame>

### Reliability

Enable fallbacks, retries, and load balancing via [Configs](https://app.portkey.ai/configs). Attach to your API key or pass inline:

```python
client = AsyncOpenAI(
    base_url=PORTKEY_GATEWAY_URL,
    api_key="YOUR_PORTKEY_API_KEY",
    default_headers=createHeaders(
        config={
            "strategy": { "mode": "fallback" },
            "targets": [
                { "override_params": { "model": "@openai-prod/gpt-4o" } },
                { "override_params": { "model": "@anthropic-prod/claude-sonnet-4" } }
            ]
        }
    )
)
```

If GPT-4o fails, requests automatically retry with Claude.

<CardGroup cols="2">
  <Card title="Automatic Retries" icon="rotate" href="../../product/ai-gateway/automatic-retries">
    Handles temporary failures automatically
  </Card>
  <Card title="Request Timeouts" icon="clock" href="../../product/ai-gateway/request-timeouts">
    Prevent agents from hanging
  </Card>
  <Card title="Conditional Routing" icon="route" href="../../product/ai-gateway/conditional-routing">
    Route based on request attributes
  </Card>
  <Card title="Load Balancing" icon="scale-balanced" href="../../product/ai-gateway/load-balancing">
    Distribute across multiple keys
  </Card>
</CardGroup>

### Guardrails

Add input/output validation:

```python
default_headers=createHeaders(
    provider="@openai-prod",
    config={
        "input_guardrails": ["guardrail-id-xxx"],
        "output_guardrails": ["guardrail-id-yyy"]
    }
)
```

Guardrails can:
- Detect and redact PII
- Filter harmful content
- Validate response formats
- Apply custom business rules

<Card title="Guardrails Guide" icon="shield-check" href="/product/guardrails">
  PII detection, content filtering, and custom rules
</Card>

### Caching

Reduce costs with response caching:

<CodeGroup>

```python Simple Cache
default_headers=createHeaders(
    provider="@openai-prod",
    config={ "cache": { "mode": "simple" } }
)
```

```python Semantic Cache
default_headers=createHeaders(
    provider="@openai-prod",
    config={ "cache": { "mode": "semantic" } }
)
```

</CodeGroup>

## Switching Providers

Use any of 1600+ models by changing the provider:

```python
# OpenAI
createHeaders(provider="@openai-prod")
# Model: gpt-4o, gpt-4o-mini, o1, etc.

# Anthropic
createHeaders(provider="@anthropic-prod")
# Model: claude-sonnet-4-20250514, claude-3-5-haiku-20241022, etc.

# Google
createHeaders(provider="@google-prod")
# Model: gemini-2.0-flash, gemini-1.5-pro, etc.
```

Agent code stays the same—just update the model name to match the provider.

<Card title="Supported Providers" icon="server" href="/integrations/llms">
  See all 1600+ supported models
</Card>

## Handoffs and Multi-Agent

Portkey works seamlessly with OpenAI Agents' handoff system:

```python
from agents import Agent, Runner, set_default_openai_client, set_default_openai_api
from openai import AsyncOpenAI
from portkey_ai import PORTKEY_GATEWAY_URL, createHeaders

client = AsyncOpenAI(
    base_url=PORTKEY_GATEWAY_URL,
    api_key="YOUR_PORTKEY_API_KEY",
    default_headers=createHeaders(
        provider="@openai-prod",
        trace_id="homework-session"
    )
)
set_default_openai_client(client, use_for_tracing=False)
set_default_openai_api("chat_completions")

# Define specialist agents
math_tutor = Agent(
    name="Math Tutor",
    handoff_description="Specialist for math questions",
    instructions="Help with math problems. Show your work.",
    model="gpt-4o"
)

history_tutor = Agent(
    name="History Tutor",
    handoff_description="Specialist for history questions",
    instructions="Help with history questions. Provide context.",
    model="gpt-4o"
)

# Triage agent with handoffs
triage = Agent(
    name="Triage",
    instructions="Route to the appropriate tutor based on the question.",
    handoffs=[math_tutor, history_tutor],
    model="gpt-4o"
)

result = Runner.run_sync(triage, "What caused World War I?")
print(result.final_output)
```

All handoffs are tracked in the same trace on your Portkey dashboard.

## Tools

Portkey provides full observability for tool usage:

```python
from agents import Agent, Runner, function_tool

@function_tool
def get_weather(city: str) -> str:
    """Get weather for a city."""
    return f"72°F and sunny in {city}"

agent = Agent(
    name="Assistant",
    instructions="You can check the weather.",
    tools=[get_weather],
    model="gpt-4o"
)

result = Runner.run_sync(agent, "What's the weather in Tokyo?")
```

Tool calls, parameters, and responses are all logged.

## Prompt Templates

Use Portkey's prompt management for versioned prompts:

```python
from portkey_ai import Portkey

portkey = Portkey(api_key="YOUR_PORTKEY_API_KEY")

prompt_data = portkey.prompts.render(
    prompt_id="YOUR_PROMPT_ID",
    variables={"subject": "calculus"}
)

agent = Agent(
    name="Tutor",
    instructions=prompt_data.data.messages[0]["content"],
    model="gpt-4o"
)
```

<Card title="Prompt Engineering Studio" icon="wand-magic-sparkles" href="/product/prompt-library">
  Prompt versioning and collaboration
</Card>

## Enterprise Governance

Set up centralized control for OpenAI Agents across your organization.

<Steps>
<Step title="Add Provider with Budget">
Go to [Model Catalog](https://app.portkey.ai/model-catalog) → Add Provider. Set budget limits and rate limits per provider.
</Step>

<Step title="Create Config">
Go to [Configs](https://app.portkey.ai/configs):

```json
{
  "override_params": { "model": "@openai-prod/gpt-4o" }
}
```

Add fallbacks, guardrails, or routing as needed.
</Step>

<Step title="Create Team API Keys">
Go to [API Keys](https://app.portkey.ai/api-keys). Create keys per team, attach configs, and set permissions.
</Step>

<Step title="Distribute to Teams">
Teams use their Portkey API key—no raw provider keys needed:

```python
client = AsyncOpenAI(
    base_url=PORTKEY_GATEWAY_URL,
    api_key="TEAM_PORTKEY_API_KEY"  # Config attached to key
)
set_default_openai_client(client, use_for_tracing=False)
set_default_openai_api("chat_completions")
```
</Step>
</Steps>

Benefits:
- Rotate provider keys without code changes
- Per-team budgets and rate limits
- Centralized usage analytics
- Instant access revocation

<Card title="Enterprise Features" icon="building" href="/product/enterprise-offering">
  Governance, security, and compliance
</Card>

## FAQ

<AccordionGroup>
  <Accordion title="Can I use Portkey with existing OpenAI Agents apps?">
    Yes. Set the default client once—agent and tool code stays unchanged.
  </Accordion>

  <Accordion title="Does Portkey work with all OpenAI Agents features?">
    Yes. Handoffs, tools, guardrails, memory, streaming—all work.
  </Accordion>

  <Accordion title="How do I track multi-agent workflows?">
    Use a consistent `trace_id` across the workflow to see all agent interactions in one trace.
  </Accordion>

  <Accordion title="Can I use my own API keys?">
    Yes. Portkey stores your provider keys securely. Rotate keys without code changes.
  </Accordion>

  <Accordion title="Does Portkey support streaming?">
    Yes. Streaming responses work normally, and Portkey logs the complete interaction.
  </Accordion>
</AccordionGroup>

## Resources

<CardGroup cols="2">
  <Card title="OpenAI Agents Docs" icon="book" href="https://openai.github.io/openai-agents-python/">
    Official documentation
  </Card>
  <Card title="Book a Demo" icon="calendar" href="https://portkey.sh/openai-agents">
    Get implementation guidance
  </Card>
</CardGroup>
