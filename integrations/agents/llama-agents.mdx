---
title: "Llama Agents"
description: "Add observability, reliability, and multi-provider support to Llama Agents."
---

Portkey enhances Llama Agents with production features:

- Complete observability of agent steps and LLM interactions
- Built-in reliability with fallbacks, retries, and load balancing
- Access to 1600+ LLMs through a single integration
- Cost tracking and optimization

## Quick Start

```python
from llama_index.llms.openai import OpenAI
from portkey_ai import PORTKEY_GATEWAY_URL, createHeaders

llm = OpenAI(
    api_base=PORTKEY_GATEWAY_URL,
    api_key="YOUR_PORTKEY_API_KEY",
    default_headers=createHeaders(provider="@openai-prod")
)
```

All agent interactions are now logged in your [Portkey dashboard](https://app.portkey.ai/logs).

[![Open in Colab](/images/guides/colab-badge.svg)](https://git.new/llama-agents)

## Setup

<Steps>
<Step title="Install packages">
```bash
pip install -qU llama-agents llama-index portkey-ai
```
</Step>

<Step title="Add provider in Model Catalog">
Go to [Model Catalog â†’ Add Provider](https://app.portkey.ai/model-catalog). Select your provider, enter API keys, and name it (e.g., `openai-prod`).

Your provider slug is `@openai-prod`.
</Step>

<Step title="Get Portkey API Key">
Create an API key at [app.portkey.ai/api-keys](https://app.portkey.ai/api-keys).

**Pro tip:** Attach a default [config](https://app.portkey.ai/configs) for fallbacks and caching.
</Step>

<Step title="Configure LlamaIndex LLM">
```python
from llama_index.llms.openai import OpenAI
from portkey_ai import PORTKEY_GATEWAY_URL, createHeaders

llm = OpenAI(
    api_base=PORTKEY_GATEWAY_URL,
    api_key="YOUR_PORTKEY_API_KEY",
    default_headers=createHeaders(provider="@openai-prod")
)
```
</Step>
</Steps>

## Production Features

### Observability

Add trace IDs and metadata for filtering:

```python
default_headers=createHeaders(
    provider="@openai-prod",
    trace_id="research-agent-1",
    metadata={"agent_type": "research", "_user": "user_123"}
)
```

### Tracing with Callback Handler

Use the Portkey callback handler for detailed traces:

```python
from portkey_ai.llamaindex import LlamaIndexCallbackHandler

portkey_handler = LlamaIndexCallbackHandler(
    api_key="YOUR_PORTKEY_API_KEY",
    metadata={"session_id": "session_1", "agent_id": "research_agent_1"}
)

llm = OpenAI(
    api_key="YOUR_OPENAI_API_KEY",
    callbacks=[portkey_handler]
)
```

<Frame>
  <img src="/images/autogen/autogen-5.gif"/>
</Frame>

### Reliability

Enable fallbacks via [Configs](https://app.portkey.ai/configs):

```json
{
  "strategy": { "mode": "fallback" },
  "targets": [
    { "override_params": { "model": "@openai-prod/gpt-4o" } },
    { "override_params": { "model": "@anthropic-prod/claude-sonnet-4" } }
  ]
}
```

### Caching

Reduce costs with response caching:

```json
{
  "cache": { "mode": "semantic" }
}
```

### Switching Providers

Change the provider to switch models:

```python
# OpenAI
createHeaders(provider="@openai-prod")

# Anthropic
createHeaders(provider="@anthropic-prod")

# Azure OpenAI
createHeaders(provider="@azure-prod")
```

<Card title="Supported Providers" icon="server" href="/integrations/llms">
  See all 1600+ supported models
</Card>

## Next Steps

<CardGroup cols={2}>
  <Card title="Model Catalog" icon="database" href="/product/model-catalog">
    Set up providers and budgets
  </Card>
  <Card title="Configs" icon="gear" href="/product/ai-gateway/configs">
    Fallbacks, caching, and routing
  </Card>
</CardGroup>
