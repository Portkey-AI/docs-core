---
title: "Swarm by OpenAI"
description: "Use Portkey with OpenAI Swarm to take your AI Agents to production"
---


# Getting Started
<Steps>
  <Step title="1. Install the Portkey SDK ">
   ```sh
   pip install -U portkey-ai
   ```
  </Step>
  <Step title=" Import the necessary">
   ```py
   from portkey_ai import Portkey
   ```
  </Step>
  <Step title="Pass the Portkey Client in Swarm class">
    ```py
    portkey = Portkey(
    api_key="YOUR_PORTKEY_API_KEY", # defaults to os.environ.get("OPENAI_API_KEY")
    virtual_key="YOUR_VIRTUAL_KEY" # defaults to os.environ.get("PORTKEY_API_KEY")
    )

    client = Swarm(client=portkey)

    ```
  </Step>
</Steps>

That's it! This is all you need to make Portkey work with Agents Swarm ðŸ


# E2E Function calling Example
Here is a basic example on using the function calling in openAI swarm with Portkey's client.

```py
from swarm import Swarm, Agent
from portkey_ai import Portkey

portkey = Portkey(
    api_key="01jOHZchZAk8QyM1g1T+QnAiLznQ", # defaults to os.environ.get("OPENAI_API_KEY")
    virtual_key="main-258f4d" # defaults to os.environ.get("PORTKEY_API_KEY")
    )

client = Swarm(client=portkey)


def get_weather(location) -> str:
    return "{'temp':67, 'unit':'F'}"


agent = Agent(
    name="Agent",
    instructions="You are a helpful agent.",
    functions=[get_weather],
)

messages = [{"role": "user", "content": "What's the weather in NYC?"}]

response = client.run(agent=agent, messages=messages)
print(response.messages[-1]["content"])

```

> The weather in New York City is currently 67Â°F.

In a nutshell, Portkey extends the familiar OpenAI schema to in Swarm to work with **200+ LLMs** without the need for importing different classes for each provider or having to configure your code separately. Portkey makes your Swarm Agents more _reliable_, _robust_, and _production-ready_.



## Enabling Portkey Features

By using Portkey client with Swarm, you get access to the following production-grade features:

<CardGroup cols="1">
  <Card title="Interoperability" href="/integrations/libraries/llama-index-python#id-1.-interoperability-calling-anthropic-gemini-mistral-and-more">
    <p>Call various LLMs like Anthropic, Gemini, Mistral, Azure OpenAI, Google Vertex AI, and AWS Bedrock with minimal code changes.</p>
  </Card>

  <Card title="Caching" href="/integrations/libraries/llama-index-python#id-2.-caching">
    <p>Speed up your requests and save money on LLM calls by storing past responses in the Portkey cache. Choose between Simple and Semantic cache modes.</p>
  </Card>

  <Card title="Reliability" href="/integrations/libraries/llama-index-python#id-3.-reliability">
    <p>Set up fallbacks between different LLMs or providers, load balance your requests across multiple instances or API keys, set automatic retries, and request timeouts.</p>
  </Card>

  <Card title="Key Metrics like Cost, Tokents, Latency, etc." href="/integrations/libraries/llama-index-python#id-4.-observability">
    <p>Portkey automatically logs all the key details about your requests, including cost, tokens used, response time, request and response bodies, and more. Send custom metadata and trace IDs for better analytics and debugging.</p>
  </Card>

  <Card title="Logs and Traces" href="/integrations/libraries/llama-index-python#id-6.-continuous-improvement">
    <p>Improve your LlamaIndex app by capturing qualitative & quantitative user feedback on your requests.</p>
  </Card>

  <Card title="Prompt Management" href="/integrations/libraries/llama-index-python#id-5.-prompt-management">
    <p>Use Portkey as a centralized hub to store, version, and experiment with prompts across multiple LLMs, and seamlessly retrieve them in your Agents Swarm code for easy integration.</p>
  </Card>

  <Card title="Continuous Improvement" href="/integrations/libraries/llama-index-python#id-6.-continuous-improvement">
    <p>Improve your LlamaIndex app by capturing qualitative & quantitative user feedback on your requests.</p>
  </Card>
  <Card title="Guardrails" href="/integrations/libraries/llama-index-python#id-7.-security-and-compliance">
    <p>Set up input & output checks like PII detection, ReGeX match, adhering JSON schema and more. </p>
  </Card>
    <Card title="Security & Compliance" href="/integrations/libraries/llama-index-python#id-7.-security-and-compliance">
    <p>Set budget limits on provider API keys and implement fine-grained user roles and permissions for both the app and the Portkey APIs.</p>
  </Card>
</CardGroup>

Many of these features are driven by **Portkey's Config architecture**. On the Portkey app, we make it easy to help you _create_, _manage_, and _version_ your Configs so that you can reference them easily in your Agent code.


## 1\. Interoperability - Calling Anthropic, Gemini, Mistral, and more

Now that we have the our Agent up and running, let's see how we can easily switch between 200+ LLMs. Call various LLMs such as Anthropic, Gemini, Mistral, Azure OpenAI, Google Vertex AI, AWS Bedrock, and many more by simply changing the `virtual_key` in `portkey_client`.

Portkeyâ€™s virtual key system allows you to securely store your LLM API keys in our vault, utilizing a unique virtual identifier to streamline API key management


<Tabs>
  <Tab title="OpenAI to Azure OpenAI">

```python
from swarm import Swarm, Agent
from portkey_ai import Portkey

portkey = Portkey(
    api_key="YOUR_PORTKEY_API_KEY", # defaults to os.environ.get("PORTKEY_API_KEY")
    virtual_key="YOUR_OPENAI_VIRTUAL_KEY" # defaults to os.environ.get("PORTKEY_API_KEY")
    )

client = Swarm(client=portkey)


def get_weather(location) -> str:
    return "{'temp':67, 'unit':'F'}"


agent = Agent(
    name="Agent",
    instructions="You are a helpful agent.",
    functions=[get_weather],
)

messages = [{"role": "user", "content": "What's the weather in NYC?"}]

response = client.run(agent=agent, messages=messages)
print(response.messages[-1]["content"])
```

now if you want to switch to Azure OpenAI just change your virtual key:

```py
portkey = Portkey(
    api_key="YOUR_PORTKEY_API_KEY", # defaults to os.environ.get("PORTKEY_API_KEY")
    virtual_key="YOUR_AZURE_VIRTUAL_KEY" # defaults to os.environ.get("PORTKEY_API_KEY")
    )
```

  </Tab>

</Tabs>


<Info>
[**Explore the Virtual Key documentation here**](/product/ai-gateway/virtual-keys)**.**
</Info>



### Using Local or Privately Hosted Models like Ollama with your Agents

Agent runs can become expensive. One way you can reduce the LLM costs is by using privately hosted LLMs. Portkey allows you to connect with any OpeAI API compatible LLM.

Check out [**Portkey docs for Ollama**](/integrations/llms/ollama) and [**other privately hosted models**](/integrations/llms/byollm).

```py Ollama
from portkey_ai import Portkey

portkey = Portkey(
    api_key="PORTKEY_API_KEY",
    provider="openai",
    custom_host="your-custom-host-url" # Your vLLM ngrok URL
    Authorization="AUTH_KEY", # If you need to pass auth
)


client = Swarm(client=portkey)


def get_weather(location) -> str:
    return "{'temp':67, 'unit':'F'}"


agent = Agent(
    name="Agent",
    instructions="You are a helpful agent.",
    functions=[get_weather],
)

messages = [{"role": "user", "content": "What's the weather in NYC?"}]

response = client.run(agent=agent, messages=messages)
print(response.messages[-1]["content"])
---
```

## 2\. Caching

Agent runs are time-consuming and expensive due to their complex pipelines. Caching can significantly reduce these costs by storing frequently used data and responses. Portkey offers a built-in caching system that stores past responses, reducing the need for agent calls saving both time and money.


You can speed up your requests and save money on your LLM requests by storing past responses in the Portkey cache. There are 2 cache modes:

* **Simple:** Matches requests verbatim. Perfect for repeated, identical prompts. Works on **all models** including image generation models.
* **Semantic:** Matches responses for requests that are semantically similar. Ideal for denoising requests with extra prepositions, pronouns, etc.

To enable Portkey cache, just add the `cache` params to your [config object](https://portkey.ai/docs/api-reference/config-object#cache-object-details).

<Tabs>
  <Tab title="Simple Cache Config">

```python
config = {
    "virtual_key": "YOUR_VIRTUAL_KEY"
    "cache": {
        "mode": "simple",
        "max_age": 60000
  }
}
```

  </Tab>
  <Tab title="Semantic Cache Config">

```python
config = {
    "virtual_key": "YOUR_VIRTUAL_KEY"
    "cache": {
        "mode": "semantic",
        "max_age": 60000
  }
}
```
  </Tab>
</Tabs>
[**For more cache settings, check out the documentation here**](/product/ai-gateway/cache-simple-and-semantic)**.**

---

## 3\. Reliability

Agents are _brittle_. Long agentic pipelines with multiple steps can fail at any stage, disrupting the entire process. Portkey solves this by offering built-in **fallbacks** between different LLMs or providers, **load-balancing** across multiple instances or API keys, and implementing automatic **retries** and request **timeouts**. This makes your agents more reliable and resilient.

Here's how you can implement these features using Portkey's config


<Tabs>

  <Tab title="Fallback from OpenAI to Anthropic">

```python
config = {
    "strategy": {
      "mode": "fallback"
    },
    "targets": [
    {
      "virtual_key": "openai-virtual-key",
      "override_params": {
        "model": "gpt-4o"
      }
    },
    {
      "virtual_key": "anthropic-virtual-key",
      "override_params": {
          "model": "claude-3-opus-20240229",
          "max_tokens":64
      }
    }
  ]
}
```
  </Tab>
    <Tab title="Load Balance 2 API Keys">

```python
config = {
    "strategy": {
      "mode": "loadbalance"
    },
    "targets": [
    {
      "virtual_key": "openai-virtual-key-1",
      "weight":1
    },
    {
      "virtual_key": "openai-virtual-key-2",
      "weight":1
    }
  ]
}
```
  </Tab>
    <Tab title="Automatic Retries">

```python
config = {
    "retry": {
        "attempts": 5
    },
    "virtual_key": "virtual-key-xxx"
}
```
  </Tab>
  <Tab title="Request Timeouts">

```python
config = {
  "strategy": { "mode": "fallback" },
  "request_timeout": 10000,
  "targets": [
    { "virtual_key": "open-ai-xxx" },
    { "virtual_key": "azure-open-ai-xxx" }
  ]
}
```
  </Tab>

       <Tab title="Conditional Routing">
    sighs!!
    </Tab>

</Tabs>

Explore deeper documentation for each feature here - [**Fallbacks**](/product/ai-gateway/fallbacks), [**Loadbalancing**](/product/ai-gateway/load-balancing), [**Retries**](/product/ai-gateway/automatic-retries), [**Timeouts**](/product/ai-gateway/request-timeouts).


## 4\. Observability

Agent runs are complex. Logs are essential for diagnosing issues, understanding agent behavior, and improving performance. They provide a detailed record of agent activities and tool use, which is crucial for debugging and optimizing processes.
Portkey offers comprehensive logging features that capture detailed information about every action and decision made by your AI agents. Access a dedicated section to view records of agent executions, including parameters, outcomes, function calls, and errors. Filter logs based on multiple parameters such as trace ID, model, tokens used, and metadata.
Automatically log all the key details about your requests, including cost, tokens used, response time, request and response bodies, and more. 


Agent runs can be costly. Tracking agent metrics is crucial for understanding the performance and reliability of your AI agents. Metrics help identify issues, optimize runs, and ensure that your agents meet their intended goals.

Portkey automatically logs comprehensive metrics for your AI agents, including **cost**, **tokens used**, **latency**, etc. Whether you need a broad overview or granular insights into your agent runs, Portkey's customizable filters provide the metrics you need. For agent-specific observability, add `Trace-id` to the request headers for each agent.







Using Portkey, you can also send custom metadata with each of your requests to further segment your logs for better analytics. Similarly, you can also trace multiple requests to a single trace ID and filter or view them separately in Portkey logs.

**Custom Metadata and Trace ID information is sent in**  **`default_headers`** **.**

<Tabs>
  <Tab title="Sending Custom Metadata">

```python



from swarm import Swarm, Agent
from portkey_ai import Portkey

portkey = Portkey(
    api_key="01jOHZchZAk8QyM1g1T+QnAiLznQ", # defaults to os.environ.get("OPENAI_API_KEY")
    virtual_key="main-258f4d",
    metadata={
            "_user": "USER_ID",
            "environment": "production",
            "session_id": "1729"
        },
    config="portkey-config" # defaults to os.environ.get("PORTKEY_API_KEY")
    )

client = Swarm(client=portkey)


def get_weather(location) -> str:
    return "{'temp':67, 'unit':'F'}"


agent = Agent(
    name="Agent",
    instructions="You are a helpful agent.",
    functions=[get_weather],
)


messages = [{"role": "user", "content": "What's the weather in NYC?"}]

response = client.run(agent=agent, messages=messages)
print(response.messages[-1]["content"])
```

  </Tab>
  <Tab title="Sending Trace ID">

```python
from swarm import Swarm, Agent
from portkey_ai import Portkey

portkey = Portkey(
    api_key="01jOHZchZAk8QyM1g1T+QnAiLznQ", # defaults to os.environ.get("OPENAI_API_KEY")
    virtual_key="main-258f4d",
    trace_id="YOUR_TRACE_ID_HERE"
    config="portkey-config" # defaults to os.environ.get("PORTKEY_API_KEY")
    )

client = Swarm(client=portkey)


def get_weather(location) -> str:
    return "{'temp':67, 'unit':'F'}"


agent = Agent(
    name="Agent",
    instructions="You are a helpful agent.",
    functions=[get_weather],
)


messages = [{"role": "user", "content": "What's the weather in NYC?"}]

response = client.run(agent=agent, messages=messages)
print(response.messages[-1]["content"])
```
  </Tab>
</Tabs>

#### Portkey shows these details separately for each log:

<Frame>
    <img src="/images/libraries/libraries-4.avif"/>
</Frame>

[**Check out Observability docs here.**](/product/observability)


## 5\. Prompt Management

Portkey features an advanced Prompts platform tailor-made for better prompt engineering. With Portkey, you can:

* **Store Prompts with Access Control and Version Control:** Keep all your prompts organized in a centralized location, easily track changes over time, and manage edit/view permissions for your team.
* **Parameterize Prompts**: Define variables and [mustache-approved tags](/product/prompt-library/prompt-templates#templating-engine) within your prompts, allowing for dynamic value insertion when calling LLMs. This enables greater flexibility and reusability of your prompts.
* **Experiment in a Sandbox Environment**: Quickly iterate on different LLMs and parameters to find the optimal combination for your use case, without modifying your LlamaIndex code.

#### Here's how you can leverage Portkey's Prompt Management in your LlamaIndex application:

1. Create your prompt template on the Portkey app, and save it to get an associated `**Prompt ID`**
2. Before making a Llamaindex request, render the prompt template using the Portkey SDK
3. Transform the retrieved prompt to be compatible with LlamaIndex and send the request!

#### Example: Using a Portkey Prompt Template in LlamaIndex

```py Portkey Prompts in LlamaIndex
import json
import os
from llama_index.llms.openai import OpenAI
from llama_index.core.llms import ChatMessage
from portkey_ai import PORTKEY_GATEWAY_URL, createHeaders, Portkey

###  Initialize Portkey client with API key

client = Portkey(api_key=os.environ.get("PORTKEY_API_KEY"))

### Render the prompt template with your prompt ID and variables

prompt_template = client.prompts.render(
    prompt_id="pp-prompt-id",
    variables={ "movie":"Dune 2" }
).data.dict()

config = {
    "virtual_key":"GROQ_VIRTUAL_KEY", # You need to send the virtual key separately
    "override_params":{
        "model":prompt_template["model"], # Set the model name based on the value in the prompt template
        "temperature":prompt_template["temperature"] # Similarly, you can also set other model params
    }
}

portkey = OpenAI(
    api_base=PORTKEY_GATEWAY_URL,
    default_headers=createHeaders(
        api_key=os.environ.get("PORTKEY_API_KEY"),
        config=config
    )
)

### Transform the rendered prompt into LlamaIndex-compatible format

messages = [ChatMessage(content=msg["content"], role=msg["role"]) for msg in prompt_template["messages"]]

resp = portkey.chat(messages)

print(resp)



from swarm import Swarm, Agent
from portkey_ai import Portkey


config = {
    "virtual_key":"GROQ_VIRTUAL_KEY", # You need to send the virtual key separately
    "override_params":{
        "model":prompt_template["model"], # Set the model name based on the value in the prompt template
        "temperature":prompt_template["temperature"] # Similarly, you can also set other model params
    }
}


portkey = Portkey(
    api_key="01jOHZchZAk8QyM1g1T+QnAiLznQ", # defaults to os.environ.get("OPENAI_API_KEY")
    virtual_key="main-258f4d",
    trace_id="YOUR_TRACE_ID_HERE"
    config="portkey-config" # defaults to os.environ.get("PORTKEY_API_KEY")
    )

client = Swarm(client=portkey)


def get_weather(location) -> str:
    return "{'temp':67, 'unit':'F'}"


agent = Agent(
    name="Agent",
    instructions="You are a helpful agent.",
    functions=[get_weather],
)


messages = [{"role": "user", "content": "What's the weather in NYC?"}]

response = client.run(agent=agent, messages=messages)
print(response.messages[-1]["content"])



```

[**Explore Prompt Management docs here**](/product/prompt-library).

---

## 6\. Continuous Improvement

Now that you know how to trace & log your Agent requests to Portkey, you can also start capturing user feedback to improve your agent! Improve your Agent runs by capturing qualitative & quantitative user feedback on your requests. Portkey's Feedback APIs provide a simple way to get weighted feedback from customers on any request you served, at any stage in your app. You can capture this feedback on a request or conversation level and analyze it by adding meta data to the relevant request.


You can append qualitative as well as quantitative feedback to any `trace ID` with the `portkey.feedback.create` method:

```py Adding Feedback
from portkey_ai import Portkey

portkey = Portkey(
    api_key="PORTKEY_API_KEY"
)

feedback = portkey.feedback.create(
    trace_id="YOUR_LLAMAINDEX_TRACE_ID",
    value=5,  # Integer between -10 and 10
    weight=1,  # Optional
    metadata={
        # Pass any additional context here like comments, _user and more
    }
)

print(feedback)
```

[**Check out the Feedback documentation for a deeper dive**](/product/observability/feedback).


## 7\. Security & Compliance

When you onboard more team members to help out on your Llamaindex app - permissioning, budgeting, and access management can become a mess! Using Portkey, you can set **budget limits** on provide API keys and implement **fine-grained user roles** and **permissions** to:

* **Control access**: Restrict team members' access to specific features, Configs, or API endpoints based on their roles and responsibilities.
* **Manage costs**: Set budget limits on API keys to prevent unexpected expenses and ensure that your LLM usage stays within your allocated budget.
* **Ensure compliance**: Implement strict security policies and audit trails to maintain compliance with industry regulations and protect sensitive data.
* **Simplify onboarding**: Streamline the onboarding process for new team members by assigning them appropriate roles and permissions, eliminating the need to share sensitive API keys or secrets.
* **Monitor usage**: Gain visibility into your team's LLM usage, track costs, and identify potential security risks or anomalies through comprehensive monitoring and reporting.

<Frame>
    <img src="/images/libraries/libraries-5.webp"/>
</Frame>

[**Read more about Portkey's Security & Enterprise offerings here**](/product/enterprise-offering).



## Saving Configs in the Portkey App

Head over to the Configs tab in Portkey app where you can save various provider Configs along with the reliability and caching features. Each Config has an associated slug that you can reference in your OpenAI Swarm code.

<Frame>
    <img src="/images/libraries/libraries-3.avif"/>
</Frame>
## Overriding a Saved Config

If you want to use a saved Config from the Portkey app in your Agent's code but need to modify certain parts of it before making a request, you can easily achieve this using Portkey's Configs API. This approach allows you to leverage the convenience of saved Configs while still having the flexibility to adapt them to your specific needs.

#### Here's an example of how you can fetch a saved Config using the Configs API and override the `model` parameter:

```py Overriding Model in a Saved Config
from llama_index.llms.openai import OpenAI
from llama_index.core.llms import ChatMessage
from portkey_ai import PORTKEY_GATEWAY_URL, createHeaders
import requests
import os

def create_config(config_slug,model):
    url = f'https://api.portkey.ai/v1/configs/{config_slug}'
    headers = {
        'x-portkey-api-key': os.environ.get("PORTKEY_API_KEY"),
        'content-type': 'application/json'
    }
    response = requests.get(url, headers=headers).json()
    config = json.loads(response['config'])
    config['override_params']['model']=model
    return config

config=create_config("pc-llamaindex-xx","gpt-4-turbo")

portkey = OpenAI(
    api_base=PORTKEY_GATEWAY_URL,
    default_headers=createHeaders(
        api_key=os.environ.get("PORTKEY_API_KEY"),
        config=config
    )
)

messages = [ChatMessage(role="user", content="1729")]

resp = portkey.chat(messages)
print(resp)
```

In this example:

1. We define a helper function `get_customized_config` that takes a `config_slug` and a `model` as parameters.
2. Inside the function, we make a GET request to the Portkey Configs API endpoint to fetch the saved Config using the provided `config_slug`.
3. We extract the `config` object from the API response.
4. We update the `model` parameter in the `override_params` section of the Config with the provided `custom_model`.
5. Finally, we return the customized Config.

We can then use this customized Config when initializing the OpenAI client from LlamaIndex, ensuring that our specific `model` override is applied to the saved Config.

For more details on working with Configs in Portkey, refer to the [**Config documentation**.](/product/ai-gateway/configs)

---



## Join Portkey Community

Join the Portkey Discord to connect with other practitioners, discuss your LlamaIndex projects, and get help troubleshooting your queries.

[**Link to Discord**](https://portkey.ai/community)

For more detailed information on each feature and how to use them, please refer to the [Portkey Documentation](https://portkey.ai/docs).