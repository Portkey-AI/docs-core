---
title: "Strands Agents"
description: "Use Portkey with AWS's Strands Agents to take your AI Agents to production"
---

Strands Agents is a simple-to-use agent framework built by AWS.

Portkey enhances Strands Agents with production-readiness features, turning your experimental agents into robust systems by providing:

- **Complete observability** of every agent step, tool use, and interaction
- **Built-in reliability** with fallbacks, retries, and load balancing
- **Cost tracking and optimization** to manage your AI spend
- **Access to 200+ LLMs** through a single integration
- **Guardrails** to keep agent behavior safe and compliant
- **Version-controlled prompts** for consistent agent performance

<Card title="Strands Agents Documentation" href="https://strandsagents.com/">
  Learn more about Strands Agents' core concepts and features
</Card>

<Steps>
  <Step title="Quickstart: Install">
```bash
pip install -U strands-agents strands-agents-tools openai portkey-ai
```
  </Step>
  <Step title="Quickstart: Configure">
Instantiate your Strands `OpenAIModel` with Portkey:
```python
from strands.models.openai import OpenAIModel
from portkey_ai import PORTKEY_GATEWAY_URL

model = OpenAIModel(
  client_args={"api_key": "YOUR_PORTKEY_API_KEY", "base_url": PORTKEY_GATEWAY_URL},
  model_id="gpt-4o",
  params={"temperature": 0.7}
)
```
  </Step>
  <Step title="Quickstart: Run">
```python
from strands import Agent
from strands_tools import calculator

agent = Agent(model=model, tools=[calculator])
response = agent("What is 2+2")
print(response)
```
  </Step>
</Steps>

## Integration

Portkey works out of the box with Strands Agents and supports all of Portkey + Strands functionalities because of our end-to-end support for the OpenAI API. You can directly import the OpenAI Model class inside Strands, set the base URL to Portkey Gateway URL, and unlock all of Portkey functionalities. Here's how:

### Portkey Setup

First, let's setup your provider keys and settings on Portkey, that you can later use in Strands with your Portkey API key.

<Steps>
<Step title="Create Provider">
Go to [Virtual Keys](https://app.portkey.ai/virtual-keys) in the Portkey App to add your AI provider key and copy the virtual key ID.
</Step>

<Step title="Create Config">
Go to [Configs](https://app.portkey.ai/configs) in the Portkey App, create a new config that uses your virtual key, then save the Config ID.
</Step>

<Step title="Create API Key">
Go to [API Keys](https://app.portkey.ai/api-keys) in the Portkey App to generate a new API key and attach your Config as the default routing.
</Step>
</Steps>

That's it! With this, you unlock all Portkey functionalities for use with your Strands Agents!

### Strands Setup

Now, let's setup Strands Agents to use the Portkey API key we just created.

<Steps>
<Step title="Install Packages">
```bash
pip install -U strands-agents strands-agents-tools openai portkey-ai
```
</Step>

<Step title="Configure Portkey Client">

When you instantiate the `OpenAIModel`, set the `base_url` to Portkey's Gateway URL and pass your Portkey API Key directly in as the main API key. 
```python
from strands.models.openai import OpenAIModel
from portkey_ai import PORTKEY_GATEWAY_URL

portkey_model = OpenAIModel(
    client_args={
        "api_key": "YOUR_PORTKEY_API_KEY",
        "base_url": PORTKEY_GATEWAY_URL
    },
    model_id="gpt-4o",
    params={
        "temperature": 0.7
    }
)
```

That's it! With this, you unlock all Portkey functionalities to be used along with your Strands Agents!
</Step>
<Step title="View the Log">
Portkey logs all of your Strands requests in the [Logs dashboard](https://app.portkey.ai/logs).
<Frame><img src="PATH_TO_PLACEHOLDER" alt="Portkey Logs screenshot"/></Frame>
</Step>
</Steps>

## End-to-end Example

```python
from strands import Agent
from strands.models.openai import OpenAIModel
from strands_tools import calculator
from portkey_ai import PORTKEY_GATEWAY_URL

model = OpenAIModel(
    client_args={
        "api_key": "YOUR_PORTKEY_API_KEY",
        "base_url": PORTKEY_GATEWAY_URL
    },
    model_id="gpt-4o",
    params={
        "max_tokens": 1000,
        "temperature": 0.7,
    }
)

agent = Agent(model=model, tools=[calculator])
response = agent("What is 2+2")
print(response)
```

We've demonstrated a simple working integration between Portkey & Strands. Check below for all the advanced functionalities Portkey offers for your Strands Agents.

## Production Features

### 1. Enhanced Observability

Portkey provides comprehensive observability for your Strands agents, helping you understand exactly what's happening during each execution.

<Tabs>
  <Tab title="Traces">
<Frame>
    <img src="/images/product/product-11-1.webp"/>
</Frame>

Traces provide a hierarchical view of your agent's execution, showing the sequence of LLM calls, tool invocations, and state transitions.

```python {10}
from strands import Agent
from strands.models.openai import OpenAIModel
from strands_tools import calculator
from portkey_ai import PORTKEY_GATEWAY_URL,createHeaders

model = OpenAIModel(
    client_args={
        "api_key": "YOUR_PORTKEY_API_KEY",
        "base_url": PORTKEY_GATEWAY_URL,
        "default_headers":createHeaders(trace_id="strands")
    },
    model_id="gpt-4o",
    params={
        "max_tokens": 1000,
        "temperature": 0.7,
    }
)

agent = Agent(model=model, tools=[calculator])
response = agent("What is 2+2")
print(response)
```
  </Tab>

  <Tab title="Logs">
<Frame>
    <img src="/images/product/product-2.avif"/>
</Frame>

Portkey logs every interaction with LLMs, including:

- Complete request and response payloads
- Latency and token usage metrics
- Cost calculations
- Tool calls and function executions

All logs can be filtered by metadata, trace IDs, models, and more, making it easy to debug specific agent runs.
  </Tab>

  <Tab title="Metrics & Dashboards">
<Frame>
    <img src="/images/product/dashboard.png"/>
</Frame>

Portkey provides built-in dashboards that help you:

- Track cost and token usage across all agent runs
- Analyze performance metrics like latency and success rates
- Identify bottlenecks in your agent workflows
- Compare different agent configurations and LLMs

You can filter and segment all metrics by custom metadata to analyze specific agent types, user groups, or use cases.
  </Tab>

  <Tab title="Metadata Filtering">
<Frame>
  <img src="/images/metadata.png" alt="Analytics with metadata filters" />
</Frame>

Add custom metadata to your Strands calls to enable powerful filtering and segmentation:

```python
from strands import Agent
from strands.models.openai import OpenAIModel
from strands_tools import calculator
from portkey_ai import PORTKEY_GATEWAY_URL,createHeaders

model = OpenAIModel(
    client_args={
        "api_key": "YOUR_PORTKEY_API_KEY",
        "base_url": PORTKEY_GATEWAY_URL,
        "default_headers":createHeaders(
            trace_id="strands",
            metadata={
                "agent_type": "strands_agent",
                "environment": "production",
                "_user": "user_123",
                "request_source": "mobile_app"
            }
        )
    },
    model_id="gpt-4o",
    params={
        "max_tokens": 1000,
        "temperature": 0.7,
    }
)

agent = Agent(model=model, tools=[calculator])
response = agent("What is 2+2")
print(response)
```

This metadata can be used to filter logs, traces, and metrics on the Portkey dashboard, allowing you to analyze specific agent runs, users, or environments.
  </Tab>
</Tabs>

### 2. Reliability - Keep Your Agents Running Smoothly

When running agents in production, things can go wrong - API rate limits, network issues, or provider outages. Portkey's reliability features ensure your agents keep running smoothly even when problems occur.

It's simple to enable fallback in your Strands Agents by using a Portkey Config that you can attach at runtime or directly to your Portkey API key. Here's an example of attaching a Config at runtime:

```python {11-20}
from strands import Agent
from strands.models.openai import OpenAIModel
from strands_tools import calculator
from portkey_ai import PORTKEY_GATEWAY_URL,createHeaders

model = OpenAIModel(
    client_args={
        "api_key": "YOUR_PORTKEY_API_KEY",
        "base_url": PORTKEY_GATEWAY_URL,
        "default_headers":createHeaders(
            config={
                "strategy": {
                    "mode": "fallback",
                    "on_status_codes": [429]
                },
                "targets": [
                    { "provider":"@azure-81fddb" },
                    { "provider":"@open-ai-key-66a67d" }
                ]
            }
        )
    },
    model_id="gpt-4o",
    params={
        "max_tokens": 1000,
        "temperature": 0.7,
    }
)

agent = Agent(model=model, tools=[calculator])
response = agent("What is 2+2")
print(response)
```

This configuration will automatically try GPT-4o on OpenAI if the Azure deployment fails, ensuring your agent can continue operating.

<CardGroup cols="2">
  <Card title="Automatic Retries" icon="rotate" href="../../product/ai-gateway/automatic-retries">
    Handles temporary failures automatically. If an LLM call fails, Portkey will retry the same request for the specified number of times - perfect for rate limits or network blips.
  </Card>
  <Card title="Request Timeouts" icon="clock" href="../../product/ai-gateway/request-timeouts">
    Prevent your agents from hanging. Set timeouts to ensure you get responses (or can fail gracefully) within your required timeframes.
  </Card>
  <Card title="Conditional Routing" icon="route" href="../../product/ai-gateway/conditional-routing">
    Send different requests to different providers. Route complex reasoning to GPT-4, creative tasks to Claude, and quick responses to Gemini based on your needs.
  </Card>
  <Card title="Fallbacks" icon="shield" href="../../product/ai-gateway/fallbacks">
    Keep running even if your primary provider fails. Automatically switch to backup providers to maintain availability.
  </Card>
  <Card title="Load Balancing" icon="scale-balanced" href="../../product/ai-gateway/load-balancing">
    Spread requests across multiple API keys or providers. Great for high-volume agent operations and staying within rate limits.
  </Card>
</CardGroup>

### 3. Guardrails for Safe Agents

Guardrails ensure your Strands agents operate safely and respond appropriately in all situations.

**Why Use Guardrails?**

Strands agents can experience various failure modes:
- Generating harmful or inappropriate content
- Leaking sensitive information like PII
- Hallucinating incorrect information
- Generating outputs in incorrect formats

Portkey's guardrails can:
- Detect and redact PII in both inputs and outputs
- Filter harmful or inappropriate content
- Validate response formats against schemas
- Check for hallucinations against ground truth
- Apply custom business logic and rules

<Card title="Learn More About Guardrails" icon="shield-check" href="/product/guardrails">
  Explore Portkey's guardrail features to enhance agent safety
</Card>

### 4. Model Interoperability

Strands supports multiple LLM providers, and Portkey extends this capability by providing access to over 200 LLMs through a unified interface. You can easily switch between different models without changing your core agent logic:

```python
from strands import Agent
from strands.models.openai import OpenAIModel
from strands_tools import calculator
from portkey_ai import PORTKEY_GATEWAY_URL,createHeaders

model = OpenAIModel(
    client_args={
        "api_key": "YOUR_PORTKEY_API_KEY",
        "base_url": PORTKEY_GATEWAY_URL,
        "default_headers":createHeaders(
            provider="anthropic",
            api_key="ANTHROPIC_API_KEY"
        )
    },
    model_id="claude-3-7-sonnet-latest",
    params={
        "max_tokens": 1000,
        "temperature": 0.7,
    }
)

agent = Agent(model=model, tools=[calculator])
response = agent("What is 2+2")
print(response)
```

Portkey provides access to LLMs from providers including:

- OpenAI (GPT-4o, GPT-4 Turbo, etc.)
- Anthropic (Claude 3.5 Sonnet, Claude 3 Opus, etc.)
- Mistral AI (Mistral Large, Mistral Medium, etc.)
- Google Vertex AI (Gemini 1.5 Pro, etc.)
- Cohere (Command, Command-R, etc.)
- AWS Bedrock (Claude, Titan, etc.)
- Local/Private Models

<Card title="Supported Providers" icon="server" href="/integrations/llms">
  See the full list of LLM providers supported by Portkey
</Card>

## Enterprise Governance

**Why Enterprise Governance?**
If you are using Strands inside your organization, you need to consider several governance aspects:
- **Cost Management**: Controlling and tracking AI spending across teams
- **Access Control**: Managing which teams can use specific models
- **Usage Analytics**: Understanding how AI is being used across the organization
- **Security & Compliance**: Maintaining enterprise security standards
- **Reliability**: Ensuring consistent service across all users

## Enterprise Governance

<Steps>
  <Step title="1. Create a Virtual Key">
Define budget and rate limits with a Virtual Key in the Portkey App.
<Frame><img src="PATH_TO_PLACEHOLDER" alt="Create Virtual Key screenshot"/></Frame>
For SSO/SCIM setup, see @[product/enterprise-offering/org-management/sso.mdx] and @[product/enterprise-offering/org-management/scim/scim.mdx].
  </Step>
  <Step title="2. Create a Config">
Configure routing, fallbacks, and overrides.
<Frame><img src="PATH_TO_PLACEHOLDER" alt="Create Config screenshot"/></Frame>
  </Step>
  <Step title="3. Create an API Key">
Assign scopes and metadata defaults.
<Frame><img src="PATH_TO_PLACEHOLDER" alt="Create API Key screenshot"/></Frame>
  </Step>
  <Step title="4. Deploy & Monitor">
Distribute keys and track usage in the Portkey dashboard.
<Frame><img src="PATH_TO_PLACEHOLDER" alt="Monitor screenshot"/></Frame>
View audit logs: @[product/enterprise-offering/audit-logs.mdx].
  </Step>
</Steps>

## Troubleshooting

<AccordionGroup>
  <Accordion title="RateLimitError">
<p>When your requests exceed quota, catch <code>RateLimitError</code>:</p>
```python
from portkey_ai import exceptions

try:
    response = agent("…")
except exceptions.RateLimitError as e:
    print("Rate limited, retrying…", e)
```
  </Accordion>
  <Accordion title="TimeoutError">
<p>Set timeouts and catch <code>TimeoutError</code>:</p>
```python
from portkey_ai import exceptions

try:
    response = agent("…", timeout=5)
except exceptions.TimeoutError:
    print("Request timed out")
```
  </Accordion>
  <Accordion title="AuthenticationError">
<p>Verify your API key and header settings:</p>
```python
from portkey_ai import exceptions

try:
    response = agent("…")
except exceptions.AuthenticationError:
    print("Invalid API key or permissions")
```
  </Accordion>
  <Accordion title="Best-practice Retry">
<p>Use a simple exponential backoff:</p>
```python
import time
from portkey_ai import exceptions

for attempt in range(3):
    try:
        result = agent("…")
        break
    except exceptions.RateLimitError:
        time.sleep(2 ** attempt)
```
  </Accordion>
</AccordionGroup>

## Contact & Support

<CardGroup cols={2}>
  <Card title="Enterprise SLAs & Support" icon="headset" href="https://app.portkey.ai/support">
    Get dedicated SLA-backed support.
  </Card>
  <Card title="Portkey Community" icon="users" href="https://community.portkey.ai">
    Join our forums and Slack channel.
  </Card>
</CardGroup>

## Frequently Asked Questions

<AccordionGroup>
  <Accordion title="How does Portkey enhance Strands Agents?">
    Portkey adds production-readiness to Strands Agents through comprehensive observability (traces, logs, metrics), reliability features (fallbacks, retries, caching), and access to 200+ LLMs through a unified interface. This makes it easier to debug, optimize, and scale your agent applications, all while preserving Strands Agents' strong type safety.
  </Accordion>

  <Accordion title="Can I use Portkey with existing Strands Agents applications?">
    Yes! Portkey integrates seamlessly with existing Strands Agents applications. You just need to replace your client initialization code with the Portkey-enabled version. The rest of your agent code remains unchanged and continues to benefit from Strands Agents' strong typing.
  </Accordion>

  <Accordion title="Does Portkey work with all Strands Agents features?">
    Portkey supports all Strands Agents features, including tool use, multi-agent systems, and more. It adds observability and reliability without limiting any of the framework's functionality.
  </Accordion>

  <Accordion title="Can I track usage across multiple agents in a workflow?">
    Yes, Portkey allows you to use a consistent `trace_id` across multiple agents and requests to track the entire workflow. This is especially useful for multi-agent systems where you want to understand the full execution path.
  </Accordion>

  <Accordion title="How do I filter logs and traces for specific agent runs?">
    Portkey allows you to add custom metadata to your agent runs, which you can then use for filtering. Add fields like `agent_name`, `agent_type`, or `session_id` to easily find and analyze specific agent executions.
  </Accordion>

  <Accordion title="Can I use my own API keys with Portkey?">
    Yes! Portkey uses your own API keys for the various LLM providers. It securely stores them, allowing you to easily manage and rotate keys without changing your code.
  </Accordion>

</AccordionGroup>

## Resources

<CardGroup cols="3">
  <Card title="Strands Agents Docs" icon="book" href="https://strandsagents.com/">
  </Card>

  <Card title="Portkey Docs" icon="book" href="https://portkey.ai/docs">
    <p>Official Portkey documentation</p>
  </Card>

  <Card title="Book a Demo" icon="calendar" href="https://calendly.com/portkey-ai">
    <p>Get personalized guidance on implementing this integration</p>
  </Card>
</CardGroup>
