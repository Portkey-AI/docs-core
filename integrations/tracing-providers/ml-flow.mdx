---
title: "MLflow Tracing"
description: "Automatic LLM tracing with MLflow and Portkey gateway"
---

[MLflow Tracing](https://mlflow.org/docs/latest/llms/tracing/index.html) provides automatic, no-code instrumentation for 20+ GenAI libraries. Combined with Portkey, get comprehensive traces plus gateway features like caching, fallbacks, and load balancing.

## Why MLflow + Portkey?

<CardGroup cols={2}>
<Card title="No-Code Integrations" icon="plug">
Automatic instrumentation for 20+ GenAI libraries with one line
</Card>
<Card title="Detailed Traces" icon="microscope">
Capture inputs, outputs, and metadata for every step
</Card>
<Card title="Debug with Confidence" icon="bug">
Easily pinpoint issues with comprehensive trace data
</Card>
<Card title="Gateway Intelligence" icon="brain">
Portkey adds caching, fallbacks, and load balancing to every request
</Card>
</CardGroup>

## Quick Start

```bash
pip install mlflow openai opentelemetry-exporter-otlp-proto-http
```

```python
import os
import mlflow
from openai import OpenAI

# Send traces to Portkey
os.environ["OTEL_EXPORTER_OTLP_ENDPOINT"] = "https://api.portkey.ai/v1/logs/otel"
os.environ["OTEL_EXPORTER_OTLP_HEADERS"] = "x-portkey-api-key=YOUR_PORTKEY_API_KEY"
os.environ["OTEL_EXPORTER_OTLP_TRACES_PROTOCOL"] = "http/protobuf"

# Enable MLflow instrumentation
mlflow.openai.autolog()

# Use Portkey gateway
client = OpenAI(
    api_key="YOUR_PORTKEY_API_KEY",
    base_url="https://api.portkey.ai/v1"
)

response = client.chat.completions.create(
    model="@openai-prod/gpt-4.1",  # Provider slug from Model Catalog
    messages=[{"role": "user", "content": "Hello!"}]
)

print(response.choices[0].message.content)
```

<Frame>
  <img src="/images/product/opentelemetry.png" alt="OpenTelemetry traces in Portkey" />
</Frame>

## Setup

1. [Add provider](https://app.portkey.ai/model-catalog) in Model Catalog â†’ get provider slug (e.g., `@openai-prod`)
2. Get [Portkey API key](https://app.portkey.ai/api-keys)
3. Use `model="@provider-slug/model-name"` in requests

## Supported Libraries

**LLM Providers**: OpenAI, Anthropic, Cohere, Google AI, Azure OpenAI

**Frameworks**: LangChain, LlamaIndex, Haystack

**Vector Databases**: Pinecone, ChromaDB, Weaviate, Qdrant

## Next Steps

<CardGroup cols={2}>
<Card title="Gateway Configs" icon="gear" href="/product/ai-gateway/configs">
Fallbacks, caching, and load balancing
</Card>
<Card title="Model Catalog" icon="server" href="/product/model-catalog">
Manage providers and credentials
</Card>
<Card title="Analytics" icon="chart-line" href="/product/observability/analytics">
Cost and performance insights
</Card>
<Card title="MLflow Docs" icon="book" href="https://mlflow.org/docs/latest/llms/tracing/index.html">
Official documentation
</Card>
</CardGroup>

---

## See Your Traces in Action

Once configured, view your MLflow instrumentation combined with Portkey gateway intelligence in the [Portkey dashboard](https://app.portkey.ai/logs):

<Frame>
  <img src="/images/product/opentelemetry.png" alt="OpenTelemetry traces in Portkey" />
</Frame>
