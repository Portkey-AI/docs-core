---
title: "OpenLIT"
description: "OpenTelemetry-native observability for LLMs, vector databases, and GPUs"
---

[OpenLIT](https://openlit.io/) enables full-stack AI observability with one line of code—covering LLMs, vector databases, and GPUs. Combined with Portkey, get automatic tracing plus gateway features like caching, fallbacks, and load balancing.

## Why OpenLIT + Portkey?

<CardGroup cols={2}>
<Card title="One-Line Setup" icon="code">
Enable complete observability with a single line of code
</Card>
<Card title="Full-Stack Monitoring" icon="layer-group">
Monitor LLMs, vector databases, and GPUs in a unified view
</Card>
<Card title="OpenTelemetry Native" icon="chart-network">
Built on OpenTelemetry standards for seamless integration
</Card>
<Card title="Gateway Intelligence" icon="brain">
Portkey adds caching, fallbacks, and load balancing to every request
</Card>
</CardGroup>

## Quick Start

```bash
pip install openlit openai
```

```python
import os
import openlit
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter
from opentelemetry.sdk.trace.export import SimpleSpanProcessor
from opentelemetry import trace
from openai import OpenAI

# Send traces to Portkey
os.environ["OTEL_EXPORTER_OTLP_ENDPOINT"] = "https://api.portkey.ai/v1/logs/otel"
os.environ["OTEL_EXPORTER_OTLP_HEADERS"] = "x-portkey-api-key=YOUR_PORTKEY_API_KEY"

# Initialize OpenLIT
trace_provider = TracerProvider()
trace_provider.add_span_processor(SimpleSpanProcessor(OTLPSpanExporter()))
trace.set_tracer_provider(trace_provider)
openlit.init(tracer=trace.get_tracer(__name__), disable_batch=True)

# Use Portkey gateway
client = OpenAI(
    api_key="YOUR_PORTKEY_API_KEY",
    base_url="https://api.portkey.ai/v1"
)

response = client.chat.completions.create(
    model="@openai-prod/gpt-4.1",  # Provider slug from Model Catalog
    messages=[{"role": "user", "content": "Hello!"}]
)

print(response.choices[0].message.content)
```

<Frame>
  <img src="/images/product/opentelemetry.png" alt="OpenTelemetry traces in Portkey" />
</Frame>

## Setup

1. [Add provider](https://app.portkey.ai/model-catalog) in Model Catalog → get provider slug (e.g., `@openai-prod`)
2. Get [Portkey API key](https://app.portkey.ai/api-keys)
3. Use `model="@provider-slug/model-name"` in requests

## Next Steps

<CardGroup cols={2}>
<Card title="Gateway Configs" icon="gear" href="/product/ai-gateway/configs">
Fallbacks, caching, and load balancing
</Card>
<Card title="Model Catalog" icon="server" href="/product/model-catalog">
Manage providers and credentials
</Card>
<Card title="Analytics" icon="chart-line" href="/product/observability/analytics">
Cost and performance insights
</Card>
<Card title="OpenLIT Docs" icon="book" href="https://openlit.io/">
Official documentation
</Card>
</CardGroup>

---

## See Your Traces in Action

Once configured, view your OpenLIT instrumentation combined with Portkey gateway intelligence in the [Portkey dashboard](https://app.portkey.ai/logs):

<Frame>
  <img src="/images/product/opentelemetry.png" alt="OpenTelemetry traces in Portkey" />
</Frame>
