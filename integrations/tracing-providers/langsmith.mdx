---
title: "Langsmith"
description: "Integrate LangSmith observability with Portkey's AI gateway for comprehensive LLM monitoring and advanced routing capabilities"
---

LangSmith is LangChain's observability platform that helps you debug, test, evaluate, and monitor your LLM applications. When combined with Portkey, you get the best of both worlds: LangSmith's detailed observability and Portkey's advanced AI gateway features.

This integration allows you to:

* Track all LLM requests in LangSmith while routing through Portkey
* Use Portkey's 1600+ LLM providers with LangSmith observability
* Implement advanced features like caching, fallbacks, and load balancing
* Maintain detailed traces and analytics in both platforms

## Quick Start Integration

Since Portkey provides an OpenAI-compatible API, integrating with LangSmith is straightforward using LangSmith's OpenAI wrapper.

### Installation

```bash
pip install portkey-ai langsmith
```

### Basic Setup

```python
import os
from portkey_ai import Portkey
from langsmith.wrappers import wrap_openai

# Set your LangSmith credentials
os.environ["LANGSMITH_TRACING"] = "true"
os.environ["LANGSMITH_API_KEY"] = "YOUR_LANGSMITH_API_KEY"
os.environ["LANGSMITH_ENDPOINT"] = "https://api.smith.langchain.com"
os.environ["LANGSMITH_PROJECT"] = "your-project-name"  # Optional

# Initialize Portkey client wrapped with LangSmith
client = wrap_openai(Portkey(api_key="YOUR_PORTKEY_API_KEY"))

# Make a request
response = client.chat.completions.create(
    model="@openai-integration/gpt-4o-mini",
    messages=[{"role": "user", "content": "Hello, world!"}],
)

print(response.choices[0].message.content)
```

<Note>
  This integration automatically logs requests to both LangSmith and Portkey, giving you observability data in both platforms.
</Note>

## Using Portkey Features with LangSmith

### 1. LLM Integrations

LLM Integrations in Portkey allow you to securely manage API keys and set usage limits. Use them with LangSmith for better security:

```python
from langsmith.wrappers import wrap_openai
from portkey_ai import Portkey

client = wrap_openai(Portkey(api_key="YOUR_PORTKEY_API_KEY"))

response = client.chat.completions.create(
    model="@openai-integration/gpt-4o",
    messages=[{"role": "user", "content": "Explain quantum computing"}]
)
```

### 2. Multiple Providers

Switch between 1600+ LLM providers while maintaining LangSmith observability:

<Tabs>
  <Tab title="OpenAI">
    ```python
    client = wrap_openai(Portkey(api_key="YOUR_PORTKEY_API_KEY"))

    response = client.chat.completions.create(
        model="@openai-integration/gpt-4o",
        messages=[{"role": "user", "content": "Hello!"}]
    )
    ```
  </Tab>

  <Tab title="Anthropic">
    ```python
    client = wrap_openai(Portkey(api_key="YOUR_PORTKEY_API_KEY"))

    response = client.chat.completions.create(
        model="@anthropic-integration/claude-3-sonnet-20240229",
        messages=[{"role": "user", "content": "Hello!"}]
    )
    ```
  </Tab>

  <Tab title="Azure OpenAI">
    ```python
    client = wrap_openai(Portkey(api_key="YOUR_PORTKEY_API_KEY"))

    response = client.chat.completions.create(
        model="@azure-openai-integration/gpt-4o",
        messages=[{"role": "user", "content": "Hello!"}]
    )
    ```
  </Tab>
</Tabs>

### 3. Advanced Routing with Configs

Use Portkey's config system for advanced features while tracking in LangSmith:

```python
from portkey_ai import Portkey, Config
from langsmith.wrappers import wrap_openai

# Create a config in Portkey dashboard first, then reference it
config = Config(config_id="your-config-id")

client = wrap_openai(Portkey(api_key="YOUR_PORTKEY_API_KEY", config=config))
```

Example config for fallback between providers:

```json
{
  "strategy": {
    "mode": "fallback"
  },
  "targets": [
    {
      "integration": "@openai-integration",
      "override_params": {"model": "gpt-4o"}
    },
    {
      "integration": "@anthropic-integration",
      "override_params": {"model": "claude-3-opus-20240229"}
    }
  ]
}
```

### 4. Caching for Cost Optimization

Enable caching to reduce costs while maintaining full observability:

```python
from portkey_ai import Portkey, Config
from langsmith.wrappers import wrap_openai

config = Config(
    cache={
        "mode": "semantic",
        "max_age": 3600
    },
    integration="@openai-integration"
)

client = wrap_openai(Portkey(api_key="YOUR_PORTKEY_API_KEY", config=config))
```

### 5. Custom Metadata and Tracing

Add custom metadata visible in both LangSmith and Portkey:

```python
from langsmith import traceable
from langsmith.wrappers import wrap_openai
from portkey_ai import Portkey

client = wrap_openai(Portkey(api_key="YOUR_PORTKEY_API_KEY",
                             metadata={
                                 "user_id": "user_123",
                                 "session_id": "session_456",
                                 "environment": "production"
                             })
                     )

def traced_llm_call(question: str):
    return client.chat.completions.create(
        model="@openai-integration/gpt-4o",
        messages=[{"role": "user", "content": question}],
    )

response = traced_llm_call("What is 1729?")
```

<CardGroup cols={3}>
  <Card title="Fallbacks" icon="life-ring" href="/product/ai-gateway/fallbacks">
    Automatically switch to backup targets if the primary target fails.
  </Card>

  <Card title="Conditional Routing" icon="route" href="/product/ai-gateway/conditional-routing">
    Route requests to different targets based on specified conditions.
  </Card>

  <Card title="Load Balancing" icon="key" href="/docs/product/ai-gateway/load-balancing">
    Distribute requests across multiple targets based on defined weights.
  </Card>

  <Card title="Caching" icon="database" href="/product/ai-gateway/caching">
    Enable caching of responses to improve performance and reduce costs.
  </Card>

  <Card title="Smart Retries" icon="database" href="/product/ai-gateway/retries">
    Automatic retry handling with exponential backoff for failed requests
  </Card>

  <Card title="Budget Limits" icon="shield-check" href="/product/ai-gateway/virtual-keys/budget-limits">
    Set and manage budget limits across teams and departments. Control costs with granular budget limits and usage tracking.
  </Card>
</CardGroup>

## Observability Features

With this integration, you get:

### In LangSmith:

* Request/response logging
* Latency tracking
* Token usage analytics
* Cost calculation
* Trace visualization

### In Portkey:

* Request logs with provider details
* Advanced analytics across providers
* Cost tracking and budgets
* Performance metrics
* Custom dashboards
* Token usage analytics

<Frame>
  <img src="https://mintlify.s3.us-west-1.amazonaws.com/portkey-docs/images/integrations/observability.png" width="600" />
</Frame>

## Migration Guide

If you're already using LangSmith with OpenAI, migrating to use Portkey is simple:

<CodeGroup>
  ```python Before
  from openai import OpenAI
  from langsmith.wrappers import wrap_openai

  client = wrap_openai(OpenAI(api_key="YOUR_OPENAI_KEY"))
  ```

  ```python After
  from portkey_ai import Portkey
  from langsmith.wrappers import wrap_openai

  client = wrap_openai(Portkey(api_key="YOUR_PORTKEY_API_KEY"))
  ```
</CodeGroup>

## Next Steps

* [Create LLM Integrations](/product/integrations) for secure API key management
* [Build Configs](/product/ai-gateway/configs) for advanced routing
* [Set up Guardrails](/product/guardrails) for content filtering
* [Implement Caching](/product/ai-gateway/cache-simple-and-semantic) for cost optimization

## Resources

* [LangSmith Documentation](https://docs.smith.langchain.com/)
* [Portkey AI Gateway Guide](/product/ai-gateway)
* [Portkey Python SDK Reference](/api-reference/portkey-sdk-client)

<Note>
  For enterprise support and custom features, contact our [enterprise team](https://calendly.com/portkey-ai).
</Note>
