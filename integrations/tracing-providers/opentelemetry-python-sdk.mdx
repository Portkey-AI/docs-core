---
title: "OpenTelemetry Python SDK"
description: "Direct OpenTelemetry instrumentation with full control over traces"
---

The [OpenTelemetry SDK](https://opentelemetry.io/docs/languages/python/) provides direct, fine-grained control over instrumentation. Combined with Portkey, get custom traces plus gateway features like caching, fallbacks, and load balancing.

## Why OpenTelemetry SDK + Portkey?

<CardGroup cols={2}>
<Card title="Full Control" icon="sliders">
Manually instrument exactly what you need with custom spans and attributes
</Card>
<Card title="Production Ready" icon="rocket">
Battle-tested OpenTelemetry standard used by enterprises worldwide
</Card>
<Card title="Custom Attributes" icon="tags">
Add any metadata you need to traces for debugging and analysis
</Card>
<Card title="Gateway Intelligence" icon="brain">
Portkey adds caching, fallbacks, and load balancing to every request
</Card>
</CardGroup>

## Quick Start

```bash
pip install opentelemetry-api opentelemetry-sdk opentelemetry-exporter-otlp-proto-http openai
```

```python
from opentelemetry import trace
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import BatchSpanProcessor
from opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter
from openai import OpenAI

# Setup OpenTelemetry
provider = TracerProvider()
trace.set_tracer_provider(provider)
provider.add_span_processor(BatchSpanProcessor(OTLPSpanExporter(
    endpoint="https://api.portkey.ai/v1/otel/v1/traces",
    headers={"x-portkey-api-key": "YOUR_PORTKEY_API_KEY"}
)))
tracer = trace.get_tracer(__name__)

# Use Portkey gateway
client = OpenAI(
    api_key="YOUR_PORTKEY_API_KEY",
    base_url="https://api.portkey.ai/v1"
)

# Custom instrumentation
def generate_response(input_text):
    with tracer.start_as_current_span("llm-call") as span:
        span.set_attribute("input", input_text)
        response = client.chat.completions.create(
            model="@openai-prod/gpt-4.1",  # Provider slug from Model Catalog
            messages=[{"role": "user", "content": input_text}]
        )
        span.set_attribute("output", response.choices[0].message.content)
        return response.choices[0].message.content

print(generate_response("Hello!"))
```

<Frame>
  <img src="/images/product/opentelemetry.png" alt="OpenTelemetry traces in Portkey" />
</Frame>

## Setup

1. [Add provider](https://app.portkey.ai/model-catalog) in Model Catalog â†’ get provider slug (e.g., `@openai-prod`)
2. Get [Portkey API key](https://app.portkey.ai/api-keys)
3. Use `model="@provider-slug/model-name"` in requests

## Next Steps

<CardGroup cols={2}>
<Card title="Gateway Configs" icon="gear" href="/product/ai-gateway/configs">
Fallbacks, caching, and load balancing
</Card>
<Card title="Model Catalog" icon="server" href="/product/model-catalog">
Manage providers and credentials
</Card>
<Card title="Analytics" icon="chart-line" href="/product/observability/analytics">
Cost and performance insights
</Card>
<Card title="OpenTelemetry Docs" icon="book" href="https://opentelemetry.io/docs/languages/python/">
Official documentation
</Card>
</CardGroup>

---

## See Your Traces in Action

Once configured, view your custom OpenTelemetry traces combined with Portkey gateway intelligence in the [Portkey dashboard](https://app.portkey.ai/logs):

<Frame>
  <img src="/images/product/opentelemetry.png" alt="OpenTelemetry traces in Portkey" />
</Frame>
