---
title: '3.1 Prompt Adaptation'
description: 'Optimizing prompts to reduce token usage and costs in LLM applications'
---

Prompt adaptation is a key technique in the FrugalGPT framework for reducing LLM inference costs while maintaining performance. It involves crafting concise, optimized prompts to minimize token usage and processing costs.

## Key Strategies

1. **Clear and concise instructions**: Eliminate unnecessary words or context that don't contribute to the desired output.

2. **Use of delimiters**: Clearly separate different parts of the prompt (e.g., context, instructions, input) using delimiters like "###" or "---".

3. **Structured prompts**: Organize information in a logical, easy-to-process format for the model.

4. **Iterative refinement**: Continuously test and refine prompts to achieve the desired output with minimal token usage.

## Example

Here's an example of prompt adaptation:

```
Before:
Please analyze the following customer review and provide a summary of the main points, including any positive or negative aspects mentioned, and suggest how the company could improve based on this feedback. Here's the review: [long customer review text]

After:
Summarize key points from this review:
Positive:
Negative:
Improvement suggestions:
###
[concise customer review text]
```

By adapting prompts in this way, you can significantly reduce token usage while still obtaining high-quality outputs from the model.