---
title: '3.2 LLM Approximation'
description: 'Using caches and model fine-tuning to reduce costs in LLM applications'
---

LLM approximation is a technique in the FrugalGPT framework that involves using caches and model fine-tuning to avoid repeated queries to expensive models. This approach can lead to substantial cost savings, especially for frequently asked questions or similar queries.

## Key Strategies

1. **Response caching**: Store responses to common queries for quick retrieval.

2. **Semantic caching**: Use similarity measures to return cached responses for semantically similar queries.

3. **Fine-tuning smaller models**: Train smaller, task-specific models on the outputs of larger models.

## Implementation Example

Here's a basic example of implementing semantic caching:

```python
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
from transformers import AutoTokenizer, AutoModel

tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')
model = AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')

def get_embeddings(texts):
    # Implementation details...

cache = {}  # Simple in-memory cache

def semantic_cache_query(query):
    query_embedding = get_embeddings([query])[0]
    for cached_query, response in cache.items():
        cached_embedding = get_embeddings([cached_query])[0]
        similarity = cosine_similarity([query_embedding], [cached_embedding])[0][0]
        if similarity > 0.95:  # Adjust threshold as needed
            return response
    return None  # No similar query found in cache

# Usage
response = semantic_cache_query("What's the weather like today?")
if response is None:
    # Query the expensive LLM and cache the result
    response = expensive_llm_query("What's the weather like today?")
    cache["What's the weather like today?"] = response
print(response)
```

This approach can lead to significant cost savings, especially for applications with repetitive queries or similar user inputs.