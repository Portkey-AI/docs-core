---
title: '6.2 Caching Strategies'
description: 'Implementing effective caching to reduce API calls and costs in LLM applications'
---

Implementing effective caching strategies can significantly reduce API calls and associated costs in LLM applications.

## Types of Caching

1. **Result Caching**: Store and reuse results for identical queries.
2. **Semantic Caching**: Cache results for semantically similar queries.
3. **Partial Result Caching**: Cache intermediate results for complex queries.

## Implementing a Semantic Cache

Here's a basic example of implementing a semantic cache:

```python
import numpy as np
from sentence_transformers import SentenceTransformer

class SemanticCache:
    def __init__(self):
        self.cache = {}
        self.model = SentenceTransformer('all-MiniLM-L6-v2')

    def get(self, query):
        query_embedding = self.model.encode([query])[0]
        for cached_query, (cached_embedding, result) in self.cache.items():
            similarity = np.dot(query_embedding, cached_embedding)
            if similarity > 0.95:  # Adjust threshold as needed
                return result
        return None

    def set(self, query, result):
        query_embedding = self.model.encode([query])[0]
        self.cache[query] = (query_embedding, result)

# Usage
cache = SemanticCache()
result = cache.get("What's the weather like today?")
if result is None:
    result = expensive_api_call("What's the weather like today?")
    cache.set("What's the weather like today?", result)
print(result)
```

By implementing effective caching strategies, organizations can significantly reduce the number of API calls to their LLM services, leading to substantial cost savings and improved response times.