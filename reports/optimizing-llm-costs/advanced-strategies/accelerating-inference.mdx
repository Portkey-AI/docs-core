---
title: '4.3 Accelerating Inference'
description: 'Techniques and tools for optimizing LLM inference speed'
---

Accelerating inference is crucial for reducing latency and operational costs. Several techniques and tools have emerged to optimize LLM inference speeds.

## Key Acceleration Techniques

1. **Quantization**: Reducing model precision without significant accuracy loss.
2. **Pruning**: Removing unnecessary weights from the model.
3. **Knowledge Distillation**: Training a smaller model to mimic a larger one.
4. **Optimized inference engines**: Using specialized software for faster inference.

## Popular Tools for Inference Acceleration

- **vLLM**: Offers up to 24x higher throughput with its PagedAttention method.
- **Text Generation Inference (TGI)**: Widely used for high-performance text generation.
- **ONNX Runtime**: Provides optimized inference across various hardware platforms.

## Example: Using vLLM for Faster Inference

Here's a basic example of using vLLM:

```python
from vllm import LLM, SamplingParams

# Initialize the model
llm = LLM(model="facebook/opt-125m")

# Set up sampling parameters
sampling_params = SamplingParams(temperature=0.8, top_p=0.95)

# Generate text
prompts = [
    "Once upon a time,",
    "In a galaxy far, far away,"
]
outputs = llm.generate(prompts, sampling_params)

# Print the generated text
for output in outputs:
    prompt = output.prompt
    generated_text = output.outputs[0].text
    print(f"Prompt: {prompt!r}")
    print(f"Generated text: {generated_text!r}")
```

By implementing these acceleration techniques and using optimized tools, you can significantly reduce inference times and operational costs for your LLM applications.