---
title: Model Catalog
description: Explore and query every AI model available to your workspace, with instant code snippets for all supported providers.
---

The **Model Catalog** is the evolution of Virtual Keys, providing a centralized and powerful way to manage, discover, and use AI models within your workspace. It consists of two main sections: **AI Providers**, where you manage your connections, and **Models**, where you explore what you can use.

### **How it Works: Inheritance from the Organization**

The most significant change with the Model Catalog is the concept of inheritance. Think of it this way:

1.  Your **Organization Admin** creates a master **Integration** at the company level (e.g., for "Azure Production"). They add the credentials and can set default budgets, rate limits, and an allow-list of approved models for that integration.
2.  When they provision this integration to your workspace, a corresponding **AI Provider** is automatically created in your Model Catalog.
3.  This new AI Provider in your workspace *inherits* all the settings from the organization-level integration, including its credentials, model access, and spending limits.

This "create once, provision many" approach provides central governance while giving workspaces the flexibility they need.

---

### **The Model Catalog Experience by Role**

Your experience with the Model Catalog will differ based on your role within the Portkey organization.

#### **For Workspace Members (Developers): Discover and Build**

As a developer, your experience is simplified and streamlined. You primarily interact with the **Models** tab, which acts as your personal "Model Garden."

-   **Discover Models:** The "Models" tab is a complete gallery of every single model you have been given access to by your admins.
-   **Get Code Snippets:** Click on any model, and Portkey will generate the exact code snippet you need to start making calls, with the correct provider and model slugs already included.
-   **Simplified API Calls:** You can now call any model directly using the `model` parameter, formatted as `@{provider_slug}/{model_slug}`. This lets you switch between providers and models on the fly with a single Portkey API key.

```python
# Switch between a model on OpenAI and one on Bedrock seamlessly
client.chat.completions.create(
    model="@openai-prod/gpt-4o", 
    messages=[...]
)

client.chat.completions.create(
    model="@bedrock-us/claude-3-sonnet-v1",
    messages=[...]
)
```

#### **For Workspace Admins: Manage and Customize**

As a Workspace Admin, you have more control over the providers within your workspace via the **AI Providers** tab.

You will see a list of providers that have been inherited from the organization. From here, you have two primary options when you click **Create Provider**:

1.  **Inherit from an Org Integration:** You can create *additional* providers that are based on an existing org-level integration. This is useful for subdividing access within your team. For example, if your workspace has a $1000 budget on the main "Azure Prod" integration, you could create a new provider from it named "azure-prod-experimental" and give it a stricter $100 budget for a specific project.
2.  **Create a New Workspace-Exclusive Integration:** If your Org Admin has enabled the permission, you can create a brand new integration from scratch. This provider is exclusive to your workspace and functions just like the old Virtual Keys did.

#### **For Organization Admins: A View into Workspaces**

While Org Admins primarily work in the main **[Integrations](/product/integrations)** dashboard, the Model Catalog provides a crucial feedback loop:

When a Workspace Admin creates a new, workspace-exclusive integration (option #2 above), you gain full visibility. This new integration will automatically appear on your main Integrations page under the **"Workspace-Created"** tab, ensuring you always have a complete audit trail of all provider credentials being used across the organization.

### **SDK Integration and Advanced Usage**

While the new `model` parameter is the recommended approach for its simplicity, Portkey maintains full backward compatibility and offers flexible integration options for various SDKs.

<Info>
Remember, the term "Virtual Key" is now synonymous with the **AI Provider slug** found in your Model Catalog.
</Info>

#### **Using with Portkey SDK**

You can set a default AI Provider at initialization or override it per request.

<Tabs>
  <Tab title="NodeJS">

```js
import Portkey from 'portkey-ai';

// Set a default AI Provider for the client
const portkey = new Portkey({
    apiKey: process.env.PORTKEY_API_KEY,
    virtualKey: "YOUR_AI_PROVIDER_SLUG"
//  provider: "@YOUR_AI_PROVIDER_SLUG"
});

// Or, override it for a specific call
const chatCompletion = await portkey.chat.completions.create({
    messages: [{ role: 'user', content: 'Say this is a test' }],
    model: 'gpt-3.5-turbo',
}, { virtualKey: "ANOTHER_AI_PROVIDER_SLUG" });
```

  </Tab>
  <Tab title="Python">

```python
from portkey_ai import Portkey

# Set a default AI Provider for the client
portkey = Portkey(
    api_key="PORTKEY_API_KEY",
    virtual_key="YOUR_AI_PROVIDER_SLUG"
#   provider="@YOUR_AI_PROVIDER_SLUG"
)

# Or, override it for a specific call
completion = portkey.with_options(
    virtual_key="ANOTHER_AI_PROVIDER_SLUG"
).chat.completions.create(
    messages = [{ "role": 'user', "content": 'Say this is a test' }],
    model = 'gpt-3.5-turbo'
)
```

  </Tab>
</Tabs>

#### **Using with OpenAI SDK**

Simply point the OpenAI client to Portkey's gateway and pass your AI Provider slug in the headers.

<Tabs>
  <Tab title="Python">

```python
from openai import OpenAI
from portkey_ai import PORTKEY_GATEWAY_URL, createHeaders

client = OpenAI(
  api_key="", # can be left blank
  base_url=PORTKEY_GATEWAY_URL,
  default_headers=createHeaders(
    api_key="PORTKEY_API_KEY",
    virtual_key="YOUR_AI_PROVIDER_SLUG"
#   provider="@YOUR_AI_PROVIDER_SLUG"
  )
)
```

  </Tab>
  <Tab title="NodeJS">

```javascript
import OpenAI from "openai";
import { PORTKEY_GATEWAY_URL, createHeaders } from 'portkey-ai';

const openai = new OpenAI({
  apiKey: '', // can be left blank
  baseURL: PORTKEY_GATEWAY_URL,
  defaultHeaders: createHeaders({
    apiKey: "PORTKEY_API_KEY",
    virtualKey: "YOUR_AI_PROVIDER_SLUG"
//  provider: "@YOUR_AI_PROVIDER_SLUG"
  })
});
```

  </Tab>
</Tabs>