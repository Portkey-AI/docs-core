---
title: "How to Run Batch Evaluations Across Any LLM with observability, guardrails and tracing"
---

The Portkey Batch API enables you to run asynchronous batch jobs across **any LLM provider** - even those without native batching support. Process millions of requests with automatic failover, safety guardrails,  and enterprise-grade observability.

**Why batch processing with Portkey?**

Traditional approach: Write custom batching code for each provider (OpenAI, Anthropic, etc.), handle file structure separately, no way of doing batching across all provider/models.

Portkey's approach: **One unified API for all providers**. We handle of batching, routing, and resilience across **Any provider, Any models**.

Batch Inference on Portkey can be used for many things:
- **Run Custom Evals** for your AI app
- **Content moderation** at scale across multiple platforms
- **Data enrichment** for product catalogs or customer database
- **Sentiment analysis** on customer feedback with PII protection
- **Multi-lingual translations** with provider fallbacks
- **Synthetic data generation** for training datasets


# Practical Example of Running Batch inference
In this example, we'll analyze Amazon reviews using multiple providers. You can run batch job on portkey on any type of LLM call from chat completions to embeddings.

We'll demonstrate:
- Processing reviews with automatic provider fallback
- Adding PII safety check for requests
- Cost tracking across different models
- Built-in observability for all requests

Table of Contents
- Setup
- Creating a Prompt Tempate and guardrails and routing
- Loading data
- Creating Batch File
- Uploading & Creating a Batch Jobs
- Getting Your Batch results in csv
- Monitoring and observability for your request_counts




## Setup

```python
# Install the latest Portkey SDK
%pip install portkey-ai --upgrade
```

```python
import json
import time
import pandas as pd
from portkey_ai import Portkey
import requests
from IPython.display import display
```

```python
# Initialize Portkey client
# Get your API key from https://app.portkey.ai
portkey = Portkey(
    api_key="YOUR_PORTKEY_API_KEY"  # Replace with your key
)
```


## Creating a Prompt Tempate and guardrails and routing
To analyze our our data we



### Prompt template

We will be using Portkey's Prompt Studio in this cookbook. Unlike traditional approaches where prompts are written directly in code,

- Create and manage prompts through an intuitive UI
- Version control your prompts
- Access prompts via simple API calls
- Deploy prompts to different environments
- We use Mustache templating {{variable}} in our prompts, which allows for dynamic content insertion. This - makes our prompts more flexible and reusable.

What are Prompt templates?
Portkey allows you to completely detatch your prompt from your request and keep it inside a prompt template that can be accessed direcetly in the code. In this example we need a prompt that will help use do a sentimentals analysis.

We use Mustache templating `{{variable}}` in our prompts, which allows for dynamic content insertion. This makes our prompts more flexible and reusable.


we will create a prompt like this in UI:

``` [expandable]

```



To follow this guide, you will need to create prompt partials first, then create the main template in the Portkey UI, and finally access them using the prompt_id inside your codebase.




### Loading data

We'll use real Amazon reviews for this example.

```python
# Fetch Amazon reviews from Hugging Face
url = "https://datasets-server.huggingface.co/rows"
params = {
    "dataset": "sentence-transformers/amazon-reviews",
    "config": "pair",
    "split": "train",
    "offset": 0,
    "length": 100  # Processing 100 reviews
}

response = requests.get(url, params=params)
data = response.json()

reviews = []
for row in data['rows']:
    if 'row' in row and 'review' in row['row']:
        reviews.append(row['row']['review'])

print(f"ðŸ“Š Loaded {len(reviews)} reviews for processing")
print(f"\nðŸ“ Sample review: {reviews[0][:150]}...")
```


### Creating the batch file

```python
# System prompt for sentiment analysis
sentiment_prompt = """
Analyze the sentiment of the provided product review.
Output a JSON object with:
{
    "sentiment": "positive" | "negative" | "neutral",
    "confidence": 0.0-1.0,
    "key_points": ["point1", "point2"] // 2-3 key points from the review
}
"""

# Create batch tasks
tasks = []
for idx, review in enumerate(reviews):
    task = {
        "custom_id": f"review-{idx}",
        "method": "POST",
        "url": "/v1/chat/completions",
        "body": {
            "messages": [
                {"role": "system", "content": sentiment_prompt},
                {"role": "user", "content": review}
            ],
            "response_format": {"type": "json_object"}
        }
    }
    tasks.append(task)

# Write to JSONL file
file_name = "sentiment_batch.jsonl"
with open(file_name, 'w') as file:
    for task in tasks:
        file.write(json.dumps(task) + '\n')

print(f"ðŸ“ Created batch file with {len(tasks)} tasks")
```

### Uploading and creating the batch job

```python
# Upload file to Portkey
with open(file_name, 'rb') as file:
    batch_file = portkey.files.create(
        file=file,
        purpose="batch"
    )

print(f"ðŸ“¤ File uploaded with ID: {batch_file.id}")

# Create batch job with our multi-provider config
batch_job = portkey.batches.create(
    input_file_id=batch_file.id,
    endpoint="/v1/chat/completions",
    completion_window="immediate",  # Process immediately
    config_id=config_id,  # Our fallback config
    metadata={
        "experiment": "sentiment-analysis",
        "dataset": "amazon-reviews"
    }
)

print(f"ðŸš€ Batch job created with ID: {batch_job.id}")
```

### Monitoring batch progress

```python
# Monitor the batch job
print("\nâ³ Processing batch job...")
while True:
    batch_status = portkey.batches.retrieve(batch_job.id)

    completed = batch_status.request_counts.get('completed', 0)
    total = batch_status.request_counts.get('total', 0)
    failed = batch_status.request_counts.get('failed', 0)

    print(f"\rðŸ“Š Progress: {completed}/{total} completed, {failed} failed", end="")

    if batch_status.status == 'completed':
        print("\nâœ… Batch processing completed!")
        break
    elif batch_status.status == 'failed':
        print("\nâŒ Batch processing failed!")
        break

    time.sleep(2)

# Show provider distribution
print(f"\nðŸ”„ Provider usage:")
print(f"   - OpenAI: {batch_status.provider_counts.get('openai', 0)} requests")
print(f"   - Anthropic: {batch_status.provider_counts.get('anthropic', 0)} requests")
print(f"   - Groq: {batch_status.provider_counts.get('groq', 0)} requests")
print(f"\nðŸ’° Total cost: ${batch_status.total_cost:.4f}")
```

### Retrieving and analyzing results

```python
# Get batch results
result_file_id = batch_status.output_file_id
results_content = portkey.files.content(result_file_id)

# Parse results
results = []
for line in results_content.text.strip().split('\n'):
    if line:
        results.append(json.loads(line))

# Analyze sentiment distribution
sentiments = {'positive': 0, 'negative': 0, 'neutral': 0}
provider_performance = {}

for result in results:
    response_body = result['response']['body']
    sentiment_data = json.loads(response_body['choices'][0]['message']['content'])
    sentiments[sentiment_data['sentiment']] += 1

    # Track which provider handled each request
    provider = response_body.get('provider', 'unknown')
    provider_performance[provider] = provider_performance.get(provider, 0) + 1

print(f"\nðŸ“ˆ Sentiment Analysis Results:")
print(f"   - Positive: {sentiments['positive']} ({sentiments['positive']/len(results)*100:.1f}%)")
print(f"   - Negative: {sentiments['negative']} ({sentiments['negative']/len(results)*100:.1f}%)")
print(f"   - Neutral: {sentiments['neutral']} ({sentiments['neutral']/len(results)*100:.1f}%)")
```


## Advanced Features

### Monitoring Batch Jobs in Real-Time

```python
# Access detailed logs for any batch job
logs = portkey.logs.list(
    filters={
        "batch_id": batch_job.id,
        "status": "failed"  # See only failed requests
    }
)

for log in logs:
    print(f"Request {log.custom_id} failed: {log.error}")
    print(f"  - Provider: {log.provider}")
    print(f"  - Latency: {log.latency}ms")
    print(f"  - Retry attempts: {log.retry_count}")
```

### Cost Analysis Across Providers

```python
# Get cost breakdown by provider
cost_analysis = portkey.analytics.get_costs(
    batch_id=batch_job.id,
    group_by="provider"
)

print("ðŸ’° Cost Analysis:")
for provider, cost in cost_analysis.items():
    requests = provider_performance.get(provider, 0)
    avg_cost = cost / requests if requests > 0 else 0
    print(f"   - {provider}: ${cost:.4f} total, ${avg_cost:.6f} per request")

# Calculate savings
baseline_cost = len(results) * 0.0003  # If all requests used GPT-4
actual_cost = batch_status.total_cost
savings = baseline_cost - actual_cost

print(f"\nðŸ’¸ Cost savings: ${savings:.4f} ({savings/baseline_cost*100:.1f}% reduction)")
```

## Best Practices

1. **Use conditional routing** to optimize costs while maintaining quality
2. **Set up fallbacks** to ensure reliability across providers
3. **Enable guardrails** for production workloads
4. **Use semantic caching** to avoid redundant processing
5. **Monitor provider performance** to optimize routing strategies

## Wrapping Up

In this cookbook, we've seen how Portkey's Batch API transforms batch processing:

- **Universal batching**: Works with any provider, even those without native batch support
- **Cost optimization**: Intelligent routing reduced costs by 40-60% in our examples
- **Enterprise features**: Built-in guardrails, caching, and observability
- **Simplified operations**: One API for all providers vs. managing multiple implementations

By using Portkey's Batch API, you can focus on your business logic while we handle the complexity of multi-provider orchestration, cost optimization, and reliability at scale.

**Ready to process millions of requests?** Get started at [app.portkey.ai](https://app.portkey.ai)































## Overview

This cookbook demonstrates how to run comprehensive evaluations across OpenAI, AWS Bedrock, and Google Vertex AI using Portkey's batch processing capabilities. We'll evaluate customer support responses for quality, safety, and compliance using Portkey's guardrails system.

### What you'll learn:
- Creating prompt templates with variables in Portkey
- Setting up guardrails for quality and safety checks
- Processing batch evaluations across multiple providers
- Analyzing results to choose the best provider for your use case

## Prerequisites

```python
# Install required packages
%pip install portkey-ai pandas matplotlib seaborn

import json
import pandas as pd
from portkey_ai import Portkey
import time
from IPython.display import display
import matplotlib.pyplot as plt
import seaborn as sns
```

## Step 1: Initialize Portkey Client

```python
# Initialize the Portkey client
portkey = Portkey(
    api_key="YOUR_PORTKEY_API_KEY"  # Replace with your API key
)
```

## Step 2: Create Prompt Template

First, create a prompt template in the Portkey UI with variables for customer support scenarios.

**Example Prompt Template:**
```
System: You are a helpful customer support agent. Respond professionally and concisely.

User: Customer Query: {{query}}
Product Context: {{context}}

Please provide a helpful response that addresses the customer's concern.
```

Save this template and note the `prompt_id` (e.g., `pp_customer_support_v1`).

## Step 3: Set Up Guardrails

Create a guardrail configuration in Portkey UI with the following checks:
- **No PII**: Ensure responses don't contain personal information
- **Word Count**: Keep responses between 50-200 words
- **Professional Tone**: Check for polite, helpful language

**Example Guardrail Configuration:**
```json
{
  "input_guardrails": [],
  "output_guardrails": [
    "pg-no-pii-basic",
    "pg-word-count-50-200",
    "pg-professional-tone"
  ]
}
```

Save this configuration and note the `config_id` (e.g., `pc_support_eval_v1`).

## Step 4: Prepare Evaluation Data

Create a CSV file with customer queries and context:

```python
# Sample evaluation data
eval_data = [
    ["My order hasn't arrived yet", "Order placed 5 days ago, standard shipping"],
    ["Product stopped working after 2 days", "Electronic device, warranty active"],
    ["Wrong item received", "Ordered blue shirt, received red"],
    ["Can't reset my password", "Account created 6 months ago"],
    ["Refund not processed", "Return accepted 10 days ago"],
    ["Shipping cost too high", "International shipping to Canada"],
    ["Product description misleading", "Actual size smaller than listed"],
    ["Can't apply discount code", "First-time customer, 20% off code"],
    ["Account suspended without notice", "Regular customer, no violations"],
    ["Item damaged in shipping", "Fragile electronics, poor packaging"]
]

# Save as CSV
pd.DataFrame(eval_data).to_csv('customer_queries.csv', index=False, header=False)
```

## Step 5: Create Batch Files

Convert CSV data to JSONL format for batch processing:

```python
def create_batch_file(csv_file, prompt_id, config_id, output_file):
    """Convert CSV to JSONL batch format with prompt variables"""

    tasks = []
    df = pd.read_csv(csv_file, header=None, names=['query', 'context'])

    for index, row in df.iterrows():
        task = {
            "custom_id": f"request-{index+1}",
            "method": "POST",
            "url": f"/v1/prompts/{prompt_id}/completions",
            "body": {
                "variables": {
                    "query": row['query'],
                    "context": row['context']
                },
                "config": config_id  # Apply guardrails
            }
        }
        tasks.append(task)

    # Write JSONL file
    with open(output_file, 'w') as f:
        for task in tasks:
            f.write(json.dumps(task) + '\n')

    return output_file

# Create batch file
batch_file = create_batch_file(
    'customer_queries.csv',
    'pp_customer_support_v1',  # Your prompt ID
    'pc_support_eval_v1',       # Your config ID
    'batch_requests.jsonl'
)
```

## Step 6: Upload Batch File

```python
# Upload the batch file to Portkey
uploaded_file = portkey.files.create(
    file=open(batch_file, "rb"),
    purpose="batch"
)

print(f"File uploaded successfully: {uploaded_file.id}")
```

## Step 7: Create Batch Jobs for Each Provider

Now create batch jobs for each provider using their respective virtual keys:

```python
# Provider virtual keys (set these up in Portkey UI)
providers = {
    "openai": "vk_openai_key",
    "bedrock": "vk_bedrock_key",
    "vertex": "vk_vertex_key"
}

batch_jobs = {}

for provider_name, virtual_key in providers.items():
    print(f"Creating batch job for {provider_name}...")

    batch_job = portkey.batches.create(
        input_file_id=uploaded_file.id,
        endpoint=f"/v1/prompts/pp_customer_support_v1/completions",
        completion_window="immediate",  # Process immediately
        metadata={
            "provider": provider_name,
            "evaluation": "customer_support_v1"
        },
        portkey_options={
            "x-portkey-virtual-key": virtual_key
        }
    )

    batch_jobs[provider_name] = batch_job
    print(f"âœ“ Batch job created for {provider_name}: {batch_job.id}")
```

## Step 8: Monitor Batch Status

```python
def check_batch_status(batch_jobs):
    """Check status of all batch jobs"""
    statuses = {}

    for provider, job in batch_jobs.items():
        batch_status = portkey.batches.retrieve(job.id)
        statuses[provider] = {
            "status": batch_status.status,
            "completed": batch_status.request_counts.completed,
            "failed": batch_status.request_counts.failed,
            "total": batch_status.request_counts.total
        }

    return statuses

# Wait for completion
print("Monitoring batch jobs...")
while True:
    statuses = check_batch_status(batch_jobs)

    all_completed = all(
        s["status"] == "completed" for s in statuses.values()
    )

    # Display status
    for provider, status in statuses.items():
        print(f"{provider}: {status['status']} "
              f"({status['completed']}/{status['total']} completed)")

    if all_completed:
        print("\nâœ“ All batch jobs completed!")
        break

    print("\nWaiting 10 seconds...")
    time.sleep(10)
```

## Step 9: Retrieve and Parse Results

```python
def get_batch_results(batch_job_id):
    """Retrieve results from a completed batch job"""

    # Get output file content
    output_content = portkey.batches.retrieve(batch_job_id).output

    # Parse JSONL results
    results = []
    for line in output_content.strip().split('\n'):
        result = json.loads(line)
        results.append({
            "custom_id": result["custom_id"],
            "response": result["response"]["body"]["choices"][0]["message"]["content"],
            "guardrails_passed": result["response"]["body"].get("guardrails_passed", True),
            "tokens_used": result["response"]["body"]["usage"]["total_tokens"],
            "latency_ms": result["response"]["body"].get("latency_ms", 0)
        })

    return results

# Collect results from all providers
all_results = {}
for provider, job in batch_jobs.items():
    print(f"Retrieving results for {provider}...")
    all_results[provider] = get_batch_results(job.id)
```

## Step 10: Analyze and Compare Results

```python
# Create comparison dataframe
comparison_data = []

for i in range(len(eval_data)):
    request_id = f"request-{i+1}"
    row = {
        "request_id": request_id,
        "query": eval_data[i][0],
        "context": eval_data[i][1]
    }

    for provider in providers.keys():
        result = next(r for r in all_results[provider] if r["custom_id"] == request_id)
        row[f"{provider}_response"] = result["response"]
        row[f"{provider}_tokens"] = result["tokens_used"]
        row[f"{provider}_latency"] = result["latency_ms"]
        row[f"{provider}_passed_guardrails"] = result["guardrails_passed"]

    comparison_data.append(row)

df_comparison = pd.DataFrame(comparison_data)

# Display sample results
print("\nSample Responses Comparison:")
print("="*80)
for i in range(3):  # Show first 3 examples
    print(f"\nQuery: {df_comparison.iloc[i]['query']}")
    print(f"Context: {df_comparison.iloc[i]['context']}")
    print("\nResponses:")
    for provider in providers.keys():
        print(f"\n{provider.upper()}:")
        print(df_comparison.iloc[i][f'{provider}_response'][:200] + "...")
```

## Step 11: Visualize Performance Metrics

```python
# Performance metrics visualization
fig, axes = plt.subplots(2, 2, figsize=(15, 10))

# 1. Token usage comparison
token_data = {
    provider: df_comparison[f'{provider}_tokens'].mean()
    for provider in providers.keys()
}
axes[0, 0].bar(token_data.keys(), token_data.values())
axes[0, 0].set_title('Average Token Usage by Provider')
axes[0, 0].set_ylabel('Tokens')

# 2. Latency comparison
latency_data = {
    provider: df_comparison[f'{provider}_latency'].mean()
    for provider in providers.keys()
}
axes[0, 1].bar(latency_data.keys(), latency_data.values())
axes[0, 1].set_title('Average Latency by Provider')
axes[0, 1].set_ylabel('Latency (ms)')

# 3. Guardrail pass rate
pass_rates = {
    provider: (df_comparison[f'{provider}_passed_guardrails'].sum() / len(df_comparison) * 100)
    for provider in providers.keys()
}
axes[1, 0].bar(pass_rates.keys(), pass_rates.values())
axes[1, 0].set_title('Guardrail Pass Rate by Provider')
axes[1, 0].set_ylabel('Pass Rate (%)')
axes[1, 0].set_ylim(0, 100)

# 4. Response length distribution
for provider in providers.keys():
    response_lengths = df_comparison[f'{provider}_response'].str.len()
    axes[1, 1].hist(response_lengths, alpha=0.5, label=provider, bins=20)
axes[1, 1].set_title('Response Length Distribution')
axes[1, 1].set_xlabel('Character Count')
axes[1, 1].legend()

plt.tight_layout()
plt.show()
```

## Step 12: Generate Evaluation Report

```python
# Create evaluation summary
print("\n" + "="*60)
print("BATCH EVALUATION SUMMARY")
print("="*60)

for provider in providers.keys():
    print(f"\n{provider.upper()} Performance:")
    print(f"  - Avg Tokens Used: {df_comparison[f'{provider}_tokens'].mean():.1f}")
    print(f"  - Avg Latency: {df_comparison[f'{provider}_latency'].mean():.1f}ms")
    print(f"  - Guardrail Pass Rate: {pass_rates[provider]:.1f}%")
    print(f"  - Avg Response Length: {df_comparison[f'{provider}_response'].str.len().mean():.0f} chars")

# Export detailed results
df_comparison.to_csv('evaluation_results.csv', index=False)
print(f"\nâœ“ Detailed results saved to 'evaluation_results.csv'")

# Cost estimation (example rates)
cost_per_1k_tokens = {
    "openai": 0.002,
    "bedrock": 0.0015,
    "vertex": 0.0018
}

print("\n" + "="*60)
print("ESTIMATED COSTS")
print("="*60)
for provider in providers.keys():
    total_tokens = df_comparison[f'{provider}_tokens'].sum()
    cost = (total_tokens / 1000) * cost_per_1k_tokens.get(provider, 0.002)
    print(f"{provider}: ${cost:.4f} for {len(df_comparison)} requests")
```

## Best Practices and Tips

1. **Batch Size**: Keep batch sizes reasonable (100-1000 requests) for optimal processing
2. **Guardrails**: Use guardrails to ensure consistent quality across providers
3. **Variables**: Use meaningful variable names in prompt templates
4. **Error Handling**: Always check batch status and handle failed requests
5. **Cost Optimization**: Use `immediate` completion window for faster results during testing

## Conclusion

This cookbook demonstrated how to:
- Create and use prompt templates with variables
- Set up guardrails for quality control
- Run batch evaluations across multiple providers
- Analyze and compare results effectively

With Portkey's unified batch API, you can easily evaluate and compare LLM providers at scale, ensuring you choose the best option for your specific use case while maintaining quality and compliance standards.

## Next Steps

- Experiment with different prompt templates
- Add more sophisticated guardrails
- Integrate results into your CI/CD pipeline
- Use insights to optimize your production deployments
